{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Swati Kar\n",
    "# email: swati.cse.ruet@gmail.com\n",
    "\n",
    "#necessary packages\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# environment code\n",
    "class FighterJetEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FighterJetEnv, self).__init__()\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.width = 800\n",
    "        self.height = 800\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"V14_exp_extend Fighter Jet Game\")\n",
    "\n",
    "\n",
    "        # Initialize font for text rendering\n",
    "        self.font = pygame.font.Font(None, 20)\n",
    "\n",
    "        # Add a semi-transparent surface for text background\n",
    "        self.text_surface = pygame.Surface((200, 100))\n",
    "        self.text_surface.set_alpha(128)\n",
    "        self.text_surface.fill((0, 0, 0))\n",
    "        self.dot_spacing = 2  # Adjust this value to change the spacing between dots\n",
    "\n",
    "        # Update colors\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "        self.LIGHT_BLUE = (100, 100, 255)  # Darker shade of light blue for better visibility\n",
    "\n",
    "        \n",
    "\n",
    "        # New colors\n",
    "        self.LIGHT_GREEN = (200, 255, 200)  # Very light green for background\n",
    "        self.DARK_GREEN = (0, 100, 0)  # Dark green for agent and bullets\n",
    "\n",
    "        \n",
    "\n",
    "        # Update background color\n",
    "        self.background = pygame.Surface((self.width, self.height))\n",
    "        self.background.fill(self.LIGHT_GREEN)\n",
    "\n",
    "        # Game parameters\n",
    "        # self.jet_speed = 3\n",
    "        self.bullet_speed = 7\n",
    "        self.targeting_zone_radius = 200\n",
    "        self.enemy_observation_radius = 250\n",
    "        self.agent_observation_radius = self.targeting_zone_radius\n",
    "        self.max_speed = 3\n",
    "        self.min_speed = 0\n",
    "        self.acceleration = 0.25\n",
    "        self.turn_rate = 0.05\n",
    "        self.current_speed = self.min_speed\n",
    "        self.good_shooting_distance_threshold = 50\n",
    "\n",
    "        # Update enemy parameters\n",
    "        self.enemy_speed = 4\n",
    "        self.enemy_bullet_speed = 7\n",
    "        self.enemy_shoot_interval = 5 \n",
    "        self.enemy_max_bullets = 1000\n",
    "\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions: 0: do nothing, 1: turn left, 2: turn right, 3: accelerate, 4: decelerate, 5: shoot\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "\n",
    "        self.obs_ranges = {\n",
    "            'jet_pos_x': (0, self.width),\n",
    "            'jet_pos_y': (0, self.height),\n",
    "            'jet_orientation': (-math.pi, math.pi),\n",
    "            'jet_velocity_x': (-self.max_speed, self.max_speed),\n",
    "            'jet_velocity_y': (-self.max_speed, self.max_speed),\n",
    "            'angle_to_target': (-math.pi, math.pi),\n",
    "            'dist_to_target': (0, math.sqrt(self.width**2 + self.height**2)),\n",
    "            'enemy_visible': (0, 1),\n",
    "            'angle_to_enemy': (-math.pi, math.pi),\n",
    "            'dist_to_enemy': (0, math.sqrt(self.width**2 + self.height**2)),\n",
    "            'bullet_visible': (0, 1),\n",
    "            'dist_to_bullet': (0, math.sqrt(self.width**2 + self.height**2)),\n",
    "            'in_target_zone': (0, 1)\n",
    "        }\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(13,), dtype=np.float32)\n",
    "\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.edge_buffer = 20\n",
    "\n",
    "        self.in_target_zone = False\n",
    "        self.enemy_visible = False\n",
    "\n",
    "        # attributes for episode limit and bullet optimization\n",
    "        self.max_steps = 2000\n",
    "        self.current_step = 0\n",
    "        self.max_bullets = 50\n",
    "        self.bullet_count = 0\n",
    "\n",
    "        self.current_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.agent_status = \"Game started\"\n",
    "\n",
    "        self.action_counts = [0] * 6\n",
    "        self.action_rewards = [0] * 6\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        print(\"Reset environment\")\n",
    "        self.episode += 1\n",
    "        # Reset jet, enemy, target positions and other game states\n",
    "        self.jet_pos = np.array([self.width // 2, self.height - 50], dtype=np.float32)\n",
    "        self.jet_orientation = -math.pi / 2 \n",
    "        self.current_speed = self.min_speed\n",
    "        self.jet_velocity = self.current_speed * np.array([math.cos(self.jet_orientation), math.sin(self.jet_orientation)])\n",
    "        self.enemy_pos = np.array([self.width // 2, 50], dtype=np.float32)\n",
    "        self.target_pos = np.array([random.randint(50, self.width-50), random.randint(50, self.height-50)], dtype=np.float32)\n",
    "        self.jet_bullets = []\n",
    "        self.enemy_bullets = []\n",
    "        self.enemy_shoot_counter = 0\n",
    "        self.enemy_bullet_count = 0\n",
    "        self.in_target_zone = False\n",
    "        self.enemy_visible = False\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.bullet_count = 0\n",
    "\n",
    "        self.action_counts = [0] * 6\n",
    "        self.action_rewards = [0] * 6\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "        self.current_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.agent_status = \"Game started\"\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, {} \n",
    "\n",
    "    def step(self, action):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return self._get_obs(), 0, True, False, {}\n",
    "            \n",
    "        prev_jet_pos = self.jet_pos.copy()\n",
    "            \n",
    "        # Process action\n",
    "        if action == 0:  # Do nothing\n",
    "            pass\n",
    "        elif action == 1:  # Turn left\n",
    "            self.jet_orientation -= self.turn_rate\n",
    "        elif action == 2:  # Turn right\n",
    "            self.jet_orientation += self.turn_rate\n",
    "        elif action == 3:  # Accelerate\n",
    "            self.current_speed = min(self.current_speed + self.acceleration, self.max_speed)\n",
    "        elif action == 4:  # Decelerate\n",
    "            self.current_speed = max(self.current_speed - self.acceleration, self.min_speed)\n",
    "        if action == 5:  # Shoot\n",
    "            self.bullet_count += 1\n",
    "            bullet_velocity = self.bullet_speed * np.array([math.cos(self.jet_orientation), math.sin(self.jet_orientation)])\n",
    "            self.jet_bullets.append([self.jet_pos.copy(), bullet_velocity])\n",
    "\n",
    "        # Move jet forward in the direction it's facing\n",
    "        self.jet_velocity = self.current_speed * np.array([math.cos(self.jet_orientation), math.sin(self.jet_orientation)])\n",
    "        new_pos = self.jet_pos + self.jet_velocity\n",
    "\n",
    "        \n",
    "\n",
    "        # Check for out-of-bounds\n",
    "        if new_pos[0] < self.edge_buffer or new_pos[0] > self.width - self.edge_buffer:\n",
    "            self.jet_orientation = math.pi - self.jet_orientation\n",
    "            self.jet_orientation = (self.jet_orientation + math.pi) % (2 * math.pi) - math.pi\n",
    "            new_pos[0] = max(min(new_pos[0], self.width - self.edge_buffer - 1), self.edge_buffer + 1)\n",
    "\n",
    "        if new_pos[1] < self.edge_buffer or new_pos[1] > self.height - self.edge_buffer:\n",
    "            self.jet_orientation = -self.jet_orientation\n",
    "            self.jet_orientation = (self.jet_orientation + math.pi) % (2 * math.pi) - math.pi\n",
    "            new_pos[1] = max(min(new_pos[1], self.height - self.edge_buffer - 1), self.edge_buffer + 1)\n",
    "\n",
    "        self.jet_pos = new_pos\n",
    "\n",
    "\n",
    "       # Update in_target_zone and enemy_visible status\n",
    "        self.in_target_zone = np.linalg.norm(self.jet_pos - self.target_pos) <= self.targeting_zone_radius\n",
    "        self.enemy_visible = self.enemy_pos is not None and np.linalg.norm(self.jet_pos - self.enemy_pos) <= self.enemy_observation_radius\n",
    "\n",
    "        # Move bullets\n",
    "        for bullet in self.jet_bullets:\n",
    "            bullet[0] += bullet[1]\n",
    "        self.jet_bullets = [bullet for bullet in self.jet_bullets if 0 <= bullet[0][0] <= self.width and 0 <= bullet[0][1] <= self.height]\n",
    "\n",
    "        for bullet in self.enemy_bullets:\n",
    "            bullet[0] += bullet[1]\n",
    "        self.enemy_bullets = [bullet for bullet in self.enemy_bullets if 0 <= bullet[0][0] <= self.width and 0 <= bullet[0][1] <= self.height]\n",
    "\n",
    "        \n",
    "\n",
    "        # enemy behavior\n",
    "        if self.enemy_pos is not None:\n",
    "            dist_to_jet = np.linalg.norm(self.jet_pos - self.enemy_pos)\n",
    "            direction = (self.jet_pos - self.enemy_pos) / dist_to_jet\n",
    "            \n",
    "            if dist_to_jet > 100:\n",
    "                self.enemy_pos += direction * self.enemy_speed\n",
    "            elif dist_to_jet < 50:\n",
    "                self.enemy_pos -= direction * self.enemy_speed\n",
    "            else:\n",
    "                self.enemy_pos += np.random.uniform(-0.5, 0.5, 2) * self.enemy_speed\n",
    "\n",
    "            self.enemy_pos = np.clip(self.enemy_pos, self.edge_buffer, [self.width - self.edge_buffer, self.height - self.edge_buffer])\n",
    "\n",
    "            self.enemy_shoot_counter += 1\n",
    "            if self.enemy_shoot_counter >= self.enemy_shoot_interval and self.enemy_bullet_count < self.enemy_max_bullets:\n",
    "                bullet_velocity = self.enemy_bullet_speed * direction\n",
    "                self.enemy_bullets.append([self.enemy_pos.copy(), bullet_velocity])\n",
    "                self.enemy_bullet_count += 1\n",
    "                self.enemy_shoot_counter = 0\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # Small negative reward for each step to encourage efficiency\n",
    "        reward -= 0.1\n",
    "\n",
    "        # Calculate distances\n",
    "        prev_dist_to_target = np.linalg.norm(self.target_pos - prev_jet_pos)\n",
    "        current_dist_to_target = np.linalg.norm(self.target_pos - self.jet_pos)\n",
    "        # print(f\"prev_dist_to_target: {prev_dist_to_target} |current_dist_to_target: {current_dist_to_target} \")\n",
    "        \n",
    "        # Calculate reward\n",
    "        distance_change = prev_dist_to_target - current_dist_to_target\n",
    "        distance_weight = 10  # Increased from 5 to 10\n",
    "        reward += distance_change * distance_weight\n",
    "\n",
    "        # Add an additional penalty for moving away from the target\n",
    "        if distance_change < 0:\n",
    "            moving_away_penalty = abs(distance_change) * 15  # Additional penalty\n",
    "            reward -= moving_away_penalty\n",
    "\n",
    "        \n",
    "        if self.in_target_zone:\n",
    "            reward += 2\n",
    "            self.agent_status = \"Agent in target zone\"\n",
    "        else:\n",
    "            reward += -1\n",
    "            self.agent_status = \"Agent out of target zone\"\n",
    "\n",
    "        if self.enemy_visible:\n",
    "            reward += 1\n",
    "            self.agent_status = \"Enemy visible\"\n",
    "        else:\n",
    "            reward -= 0.5\n",
    "\n",
    "        if self.bullet_count > self.max_bullets:\n",
    "            reward -= 0.5\n",
    "            self.agent_status = \"Excessive bullet usage\"\n",
    "\n",
    "\n",
    "        if self.in_target_zone and any(np.linalg.norm(bullet[0] - self.target_pos) < 15 for bullet in self.jet_bullets):\n",
    "            reward += 200\n",
    "            done = True\n",
    "            self.agent_status = \"Agent hit target within target zone\"\n",
    "\n",
    "        if self.enemy_visible and self.enemy_pos is not None and any(np.linalg.norm(bullet[0] - self.enemy_pos) < 15 for bullet in self.jet_bullets):\n",
    "            reward += 100\n",
    "            self.agent_status = \"Agent hit enemy\"\n",
    "            self.enemy_pos = None\n",
    "            self.enemy_visible = False\n",
    "            self.enemy_bullets = []\n",
    "\n",
    "        if self.enemy_visible and any(np.linalg.norm(bullet[0] - self.jet_pos) < 15 for bullet in self.enemy_bullets):\n",
    "            reward += -500\n",
    "            done = True\n",
    "            self.agent_status = \"Enemy hit agent\"\n",
    "\n",
    "        # Check if max steps reached\n",
    "        if self.current_step >= self.max_steps:\n",
    "            reward -= 1000 \n",
    "            done = True\n",
    "            self.agent_status = \"Max steps reached without hitting target\"\n",
    "\n",
    "        self.last_state = self._get_obs()\n",
    "        self.last_action = action\n",
    "        self.action_counts[action] += 1\n",
    "        self.action_rewards[action] += reward\n",
    "\n",
    "        self.current_step += 1\n",
    "        self.episode_reward += reward\n",
    "        self.current_reward = reward\n",
    "        self.render()\n",
    "        return self._get_obs(), reward, done, False, {\"status\": self.agent_status, \"episode_reward\": self.episode_reward}\n",
    "      \n",
    "\n",
    "    def _get_obs(self):\n",
    "        jet_to_target = self.target_pos - self.jet_pos\n",
    "        \n",
    "        angle_to_target = math.atan2(jet_to_target[1], jet_to_target[0])\n",
    "        dist_to_target = np.linalg.norm(jet_to_target)\n",
    "        \n",
    "        if self.enemy_pos is not None and self.enemy_visible:\n",
    "            jet_to_enemy = self.enemy_pos - self.jet_pos\n",
    "            angle_to_enemy = math.atan2(jet_to_enemy[1], jet_to_enemy[0])\n",
    "            dist_to_enemy = np.linalg.norm(jet_to_enemy)\n",
    "        else:\n",
    "            angle_to_enemy = 0\n",
    "            dist_to_enemy = self.enemy_observation_radius\n",
    "\n",
    "        closest_enemy_bullet = min(self.enemy_bullets, key=lambda bullet: np.linalg.norm(bullet[0] - self.jet_pos)) if self.enemy_bullets else [self.jet_pos, np.zeros(2)]\n",
    "        jet_to_bullet = closest_enemy_bullet[0] - self.jet_pos\n",
    "        angle_to_bullet = math.atan2(jet_to_bullet[1], jet_to_bullet[0])\n",
    "        dist_to_bullet = np.linalg.norm(jet_to_bullet)\n",
    "        bullet_visible = int(len(self.enemy_bullets) > 0)\n",
    "\n",
    "        obs = [\n",
    "            self.jet_pos[0],\n",
    "            self.jet_pos[1],\n",
    "            self.jet_orientation,\n",
    "            self.jet_velocity[0],\n",
    "            self.jet_velocity[1],\n",
    "            angle_to_target,\n",
    "            dist_to_target,\n",
    "            int(self.enemy_pos is not None and self.enemy_visible),\n",
    "            angle_to_enemy,\n",
    "            dist_to_enemy,\n",
    "            bullet_visible,\n",
    "            dist_to_bullet,\n",
    "            int(self.in_target_zone)\n",
    "        ]\n",
    "\n",
    "        # Normalize observations\n",
    "        normalized_obs = [\n",
    "            (obs[i] - self.obs_ranges[key][0]) / (self.obs_ranges[key][1] - self.obs_ranges[key][0])\n",
    "            for i, key in enumerate(self.obs_ranges.keys())\n",
    "        ]\n",
    "\n",
    "        return np.array(normalized_obs, dtype=np.float32)\n",
    "    \n",
    "    def draw_dotted_circle(self, surface, color, center, radius, width=1):\n",
    "\n",
    "        for i in range(0, 360, self.dot_spacing):\n",
    "\n",
    "            angle = i * math.pi / 180\n",
    "\n",
    "            start_pos = (center[0] + int(radius * math.cos(angle)),\n",
    "\n",
    "                        center[1] + int(radius * math.sin(angle)))\n",
    "\n",
    "            end_pos = (center[0] + int(radius * math.cos(angle + math.pi / 180)),\n",
    "\n",
    "                    center[1] + int(radius * math.sin(angle + math.pi / 180)))\n",
    "\n",
    "            pygame.draw.line(surface, color, start_pos, end_pos, width)\n",
    "\n",
    "    def render(self):\n",
    "        # Fill the background with light green\n",
    "        self.screen.blit(self.background, (0, 0))\n",
    "        \n",
    "        # Draw targeting zone\n",
    "        self.draw_dotted_circle(self.screen, self.LIGHT_BLUE, self.target_pos.astype(int), self.targeting_zone_radius, 2)\n",
    "        \n",
    "        # Draw observation ranges\n",
    "        self.draw_dotted_circle(self.screen, self.DARK_GREEN, self.jet_pos.astype(int), self.agent_observation_radius, 1)        \n",
    "       \n",
    "        # Draw jet\n",
    "        jet_direction = np.array([math.cos(self.jet_orientation), math.sin(self.jet_orientation)])\n",
    "        jet_nose = self.jet_pos + 20 * jet_direction\n",
    "        jet_left = self.jet_pos + 10 * np.array([-jet_direction[1], jet_direction[0]])\n",
    "        jet_right = self.jet_pos + 10 * np.array([jet_direction[1], -jet_direction[0]])\n",
    "        pygame.draw.polygon(self.screen, self.DARK_GREEN, [jet_nose, jet_left, jet_right])\n",
    "\n",
    "        if self.enemy_pos is not None:\n",
    "            self.draw_dotted_circle(self.screen, self.RED, self.enemy_pos.astype(int), self.enemy_observation_radius, 1)\n",
    "            enemy_direction = (self.jet_pos - self.enemy_pos) / np.linalg.norm(self.jet_pos - self.enemy_pos)\n",
    "            enemy_nose = self.enemy_pos + 20 * enemy_direction\n",
    "            enemy_left = self.enemy_pos + 10 * np.array([-enemy_direction[1], enemy_direction[0]])\n",
    "            enemy_right = self.enemy_pos + 10 * np.array([enemy_direction[1], -enemy_direction[0]])\n",
    "            pygame.draw.polygon(self.screen, self.RED, [enemy_nose, enemy_left, enemy_right])\n",
    "        \n",
    "        pygame.draw.circle(self.screen, self.BLUE, self.target_pos.astype(int), 15)\n",
    "        \n",
    "        for bullet in self.jet_bullets:\n",
    "            pygame.draw.circle(self.screen, self.DARK_GREEN, bullet[0].astype(int), 3)\n",
    "        for bullet in self.enemy_bullets:\n",
    "            pygame.draw.circle(self.screen, self.RED, bullet[0].astype(int), 3)\n",
    "\n",
    "        text_color = (0, 0, 0)  # Black text\n",
    "\n",
    "        status_text = self.font.render(f\"Episode: {self.episode}\", True, text_color)\n",
    "        self.screen.blit(status_text, (15, 15))\n",
    "\n",
    "\n",
    "        reward_text = self.font.render(f\"Reward: {self.episode_reward:.2f}\", True, text_color)\n",
    "        self.screen.blit(reward_text, (15, 35))\n",
    "\n",
    "\n",
    "        step_text = self.font.render(f\"Steps: {self.current_step}/{self.max_steps}\", True, text_color)\n",
    "        self.screen.blit(step_text, (15, 55))\n",
    "\n",
    "\n",
    "        bullet_text = self.font.render(f\"Bullets: {self.bullet_count}/{self.max_bullets}\", True, text_color)\n",
    "        self.screen.blit(bullet_text, (15, 75))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "# from stable_baselines3.dqn import DDQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_reward = 0\n",
    "        self.q_values = []\n",
    "        \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "         # Access the VecEnv\n",
    "        env = self.training_env.envs[0]\n",
    "        self.episode_reward += self.locals['rewards'][0]\n",
    "\n",
    "\n",
    "        # Log epsilon value\n",
    "        epsilon = self.model.exploration_rate\n",
    "        self.logger.record(\"exploration/epsilon\", epsilon)\n",
    "        # Log Q-values\n",
    "        obs = self.locals['new_obs']\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model.q_net(torch.as_tensor(obs).to(self.model.device))\n",
    "        q_values = q_values.cpu().numpy()\n",
    "        self.q_values.append(q_values)\n",
    "\n",
    "        if self.locals['dones']:\n",
    "            self.episode_lengths.append(self.locals['infos'][0]['episode']['l'])\n",
    "            self.logger.record('main/ep_rew_total', self.episode_reward)\n",
    "            self.logger.record('main/ep_len_mean', sum(self.episode_lengths) / len(self.episode_lengths))\n",
    "\n",
    "            \n",
    "            # print(\"Episode reward: \",self.episode_reward)\n",
    "            # print(\"Total Steps: \", self.episode_lengths )\n",
    "            # print(env.agent_status)\n",
    "            # Log Q-values\n",
    "            mean_q_values = np.mean(self.q_values, axis=0)\n",
    "            for i, q_value in enumerate(mean_q_values[0]):\n",
    "                self.logger.record(f'q_values/action_{i}', q_value)\n",
    "            self.logger.record('q_values/max', np.max(mean_q_values))\n",
    "            self.logger.record('q_values/min', np.min(mean_q_values))\n",
    "            self.logger.record('q_values/mean', np.mean(mean_q_values))\n",
    "\n",
    "            print(\"Episode reward:\", self.episode_reward)\n",
    "            print(\"Total Steps:\", self.episode_lengths[-1])\n",
    "            print(\"Agent status:\", env.agent_status)\n",
    "            print(\"Mean Q-values:\", mean_q_values[0])\n",
    "\n",
    "            self.episode_reward  = 0\n",
    "        return True\n",
    "    \n",
    "\n",
    "# Create the callback\n",
    "callback = TensorboardCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters(model):\n",
    "    print(\"\\nDQN Hyperparameters:\")\n",
    "    print(f\"Learning rate: {model.learning_rate}\")\n",
    "    print(f\"Gamma (discount factor): {model.gamma}\")\n",
    "    print(f\"Tau (soft update coefficient): {model.tau}\")\n",
    "    print(f\"Train frequency: {model.train_freq}\")\n",
    "    print(f\"Gradient steps: {model.gradient_steps}\")\n",
    "    print(f\"Batch size: {model.batch_size}\")\n",
    "    print(f\"Learning starts: {model.learning_starts}\")\n",
    "    print(f\"Buffer size: {model.buffer_size}\")\n",
    "    print(f\"Target update interval: {model.target_update_interval}\")\n",
    "    print(f\"Exploration initial epsilon: {model.exploration_initial_eps}\")\n",
    "    print(f\"Exploration final epsilon: {model.exploration_final_eps}\")\n",
    "    print(f\"Exploration fraction: {model.exploration_fraction}\")\n",
    "    print(f\"Max gradient norm: {model.max_grad_norm}\")\n",
    "    print(\"\\nPolicy network architecture:\")\n",
    "    print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "No existing model found. Creating a new DQN agent.\n",
      "\n",
      "DQN Hyperparameters:\n",
      "Learning rate: 5e-05\n",
      "Gamma (discount factor): 0.99\n",
      "Tau (soft update coefficient): 0.001\n",
      "Train frequency: TrainFreq(frequency=1, unit=<TrainFrequencyUnit.STEP: 'step'>)\n",
      "Gradient steps: 1\n",
      "Batch size: 256\n",
      "Learning starts: 50000\n",
      "Buffer size: 500000\n",
      "Target update interval: 5000\n",
      "Exploration initial epsilon: 1.0\n",
      "Exploration final epsilon: 0.1\n",
      "Exploration fraction: 0.7\n",
      "Max gradient norm: 10\n",
      "\n",
      "Policy network architecture:\n",
      "DQNPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=13, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=13, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Reset environment\n",
      "Reset environment\n",
      "Episode reward: -393.7033559605479\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09519541 -0.0529201  -0.0448711   0.00045231 -0.00550619  0.03304763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swati\\anaconda3\\envs\\jetFight_\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.agent_status to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_status` for environment variables or `env.get_wrapper_attr('agent_status')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset environment\n",
      "Episode reward: -349.37176993489265\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09653186 -0.05284173 -0.04410682  0.00290264 -0.00592436  0.03353243]\n",
      "Reset environment\n",
      "Episode reward: -654.7779326438904\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09625923 -0.05325924 -0.04319811  0.00262644 -0.00601663  0.03314087]\n",
      "Reset environment\n",
      "Episode reward: -254.21439254283905\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09640966 -0.05903421 -0.04245711  0.00288892 -0.00837319  0.03522398]\n",
      "Reset environment\n",
      "Episode reward: 845.8703354597092\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09758884 -0.05637693 -0.04348695  0.00407007 -0.00839123  0.03616947]\n",
      "Reset environment\n",
      "Episode reward: 285.80650091171265\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09782453 -0.05522942 -0.04396492  0.00440773 -0.00837089  0.0364355 ]\n",
      "Reset environment\n",
      "Episode reward: -426.824201464653\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09809775 -0.05482308 -0.04405056  0.00495139 -0.00826402  0.03618592]\n",
      "Reset environment\n",
      "Episode reward: -460.4126908034086\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09798922 -0.05446402 -0.04400893  0.00507946 -0.0079716   0.03605053]\n",
      "Reset environment\n",
      "Episode reward: -812.0882952772081\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09801009 -0.05402603 -0.04416601  0.00481487 -0.0081791   0.03613395]\n",
      "Reset environment\n",
      "Episode reward: -174.34831353276968\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09782808 -0.05387539 -0.0443392   0.00457913 -0.00796078  0.03598205]\n",
      "Reset environment\n",
      "Episode reward: -389.5454383008182\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09801876 -0.05381161 -0.04440623  0.00481952 -0.00794539  0.03583435]\n",
      "Reset environment\n",
      "Episode reward: -23550.402825444937\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09180116 -0.05025187 -0.04107563  0.00404812 -0.00848661  0.04398378]\n",
      "Reset environment\n",
      "Episode reward: -625.407910682261\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09209257 -0.05038042 -0.04124046  0.00413622 -0.00848469  0.04368962]\n",
      "Reset environment\n",
      "Episode reward: -169.25803637504578\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09233434 -0.05049073 -0.04142125  0.00420596 -0.00846238  0.04344403]\n",
      "Reset environment\n",
      "Episode reward: -347.71733410283923\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0925514  -0.05061013 -0.04159373  0.00424159 -0.0084476   0.04323477]\n",
      "Reset environment\n",
      "Episode reward: -545.787670135498\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09273123 -0.05062352 -0.04169878  0.00439338 -0.00834914  0.04304916]\n",
      "Reset environment\n",
      "Episode reward: -415.26348197460175\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09292446 -0.05063024 -0.04182573  0.00455134 -0.0082917   0.04288113]\n",
      "Reset environment\n",
      "Episode reward: 736.1535261645913\n",
      "Total Steps: 774\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0925051  -0.05132158 -0.04132659  0.0029086  -0.00877669  0.04359739]\n",
      "Reset environment\n",
      "Episode reward: -430.23021399672143\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09262579 -0.051383   -0.04134748  0.00292713 -0.00874435  0.04333332]\n",
      "Reset environment\n",
      "Episode reward: -179.67664885520935\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09278559 -0.05139741 -0.04146497  0.00305301 -0.00871682  0.0431707 ]\n",
      "Reset environment\n",
      "Episode reward: 3416.513030529022\n",
      "Total Steps: 231\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09342488 -0.05122199 -0.04190493  0.00305467 -0.00913227  0.04310159]\n",
      "Reset environment\n",
      "Episode reward: 438.9998061656952\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09351633 -0.05120662 -0.04205304  0.00314128 -0.00905673  0.04303161]\n",
      "Reset environment\n",
      "Episode reward: -149.40624034404755\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09355167 -0.05173292 -0.04202458  0.00315758 -0.0091721   0.04298277]\n",
      "Reset environment\n",
      "Episode reward: 575.5123227834702\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09361883 -0.05169628 -0.04215986  0.0032137  -0.00909235  0.04291783]\n",
      "Reset environment\n",
      "Episode reward: -129.87688356637955\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09370018 -0.05169332 -0.04222505  0.00327742 -0.00903712  0.04278632]\n",
      "Reset environment\n",
      "Episode reward: 52.13270103931427\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09375909 -0.05164943 -0.04232967  0.00335509 -0.00895765  0.04272374]\n",
      "Reset environment\n",
      "Episode reward: -226.5271829366684\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0938564  -0.05168708 -0.04236548  0.00339029 -0.00893306  0.04254955]\n",
      "Reset environment\n",
      "Episode reward: -519.95433915779\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0939296  -0.051693   -0.0423861   0.00345209 -0.00887652  0.0424018 ]\n",
      "Reset environment\n",
      "Episode reward: -380.731426179409\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0940167  -0.05173583 -0.04239785  0.00347712 -0.00885532  0.0422223 ]\n",
      "Reset environment\n",
      "Episode reward: 14.26464231312275\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09412088 -0.05175295 -0.04239343  0.00345845 -0.00887572  0.04201038]\n",
      "Reset environment\n",
      "Episode reward: 3949.538438439369\n",
      "Total Steps: 628\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09386624 -0.05152049 -0.04234536  0.00265615 -0.0088589   0.04219502]\n",
      "Reset environment\n",
      "Episode reward: 167.3137777596712\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0941633  -0.05148698 -0.04238652  0.00274982 -0.00901318  0.04198627]\n",
      "Reset environment\n",
      "Episode reward: -421.11266105622053\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09424729 -0.05157737 -0.0424296   0.00277973 -0.00900882  0.04186093]\n",
      "Reset environment\n",
      "Episode reward: 141.18885099887848\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09428834 -0.05157366 -0.04248462  0.00281561 -0.00895131  0.04178735]\n",
      "Reset environment\n",
      "Episode reward: 187.23055525496602\n",
      "Total Steps: 210\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09468716 -0.05144831 -0.04257504  0.00294849 -0.00920839  0.0416013 ]\n",
      "Reset environment\n",
      "Episode reward: -9279.25405109278\n",
      "Total Steps: 1170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09361042 -0.0507181  -0.04173998  0.00226773 -0.00937347  0.04265078]\n",
      "Reset environment\n",
      "Episode reward: -481.4968702197075\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0936878  -0.05076954 -0.04176331  0.00230499 -0.00935814  0.04251833]\n",
      "Reset environment\n",
      "Episode reward: -250.49796038866043\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09375619 -0.05078306 -0.04180519  0.00237228 -0.00933247  0.04242877]\n",
      "Reset environment\n",
      "Episode reward: -335.0266509652138\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09381716 -0.05080193 -0.04183937  0.00242781 -0.00929206  0.04233653]\n",
      "Reset environment\n",
      "Episode reward: 640.5966736078262\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09381208 -0.05091495 -0.04184586  0.00244293 -0.00932546  0.04232717]\n",
      "Reset environment\n",
      "Episode reward: -371.63411062955856\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09387198 -0.05091212 -0.0419006   0.00253348 -0.00927186  0.04227878]\n",
      "Reset environment\n",
      "Episode reward: -267.1223157644272\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09393246 -0.05091829 -0.04194818  0.0026049  -0.0092269   0.04221632]\n",
      "Reset environment\n",
      "Episode reward: -313.10110069438815\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09399886 -0.05096018 -0.04197709  0.00263646 -0.00922359  0.04211396]\n",
      "Reset environment\n",
      "Episode reward: -75.37335497140884\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09404507 -0.05100604 -0.04199865  0.00264521 -0.00921169  0.04201147]\n",
      "Reset environment\n",
      "Episode reward: 431.6373388171196\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09405889 -0.05115444 -0.0420239   0.00264216 -0.00925085  0.04197534]\n",
      "Reset environment\n",
      "Episode reward: -341.8300263285637\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0941238  -0.05116169 -0.04206927  0.00271413 -0.00921251  0.04191227]\n",
      "Reset environment\n",
      "Episode reward: 264.88690531253815\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09412168 -0.051388   -0.04207059  0.00271479 -0.00927128  0.0419038 ]\n",
      "Reset environment\n",
      "Episode reward: -33.01386517286301\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0941712  -0.05143613 -0.04210369  0.00273381 -0.00926535  0.04181995]\n",
      "Reset environment\n",
      "Episode reward: 13.214362919330597\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09429587 -0.05140493 -0.04215328  0.00282478 -0.00927131  0.0417706 ]\n",
      "Reset environment\n",
      "Episode reward: -377.2164349555969\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09433696 -0.05140116 -0.04219262  0.00289052 -0.0092177   0.04172831]\n",
      "Reset environment\n",
      "Episode reward: -191.25024020671844\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09438543 -0.05142993 -0.04221291  0.00290891 -0.00920484  0.04163451]\n",
      "Reset environment\n",
      "Episode reward: -1436.5048993825912\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0944436  -0.05165583 -0.04224709  0.0028978  -0.00933372  0.04164428]\n",
      "Reset environment\n",
      "Episode reward: -77.66645881533623\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09449837 -0.05165982 -0.04230271  0.00295122 -0.00929947  0.04159871]\n",
      "Reset environment\n",
      "Episode reward: -127.51760911941528\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09448657 -0.05190237 -0.04229829  0.00296596 -0.00936249  0.04159518]\n",
      "Reset environment\n",
      "Episode reward: -290.749383687973\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09453084 -0.05189243 -0.04233118  0.00302534 -0.00933303  0.0415431 ]\n",
      "Reset environment\n",
      "Episode reward: -307.3753117322922\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09458213 -0.05190581 -0.04235647  0.00306109 -0.0093141   0.04146654]\n",
      "Reset environment\n",
      "Episode reward: 537.0401018559933\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09468898 -0.05186366 -0.0424238   0.00312879 -0.00931286  0.0414413 ]\n",
      "Reset environment\n",
      "Episode reward: -563.3488516509533\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09474606 -0.05188124 -0.04244214  0.00317186 -0.0092972   0.04135991]\n",
      "Reset environment\n",
      "Episode reward: -253.26060243323445\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09476656 -0.05187927 -0.04246109  0.00319032 -0.00926668  0.04130109]\n",
      "Reset environment\n",
      "Episode reward: -951.2659623622894\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09486933 -0.05193198 -0.04250841  0.00325    -0.00928876  0.04130057]\n",
      "Reset environment\n",
      "Episode reward: -199.5061430335045\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09496848 -0.0519204  -0.04253135  0.00328461 -0.00935634  0.04124565]\n",
      "Reset environment\n",
      "Episode reward: -529.5469960724004\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09502387 -0.05194467 -0.04256227  0.00331419 -0.009339    0.04117212]\n",
      "Reset environment\n",
      "Episode reward: 1832.6463978290558\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09515768 -0.05186934 -0.0426182   0.00334327 -0.00942228  0.04115406]\n",
      "Reset environment\n",
      "Episode reward: -330.3896938562393\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09519993 -0.05186636 -0.0426476   0.00339639 -0.00940175  0.04110575]\n",
      "Reset environment\n",
      "Episode reward: -122.91840434074402\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09519767 -0.05204707 -0.04264463  0.00340195 -0.00944181  0.04109792]\n",
      "Reset environment\n",
      "Episode reward: -222.13076722621918\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09523278 -0.05206275 -0.04266104  0.00341759 -0.00941892  0.04102664]\n",
      "Reset environment\n",
      "Episode reward: -555.9150981605053\n",
      "Total Steps: 940\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09482516 -0.05178736 -0.04217613  0.00311611 -0.00957708  0.04173942]\n",
      "Reset environment\n",
      "Episode reward: -229.88246250152588\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09481448 -0.05198623 -0.04217377  0.00313048 -0.00961713  0.04173869]\n",
      "Reset environment\n",
      "Episode reward: 346.15827494859695\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09482718 -0.05202609 -0.04218886  0.00314273 -0.00961312  0.04169661]\n",
      "Reset environment\n",
      "Episode reward: -290.25027191638947\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09486282 -0.05202194 -0.04221792  0.00318614 -0.00958263  0.04165309]\n",
      "Reset environment\n",
      "Episode reward: 130.08119767904282\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09489165 -0.05201347 -0.04225507  0.00321701 -0.00955659  0.0416197 ]\n",
      "Reset environment\n",
      "Episode reward: -406.5685101747513\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0949129  -0.05200021 -0.04227954  0.00325905 -0.0095168   0.04158606]\n",
      "Reset environment\n",
      "Episode reward: -409.18113712221384\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09492736 -0.05201523 -0.04228285  0.00325315 -0.00948615  0.04151156]\n",
      "Reset environment\n",
      "Episode reward: -318.9132836163044\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09494755 -0.05200211 -0.04230748  0.00329098 -0.0094519   0.04147919]\n",
      "Reset environment\n",
      "Episode reward: -361.27896420657635\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09496219 -0.05201829 -0.0423123   0.00328181 -0.0094287   0.04140583]\n",
      "Reset environment\n",
      "Episode reward: -400.89550107717514\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09499293 -0.05200242 -0.04234125  0.00333731 -0.00940257  0.04137629]\n",
      "Reset environment\n",
      "Episode reward: 1283.575928390026\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09505454 -0.05196195 -0.04239608  0.00336021 -0.00941429  0.041362  ]\n",
      "Reset environment\n",
      "Episode reward: -243.36223751306534\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09508937 -0.05197892 -0.04241513  0.00337539 -0.00940024  0.04129955]\n",
      "Reset environment\n",
      "Episode reward: -8527.210494995117\n",
      "Total Steps: 579\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09490157 -0.05208125 -0.04222411  0.0032831  -0.00952044  0.04149168]\n",
      "Reset environment\n",
      "Episode reward: -61.60947370529175\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09492486 -0.05207031 -0.04225885  0.00332039 -0.00948757  0.04146982]\n",
      "Reset environment\n",
      "Episode reward: -113.65666425228119\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09492041 -0.05224719 -0.04225077  0.00332295 -0.00953544  0.04146606]\n",
      "Reset environment\n",
      "Episode reward: -62.82309854030609\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09494612 -0.05224358 -0.04227653  0.00335031 -0.00950754  0.04143033]\n",
      "Reset environment\n",
      "Episode reward: -390.2530895024538\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09496234 -0.05224633 -0.04228814  0.00336309 -0.00947349  0.04137959]\n",
      "Reset environment\n",
      "Episode reward: -92.9607640504837\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09499245 -0.05226168 -0.04230876  0.00337123 -0.00946022  0.04132383]\n",
      "Reset environment\n",
      "Episode reward: -474.21167492866516\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09501986 -0.05225078 -0.04233032  0.0034163  -0.00943501  0.0412925 ]\n",
      "Reset environment\n",
      "Episode reward: -346.66930271685123\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0950354  -0.05241233 -0.04231602  0.00341679 -0.00947096  0.041285  ]\n",
      "Reset environment\n",
      "Episode reward: -471.9104733578861\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09504298 -0.05241857 -0.04232051  0.00340991 -0.00944278  0.04122782]\n",
      "Reset environment\n",
      "Episode reward: -552.4347367631271\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09511247 -0.05245126 -0.04235921  0.00343834 -0.00945175  0.04121957]\n",
      "Reset environment\n",
      "Episode reward: 380.6479067802429\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09510522 -0.05252152 -0.04235992  0.00345125 -0.00946767  0.04121721]\n",
      "Reset environment\n",
      "Episode reward: -66.396490175277\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09516633 -0.05250951 -0.04239185  0.00348625 -0.0094763   0.04118303]\n",
      "Reset environment\n",
      "Episode reward: -293.9204851984978\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09519005 -0.05250082 -0.04241223  0.00351896 -0.00945378  0.04115047]\n",
      "Reset environment\n",
      "Episode reward: -21659.865227364236\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09444126 -0.05111351 -0.04243193  0.00358971 -0.00912078  0.04152686]\n",
      "Reset environment\n",
      "Episode reward: -321.4457836151123\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0944666  -0.05111044 -0.04245507  0.00362631 -0.00909687  0.04150687]\n",
      "Reset environment\n",
      "Episode reward: -143.31039237976074\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09448629 -0.0511157  -0.04246769  0.00363818 -0.00908351  0.04146789]\n",
      "Reset environment\n",
      "Episode reward: -375.5607799887657\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09450731 -0.05111159 -0.0424883   0.00367146 -0.00905587  0.04144906]\n",
      "Reset environment\n",
      "Episode reward: -574.460594529286\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09451623 -0.05113258 -0.04248206  0.00365529 -0.00904089  0.04138714]\n",
      "Reset environment\n",
      "Episode reward: -515.1206048354506\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09457004 -0.0511411  -0.04249821  0.00367557 -0.00905436  0.04134141]\n",
      "Reset environment\n",
      "Episode reward: -255.96356010437012\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09459323 -0.05115396 -0.04250793  0.0036839  -0.00903612  0.04129505]\n",
      "Reset environment\n",
      "Episode reward: -194.9131746813655\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09460624 -0.05116424 -0.04251406  0.00368155 -0.00901574  0.0412478 ]\n",
      "Reset environment\n",
      "Episode reward: -464.0213190317154\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09462532 -0.05116667 -0.04252594  0.0037037  -0.00899161  0.04121815]\n",
      "Reset environment\n",
      "Episode reward: 17.00136935710907\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09464376 -0.05118052 -0.04253884  0.00370341 -0.00897692  0.04117425]\n",
      "Reset environment\n",
      "Episode reward: -169.99234265089035\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09467214 -0.05119035 -0.04255705  0.00372122 -0.00896688  0.04113737]\n",
      "Reset environment\n",
      "Episode reward: -471.84513622522354\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09469111 -0.05118885 -0.04257184  0.00374988 -0.00894286  0.04111547]\n",
      "Reset environment\n",
      "Episode reward: -261.2837581783533\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09469426 -0.05129838 -0.04257074  0.00374997 -0.00896828  0.0411103 ]\n",
      "Reset environment\n",
      "Episode reward: -266.43699237704277\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09470867 -0.05128622 -0.04259493  0.00378116 -0.00894402  0.04110071]\n",
      "Reset environment\n",
      "Episode reward: -239.9816973451525\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09473669 -0.05129774 -0.04261517  0.00379537 -0.00893832  0.04106856]\n",
      "Reset environment\n",
      "Episode reward: -258.88158486783504\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09475046 -0.05132743 -0.04261749  0.00378899 -0.00893175  0.04102461]\n",
      "Reset environment\n",
      "Episode reward: -49.46723234653473\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09477217 -0.05132027 -0.04264418  0.00381724 -0.00891424  0.04101176]\n",
      "Reset environment\n",
      "Episode reward: -302.7151944041252\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09479387 -0.05132855 -0.04265355  0.00382974 -0.00889982  0.04097434]\n",
      "Reset environment\n",
      "Episode reward: -242.79320284724236\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09480537 -0.05132791 -0.04266606  0.00384079 -0.00887998  0.04094839]\n",
      "Reset environment\n",
      "Episode reward: -318.6589975953102\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09482765 -0.05134119 -0.04267158  0.00384741 -0.0088711   0.04090483]\n",
      "Reset environment\n",
      "Episode reward: 44.233095437288284\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0948517  -0.05134036 -0.04269836  0.00387028 -0.00885807  0.04088726]\n",
      "Reset environment\n",
      "Episode reward: -121.61933648586273\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09487117 -0.05133866 -0.04271722  0.00389236 -0.00884027  0.04086856]\n",
      "Reset environment\n",
      "Episode reward: -279.10254034399986\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09489612 -0.05133798 -0.04273585  0.00391891 -0.00883007  0.04084546]\n",
      "Reset environment\n",
      "Episode reward: -776.9843800850213\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09492286 -0.05133472 -0.0427468   0.00390186 -0.00884965  0.04081127]\n",
      "Reset environment\n",
      "Episode reward: -3568.441743641626\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09503729 -0.05132274 -0.04276631  0.00393313 -0.00892582  0.04076002]\n",
      "Reset environment\n",
      "Episode reward: 257.2819130420685\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09505485 -0.05132311 -0.04279151  0.00394935 -0.00890939  0.040744  ]\n",
      "Reset environment\n",
      "Episode reward: 2556.9561291337013\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09514707 -0.05130907 -0.0428282   0.00392708 -0.0090086   0.0407266 ]\n",
      "Reset environment\n",
      "Episode reward: -160.5674296617508\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09514162 -0.05143876 -0.04282023  0.00393011 -0.00904092  0.04072776]\n",
      "Reset environment\n",
      "Episode reward: -23170.97967327386\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0946508  -0.05095913 -0.04254451  0.00357243 -0.00909704  0.04119787]\n",
      "Reset environment\n",
      "Episode reward: -225.9269419685006\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0946691  -0.05095888 -0.04255659  0.0035921  -0.00907822  0.04117802]\n",
      "Reset environment\n",
      "Episode reward: -16415.870984143876\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09403613 -0.05031031 -0.04218942  0.0034843  -0.00891697  0.04185328]\n",
      "Reset environment\n",
      "Episode reward: -47.92058369517326\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09405079 -0.05032115 -0.04219789  0.00348544 -0.00890345  0.04181848]\n",
      "Reset environment\n",
      "Episode reward: 1312.0714862942696\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09411655 -0.05031089 -0.04221447  0.0035107  -0.0089268   0.04178454]\n",
      "Reset environment\n",
      "Episode reward: -6436.491446793079\n",
      "Total Steps: 1349\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09392628 -0.05008516 -0.04217376  0.00328503 -0.00889045  0.04175271]\n",
      "Reset environment\n",
      "Episode reward: -319.1466832458973\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0939429  -0.05008933 -0.0421873   0.00330593 -0.0088734   0.04173575]\n",
      "Reset environment\n",
      "Episode reward: 495.573308467865\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0939424  -0.05016698 -0.04218432  0.00331195 -0.00889456  0.04173071]\n",
      "Reset environment\n",
      "Episode reward: -13923.858254340099\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09366635 -0.04990017 -0.04179884  0.00336624 -0.00903336  0.04198876]\n",
      "Reset environment\n",
      "Episode reward: -23.412108200485818\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09371527 -0.04990794 -0.04182232  0.00338013 -0.00904542  0.04196043]\n",
      "Reset environment\n",
      "Episode reward: -17696.246101475786\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09348419 -0.04993221 -0.04155847  0.00313811 -0.00917792  0.04223195]\n",
      "Reset environment\n",
      "Episode reward: -22515.889326899778\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09330373 -0.04981438 -0.0413468   0.00331303 -0.00929207  0.04239735]\n",
      "Reset environment\n",
      "Episode reward: -523.8559786528349\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09331964 -0.04982114 -0.04135519  0.00332651 -0.00928042  0.04237371]\n",
      "Reset environment\n",
      "Episode reward: 1064.7918587327003\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09334993 -0.04982225 -0.04137648  0.00333725 -0.00927783  0.04235617]\n",
      "Reset environment\n",
      "Episode reward: -209.83304047584534\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09335595 -0.04989881 -0.04137938  0.0033419  -0.00929025  0.04235014]\n",
      "Reset environment\n",
      "Episode reward: -400.712841629982\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09337778 -0.04990714 -0.0413922   0.00335793 -0.00928315  0.04232589]\n",
      "Reset environment\n",
      "Episode reward: -530.683784455061\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09340042 -0.04992003 -0.04140162  0.00336986 -0.00927917  0.04229649]\n",
      "Reset environment\n",
      "Episode reward: 408.3656874895096\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09342765 -0.04991785 -0.04141998  0.00338665 -0.00927584  0.04228287]\n",
      "Reset environment\n",
      "Episode reward: -487.7627795934677\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09344154 -0.04991836 -0.04143387  0.00340652 -0.00925996  0.04227065]\n",
      "Reset environment\n",
      "Episode reward: -344.78234750032425\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09345815 -0.0499352  -0.04143997  0.00340876 -0.009256    0.04223996]\n",
      "Reset environment\n",
      "Episode reward: -272.7412692606449\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09347446 -0.04994238 -0.0414488   0.0034196  -0.00924942  0.04221615]\n",
      "Reset environment\n",
      "Episode reward: 1.7433995008468628\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09349113 -0.04994284 -0.04146853  0.00343587 -0.00923858  0.04220506]\n",
      "Reset environment\n",
      "Episode reward: -55.877366840839386\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09350661 -0.04994448 -0.04148549  0.00345148 -0.0092268   0.04219336]\n",
      "Reset environment\n",
      "Episode reward: 68.45086562633514\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09350786 -0.05001748 -0.04148716  0.00345287 -0.00924383  0.04218986]\n",
      "Reset environment\n",
      "Episode reward: 47.37736093997955\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09352423 -0.05001741 -0.04150772  0.00346999 -0.00923286  0.04218096]\n",
      "Reset environment\n",
      "Episode reward: -552.4346152953804\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0935478  -0.05003079 -0.04151883  0.0034818  -0.00923132  0.04215349]\n",
      "Reset environment\n",
      "Episode reward: -3642.076382637024\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09359121 -0.05003469 -0.04153323  0.00349374 -0.00925903  0.04216169]\n",
      "Reset environment\n",
      "Episode reward: -13969.978862255812\n",
      "Total Steps: 1269\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09342314 -0.05032862 -0.04137289  0.003439   -0.00935772  0.04231564]\n",
      "Reset environment\n",
      "Episode reward: 204.19998693466187\n",
      "Total Steps: 3\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09342282 -0.05032891 -0.04137281  0.00343844 -0.00935754  0.04231597]\n",
      "Reset environment\n",
      "Episode reward: 468.37164494395256\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09345048 -0.05031509 -0.0413864   0.00345614 -0.00936908  0.04230386]\n",
      "Reset environment\n",
      "Episode reward: -405.49679815769196\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09346532 -0.05031841 -0.04139599  0.00347005 -0.00935958  0.04228484]\n",
      "Reset environment\n",
      "Episode reward: 208.39994943141937\n",
      "Total Steps: 6\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09346486 -0.0503203  -0.04139573  0.00346916 -0.00935967  0.04228522]\n",
      "Reset environment\n",
      "Episode reward: 31.305644690990448\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09348027 -0.05032635 -0.04140808  0.00347641 -0.00935313  0.04226439]\n",
      "Reset environment\n",
      "Episode reward: -4937.9954397678375\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09355318 -0.0503193  -0.04143005  0.00350279 -0.00939145  0.04223447]\n",
      "Reset environment\n",
      "Episode reward: -574.8012657612562\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09356234 -0.05033212 -0.04142997  0.00349771 -0.00938395  0.04220187]\n",
      "Reset environment\n",
      "Episode reward: -410.76257967948914\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09358022 -0.05034458 -0.04143845  0.00350205 -0.00937817  0.04217394]\n",
      "Reset environment\n",
      "Episode reward: -70.23281222581863\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09360644 -0.05035097 -0.04143922  0.00350752 -0.00938995  0.04214117]\n",
      "Reset environment\n",
      "Episode reward: -296.52756636962295\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09361508 -0.050358   -0.04144559  0.00350668 -0.00938143  0.04211739]\n",
      "Reset environment\n",
      "Episode reward: -329.4448817074299\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09362784 -0.05036499 -0.0414517   0.00351211 -0.00937352  0.04209396]\n",
      "Reset environment\n",
      "Episode reward: 226.67978954315186\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09363652 -0.05036706 -0.04146456  0.00351506 -0.00936379  0.04207936]\n",
      "Reset environment\n",
      "Episode reward: -126.65161192417145\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09363844 -0.05044234 -0.04146233  0.00351565 -0.00938333  0.04207514]\n",
      "Reset environment\n",
      "Episode reward: -424.1690800189972\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09365017 -0.05045062 -0.04146787  0.00351922 -0.00937318  0.04205026]\n",
      "Reset environment\n",
      "Episode reward: 159.7709288597107\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09366053 -0.05045107 -0.0414829   0.00352793 -0.00936087  0.04203934]\n",
      "Reset environment\n",
      "Episode reward: -278.5393134057522\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09367774 -0.05045239 -0.04149864  0.0035462  -0.00935209  0.0420275 ]\n",
      "Reset environment\n",
      "Episode reward: -523.1798701807857\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09368722 -0.05045931 -0.041503    0.00354936 -0.00934192  0.04200416]\n",
      "Reset environment\n",
      "Episode reward: 535.1551184058189\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09370453 -0.05045633 -0.041518    0.00355529 -0.00934177  0.04199125]\n",
      "Reset environment\n",
      "Episode reward: 381.37055283784866\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09370828 -0.05046078 -0.04152334  0.0035578  -0.00934096  0.04198247]\n",
      "Reset environment\n",
      "Episode reward: -532.125954657793\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09372094 -0.05046986 -0.04152697  0.00356201 -0.00933208  0.0419567 ]\n",
      "Reset environment\n",
      "Episode reward: 1803.9248916506767\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09375363 -0.05047598 -0.04153591  0.00356686 -0.00933633  0.04192298]\n",
      "Reset environment\n",
      "Episode reward: 38.02251493930817\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09377483 -0.05047789 -0.04155155  0.00357938 -0.00933275  0.04190876]\n",
      "Reset environment\n",
      "Episode reward: -200.96615743637085\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09377536 -0.05055037 -0.04155111  0.00358197 -0.0093493   0.04190677]\n",
      "Reset environment\n",
      "Episode reward: -365.08821875136346\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09381033 -0.05055074 -0.04156633  0.00359023 -0.00936814  0.04189277]\n",
      "Reset environment\n",
      "Episode reward: -441.75764912366867\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09381625 -0.05061361 -0.04156876  0.00359142 -0.00938537  0.04189439]\n",
      "Reset environment\n",
      "Episode reward: -492.93108877539635\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09383378 -0.05062044 -0.04157636  0.00360278 -0.00937996  0.04187177]\n",
      "Reset environment\n",
      "Episode reward: -499.7542040422559\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0938383  -0.05066852 -0.04158164  0.00359638 -0.00939796  0.04187391]\n",
      "Reset environment\n",
      "Episode reward: -4239.2486053183675\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09387767 -0.05073226 -0.04159027  0.00361282 -0.00944308  0.04185963]\n",
      "Reset environment\n",
      "Episode reward: -520.8870758190751\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09388791 -0.05073461 -0.041597    0.00362255 -0.00943176  0.04184217]\n",
      "Reset environment\n",
      "Episode reward: 480.1435865163803\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09390364 -0.05072836 -0.04161387  0.00363352 -0.00942213  0.04183458]\n",
      "Reset environment\n",
      "Episode reward: 536.7745890617371\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09391412 -0.05072612 -0.04163463  0.00364261 -0.00941092  0.04183002]\n",
      "Reset environment\n",
      "Episode reward: -84.62500011920929\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09392697 -0.0507302  -0.04164536  0.00365025 -0.00940399  0.04181355]\n",
      "Reset environment\n",
      "Episode reward: -528.2029586164281\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09393489 -0.05073796 -0.0416523   0.00364732 -0.00939614  0.04179488]\n",
      "Reset environment\n",
      "Episode reward: 1407.3124281167984\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09396888 -0.05082426 -0.04166833  0.00364396 -0.0094479   0.04178308]\n",
      "Reset environment\n",
      "Episode reward: 23.989053666591644\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0939822  -0.05082322 -0.04168414  0.00365741 -0.00943929  0.04177412]\n",
      "Reset environment\n",
      "Episode reward: 196.31621354818344\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09399238 -0.0508234  -0.04169825  0.00366531 -0.00942888  0.04176368]\n",
      "Reset environment\n",
      "Episode reward: -352.4530948586762\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09400799 -0.05083184 -0.04170844  0.00367074 -0.00942562  0.04174417]\n",
      "Reset environment\n",
      "Episode reward: -7599.460911076516\n",
      "Total Steps: 1278\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09373564 -0.0505219  -0.04162037  0.00351625 -0.00933569  0.04205831]\n",
      "Reset environment\n",
      "Episode reward: 140.70100757479668\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0937537  -0.05051887 -0.0416296   0.00352395 -0.00933709  0.04204335]\n",
      "Reset environment\n",
      "Episode reward: -79.07658886909485\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09376588 -0.05052074 -0.04164172  0.00353509 -0.00932772  0.04203128]\n",
      "Reset environment\n",
      "Episode reward: 294.59258818626404\n",
      "Total Steps: 12\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0937655  -0.05052635 -0.04164121  0.00353453 -0.00932928  0.04203121]\n",
      "Reset environment\n",
      "Episode reward: -1413.6833354830742\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09377222 -0.05058583 -0.04164675  0.00353015 -0.00934984  0.04203442]\n",
      "Reset environment\n",
      "Episode reward: -557.6756104651431\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09379046 -0.05059423 -0.04165613  0.00354066 -0.00934666  0.04201318]\n",
      "Reset environment\n",
      "Episode reward: -19838.266591145657\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09353033 -0.05026775 -0.04159434  0.00343948 -0.0092125   0.04218593]\n",
      "Reset environment\n",
      "Episode reward: 1260.181912779808\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09355462 -0.05026547 -0.0416103   0.00344707 -0.00921347  0.04217324]\n",
      "Reset environment\n",
      "Episode reward: -463.3474728312576\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09356006 -0.05027432 -0.04161244  0.00344118 -0.00920673  0.04214886]\n",
      "Reset environment\n",
      "Episode reward: -1927.1733879446983\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0935911  -0.05027881 -0.04162269  0.0034522  -0.00922483  0.0421497 ]\n",
      "Reset environment\n",
      "Episode reward: 344.82896983623505\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09359068 -0.05029945 -0.04162231  0.0034546  -0.00923052  0.04214811]\n",
      "Reset environment\n",
      "Episode reward: -868.6474073529243\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09359907 -0.05030753 -0.0416323   0.0034541  -0.00922475  0.04213862]\n",
      "Reset environment\n",
      "Episode reward: -394.87568640708923\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09361191 -0.05031285 -0.04163881  0.00346238 -0.00921742  0.04212017]\n",
      "Reset environment\n",
      "Episode reward: -65.16454672813416\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09362415 -0.0503146  -0.04165094  0.00347327 -0.00920975  0.04210894]\n",
      "Reset environment\n",
      "Episode reward: -353.54235687851906\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09364083 -0.05032432 -0.04166222  0.00348071 -0.00920671  0.04209041]\n",
      "Reset environment\n",
      "Episode reward: -168.4242826104164\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09365325 -0.0503329  -0.04166857  0.00348373 -0.00920368  0.04206934]\n",
      "Reset environment\n",
      "Episode reward: -144.0942433476448\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09366465 -0.05033021 -0.0416822   0.00349832 -0.00919461  0.04206261]\n",
      "Reset environment\n",
      "Episode reward: 220.9730158150196\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09367678 -0.05033508 -0.04169728  0.00350478 -0.00918764  0.0420526 ]\n",
      "Reset environment\n",
      "Episode reward: -161.33591892197728\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09368265 -0.05034111 -0.04170322  0.00350201 -0.00918016  0.04203356]\n",
      "Reset environment\n",
      "Episode reward: 95.2312775850296\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09368491 -0.05038642 -0.04170535  0.00350123 -0.00919046  0.04203015]\n",
      "Reset environment\n",
      "Episode reward: 303.0444105863571\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09369161 -0.05038672 -0.04171909  0.00350672 -0.00917966  0.04202212]\n",
      "Reset environment\n",
      "Episode reward: -261.70140451192856\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0937015  -0.05039196 -0.04172368  0.00351035 -0.0091748   0.04200332]\n",
      "Reset environment\n",
      "Episode reward: -179.51758861541748\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09371141 -0.05039086 -0.04173503  0.00352236 -0.00916552  0.0419947 ]\n",
      "Reset environment\n",
      "Episode reward: 1458.3589463792741\n",
      "Total Steps: 260\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09378175 -0.05039061 -0.04178238  0.00350772 -0.00921192  0.04197817]\n",
      "Reset environment\n",
      "Episode reward: 1433.031822502613\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09380458 -0.05038764 -0.041797    0.0035104  -0.00922156  0.04196769]\n",
      "Reset environment\n",
      "Episode reward: 1181.3732860684395\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09380362 -0.05037199 -0.04180553  0.00350018 -0.00920838  0.04194956]\n",
      "Reset environment\n",
      "Episode reward: -192.24056947231293\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09380932 -0.05042683 -0.04180444  0.0035009  -0.00921943  0.04194459]\n",
      "Reset environment\n",
      "Episode reward: 511.01944160461426\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09382682 -0.05042844 -0.04181132  0.00350378 -0.00922646  0.04192685]\n",
      "Reset environment\n",
      "Episode reward: 1249.877967506647\n",
      "Total Steps: 775\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09374701 -0.05040823 -0.04175926  0.00340077 -0.00923189  0.0420559 ]\n",
      "Reset environment\n",
      "Episode reward: -545.3752381801605\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09375735 -0.0504117  -0.04176483  0.00341025 -0.00922243  0.04204061]\n",
      "Reset environment\n",
      "Episode reward: 487.9699465036392\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09375739 -0.05042842 -0.04176448  0.00341146 -0.00922682  0.04203947]\n",
      "Reset environment\n",
      "Episode reward: -227.25171179324389\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09376435 -0.05043663 -0.0417676   0.00340856 -0.00921955  0.0420172 ]\n",
      "Reset environment\n",
      "Episode reward: -292.0425880062394\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09377092 -0.05043872 -0.041775    0.00341239 -0.00921138  0.04200337]\n",
      "Reset environment\n",
      "Episode reward: 538.8059565424919\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09379263 -0.05044131 -0.04178095  0.00341917 -0.00921134  0.04198215]\n",
      "Reset environment\n",
      "Episode reward: -419.66727735847235\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09380154 -0.05044807 -0.0417831   0.00342128 -0.00920601  0.04196163]\n",
      "Reset environment\n",
      "Episode reward: 785.4797559380531\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09381711 -0.0504409  -0.04179807  0.00342429 -0.00921158  0.04195426]\n",
      "Reset environment\n",
      "Episode reward: -238.3674814105034\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09382864 -0.05044807 -0.04180316  0.00342893 -0.00920942  0.04193538]\n",
      "Reset environment\n",
      "Episode reward: 147.59918040037155\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09383738 -0.05044707 -0.04181538  0.00343796 -0.00920052  0.04192827]\n",
      "Reset environment\n",
      "Episode reward: -561.7973163640127\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09385325 -0.05045479 -0.04182462  0.00344742 -0.00919705  0.04191118]\n",
      "Reset environment\n",
      "Episode reward: -21661.037425551564\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09359523 -0.05027641 -0.04166039  0.00353544 -0.00920766  0.04218988]\n",
      "Reset environment\n",
      "Episode reward: -19971.599818057788\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09138379 -0.05323787 -0.04599665  0.01411659 -0.01940869  0.03795183]\n",
      "Reset environment\n",
      "Episode reward: -5652.527540169656\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13911939 -0.28110597 -0.27561033 -0.18671055 -0.24157438 -0.19420329]\n",
      "Reset environment\n",
      "Episode reward: -21.77416229248047\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12966533 -0.2714631  -0.26563925 -0.17284727 -0.2350947  -0.18458374]\n",
      "Reset environment\n",
      "Episode reward: -3438.256954714656\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13490751 -0.27617976 -0.27004808 -0.1740851  -0.24203931 -0.18959865]\n",
      "Reset environment\n",
      "Episode reward: -280.737005636096\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12959051 -0.27065378 -0.26431444 -0.16528727 -0.23905744 -0.18413617]\n",
      "Reset environment\n",
      "Episode reward: -929.8095190823078\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12481206 -0.2656143  -0.25893617 -0.15705538 -0.23618264 -0.1789539 ]\n",
      "Reset environment\n",
      "Episode reward: 679.4748405814171\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0838185  -0.22464046 -0.21719985 -0.10927636 -0.20248187 -0.1372897 ]\n",
      "Reset environment\n",
      "Episode reward: -149.63853180408478\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.07712933 -0.2176533  -0.21020919 -0.09865659 -0.19850992 -0.13044614]\n",
      "Reset environment\n",
      "Episode reward: 646.7869578897953\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.05716579 -0.19743194 -0.18967006 -0.07400995 -0.18271184 -0.11000774]\n",
      "Reset environment\n",
      "Episode reward: 18.17131680250168\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0473065  -0.18729502 -0.17946826 -0.06001226 -0.17584465 -0.09993988]\n",
      "Reset environment\n",
      "Episode reward: -265.68373745679855\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.04343678 -0.18311447 -0.17527603 -0.05219967 -0.17434311 -0.09591667]\n",
      "Reset environment\n",
      "Episode reward: 920.3622718676925\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00863843 -0.14827105 -0.13980585 -0.01026264 -0.14637814 -0.06065495]\n",
      "Reset environment\n",
      "Episode reward: 207.87658935785294\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.00366417 -0.13579787 -0.1269147   0.0061073  -0.13727841 -0.04799679]\n",
      "Reset environment\n",
      "Episode reward: 652.2980930805206\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0213733  -0.1180803  -0.10881371  0.02763409 -0.12301572 -0.03010258]\n",
      "Reset environment\n",
      "Episode reward: 352.8996610045433\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03505271 -0.10418991 -0.09464563  0.04528554 -0.1126582  -0.01610361]\n",
      "Reset environment\n",
      "Episode reward: 31.228608787059784\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.04472639 -0.09430362 -0.0847929   0.05902907 -0.10602744 -0.00640128]\n",
      "Reset environment\n",
      "Episode reward: 463.77975875139236\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.06308015 -0.07602111 -0.06601486  0.08281858 -0.09221914  0.01208848]\n",
      "Reset environment\n",
      "Episode reward: 1433.829395890236\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.07841639 -0.06072184 -0.05026881  0.10113902 -0.07966662  0.02759144]\n",
      "Reset environment\n",
      "Episode reward: -2.7322299480438232\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.08639923 -0.05257218 -0.04193647  0.11272663 -0.0743176   0.03574098]\n",
      "Reset environment\n",
      "Episode reward: -9910.564385456964\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.02777527 -0.16518025 -0.15324564  0.00375784 -0.18327276 -0.0800497 ]\n",
      "Reset environment\n",
      "Episode reward: 428.70561361312866\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01228716 -0.14945614 -0.13728848  0.02360881 -0.17126152 -0.0642742 ]\n",
      "Reset environment\n",
      "Episode reward: 649.4906994700432\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01118658 -0.1256863  -0.11341339  0.0532754  -0.1530323  -0.04052424]\n",
      "Reset environment\n",
      "Episode reward: -131.02867011725903\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01821974 -0.1183     -0.1062021   0.06391931 -0.14851305 -0.0334854 ]\n",
      "Reset environment\n",
      "Episode reward: 507.40799432992935\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03473976 -0.10171046 -0.0895224   0.0842749  -0.13546796 -0.0169372 ]\n",
      "Reset environment\n",
      "Episode reward: -176.93489241600037\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0402066  -0.09601042 -0.0837925   0.09330077 -0.1324017  -0.01138612]\n",
      "Reset environment\n",
      "Episode reward: 295.45365810394287\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.05386592 -0.08207598 -0.06972642  0.11036532 -0.12136288  0.0023958 ]\n",
      "Reset environment\n",
      "Episode reward: -9641.409245582297\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.18084204 -0.31578457 -0.30019498 -0.1366545  -0.33217037 -0.23389846]\n",
      "Reset environment\n",
      "Episode reward: -33.74499171972275\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.17251861 -0.30724287 -0.29160124 -0.12476337 -0.32648218 -0.22553065]\n",
      "Reset environment\n",
      "Episode reward: 2950.1620809286833\n",
      "Total Steps: 211\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13029517 -0.26506788 -0.24870121 -0.0758583  -0.29084212 -0.18283582]\n",
      "Reset environment\n",
      "Episode reward: -80.10086974175647\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12408819 -0.258738   -0.24201618 -0.06689087 -0.2864521  -0.1764029 ]\n",
      "Reset environment\n",
      "Episode reward: -382.5518055856228\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12066904 -0.25521606 -0.23805478 -0.06218492 -0.2837729  -0.17283283]\n",
      "Reset environment\n",
      "Episode reward: 3311.8901509046555\n",
      "Total Steps: 284\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0856181  -0.22073258 -0.20159335 -0.02122284 -0.25459328 -0.13734853]\n",
      "Reset environment\n",
      "Episode reward: -197.40807335078716\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0813852  -0.21659735 -0.19653194 -0.01516601 -0.2516336  -0.13275416]\n",
      "Reset environment\n",
      "Episode reward: -72.31383258104324\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.07402464 -0.209027   -0.18904474 -0.00420414 -0.24669921 -0.12535913]\n",
      "Reset environment\n",
      "Episode reward: 623.103824198246\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.05655751 -0.19145986 -0.17139098  0.01695319 -0.23258625 -0.10779873]\n",
      "Reset environment\n",
      "Episode reward: 1040.4943432211876\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.03054342 -0.16542396 -0.14488935  0.04808325 -0.21117936 -0.08155245]\n",
      "Reset environment\n",
      "Episode reward: -8869.23987014126\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24295947 -0.371011   -0.3525913  -0.20529167 -0.3784686  -0.29596242]\n",
      "Reset environment\n",
      "Episode reward: 3.962710738182068\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.23403955 -0.36192906 -0.34341776 -0.19499409 -0.37085417 -0.28695726]\n",
      "Reset environment\n",
      "Episode reward: 73.73037452995777\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.22387968 -0.35181287 -0.33278126 -0.1821738  -0.36282668 -0.27652392]\n",
      "Reset environment\n",
      "Episode reward: 942.6959917545319\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2026369  -0.33042985 -0.31123322 -0.15674269 -0.34544358 -0.25505245]\n",
      "Reset environment\n",
      "Episode reward: 3292.5647428035736\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.15214328 -0.28025758 -0.26013282 -0.09956889 -0.30216414 -0.20437863]\n",
      "Reset environment\n",
      "Episode reward: -166.3553824974224\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.14623815 -0.27402192 -0.25423625 -0.09099384 -0.29825783 -0.19847263]\n",
      "Reset environment\n",
      "Episode reward: -64.37193137407303\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13921686 -0.26665974 -0.24703489 -0.08077807 -0.29353616 -0.19136234]\n",
      "Reset environment\n",
      "Episode reward: 725.0033448338509\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12104816 -0.2484035  -0.2287229  -0.0593042  -0.27859387 -0.17306341]\n",
      "Reset environment\n",
      "Episode reward: 863.5517031550407\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0990481  -0.22632357 -0.2064238  -0.03249903 -0.26082483 -0.15082675]\n",
      "Reset environment\n",
      "Episode reward: 541.4886413945351\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.21603234 -0.33549958 -0.32656106 -0.20011535 -0.3336321  -0.27278456]\n",
      "Reset environment\n",
      "Episode reward: 434.7105582356453\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.200908   -0.32039782 -0.3109351  -0.18240519 -0.3209246  -0.2574762 ]\n",
      "Reset environment\n",
      "Episode reward: -639.0969988424331\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.20237325 -0.3214333  -0.31253743 -0.18199223 -0.32342947 -0.25896   ]\n",
      "Reset environment\n",
      "Episode reward: -222.92491644620895\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.1978425  -0.31656262 -0.30783927 -0.17392138 -0.32117522 -0.25432605]\n",
      "Reset environment\n",
      "Episode reward: 477.6519795060158\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.1828007  -0.30160502 -0.29239944 -0.15575442 -0.30887958 -0.23906718]\n",
      "Reset environment\n",
      "Episode reward: 302.9751386642456\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.17035764 -0.2891556  -0.2797728  -0.14062704 -0.29886281 -0.22657628]\n",
      "Reset environment\n",
      "Episode reward: 342.82817500829697\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.15757522 -0.27611223 -0.26688412 -0.12460519 -0.28869444 -0.2137008 ]\n",
      "Reset environment\n",
      "Episode reward: -320.0382853522897\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.15277015 -0.27077162 -0.26155487 -0.12048373 -0.2835901  -0.20873666]\n",
      "Reset environment\n",
      "Episode reward: 401.7902653813362\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13886175 -0.25678086 -0.24746677 -0.10321663 -0.27270928 -0.19470634]\n",
      "Reset environment\n",
      "Episode reward: 483.0012160539627\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12486277 -0.24273364 -0.23325409 -0.0859194  -0.26149195 -0.18061927]\n",
      "Reset environment\n",
      "Episode reward: -163.48216822743416\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.11911349 -0.23674148 -0.22759824 -0.07791978 -0.2574673  -0.1748223 ]\n",
      "Reset environment\n",
      "Episode reward: 554.2127282619476\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.10275286 -0.22037096 -0.21106604 -0.05820369 -0.24424285 -0.1584028 ]\n",
      "Reset environment\n",
      "Episode reward: -13686.268255196512\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2618602  -0.37271777 -0.37190163 -0.2328337  -0.37570035 -0.3176045 ]\n",
      "Reset environment\n",
      "Episode reward: 559.221399307251\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24636012 -0.3571309  -0.3562573  -0.21429516 -0.36312535 -0.30198893]\n",
      "Reset environment\n",
      "Episode reward: 1171.956661760807\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2220085  -0.33298597 -0.3314237  -0.1856617  -0.34290177 -0.27747294]\n",
      "Reset environment\n",
      "Episode reward: 1060.3873026371002\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.1991639  -0.30998322 -0.3084284  -0.15836309 -0.32424992 -0.25444192]\n",
      "Reset environment\n",
      "Episode reward: -410.30180257745087\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.19597249 -0.30624393 -0.30533472 -0.15239526 -0.32278875 -0.2511845 ]\n",
      "Reset environment\n",
      "Episode reward: 142.6924450583756\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.17988755 -0.28979543 -0.28916696 -0.13285592 -0.3096647  -0.2349766 ]\n",
      "Reset environment\n",
      "Episode reward: -253.1229283809662\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.17582665 -0.28550225 -0.2850463  -0.1255544  -0.30777332 -0.23081838]\n",
      "Reset environment\n",
      "Episode reward: 197.93754887580872\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.16591798 -0.27548558 -0.27509323 -0.11254191 -0.30046517 -0.22085954]\n",
      "Reset environment\n",
      "Episode reward: -1514.0608294345438\n",
      "Total Steps: 250\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.16995616 -0.28101966 -0.2767762  -0.11662818 -0.3042234  -0.22456706]\n",
      "Reset environment\n",
      "Episode reward: -120.98217505216599\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.16415794 -0.27521208 -0.27062628 -0.10793221 -0.3005623  -0.21864323]\n",
      "Reset environment\n",
      "Episode reward: -434.57841062545776\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.16277184 -0.27379176 -0.2688622  -0.10374594 -0.30095285 -0.21713877]\n",
      "Reset environment\n",
      "Episode reward: 1104.3462929725647\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.142224   -0.2532714  -0.24793054 -0.0791961  -0.28418314 -0.19649154]\n",
      "Reset environment\n",
      "Episode reward: -12.604323387145996\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13523267 -0.24615471 -0.24079785 -0.06914907 -0.2795198  -0.18943113]\n",
      "Reset environment\n",
      "Episode reward: 1218.5594828724861\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.11031159 -0.2214806  -0.21541083 -0.04045502 -0.25853142 -0.1643719 ]\n",
      "Reset environment\n",
      "Episode reward: 1483.761881172657\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0828325  -0.1940651  -0.18760721 -0.00883806 -0.23541081 -0.13678083]\n",
      "Reset environment\n",
      "Episode reward: -80.89548192173243\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-7.6728471e-02 -1.8799536e-01 -1.8113257e-01  8.6079679e-05\n",
      " -2.3140003e-01 -1.3057779e-01]\n",
      "Reset environment\n",
      "Episode reward: 61.05683070421219\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.06846994 -0.17982483 -0.17254397  0.01115488 -0.22550927 -0.12223986]\n",
      "Reset environment\n",
      "Episode reward: -8869.748465104029\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.21645926 -0.3280307  -0.31709877 -0.14793512 -0.35081497 -0.26873326]\n",
      "Reset environment\n",
      "Episode reward: -14199.287317618728\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.47445518 -0.5906597  -0.565624   -0.44741777 -0.56206614 -0.5247763 ]\n",
      "Reset environment\n",
      "Episode reward: -18260.906391153112\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7056822  -0.817866   -0.78886974 -0.72876966 -0.7431401  -0.75245154]\n",
      "Reset environment\n",
      "Episode reward: 871.6801139116287\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.68653774 -0.79900366 -0.76933104 -0.70673805 -0.7271991  -0.73322654]\n",
      "Reset environment\n",
      "Episode reward: -7291.582428438123\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7409617  -0.85739356 -0.8176666  -0.77099365 -0.76687014 -0.78629035]\n",
      "Reset environment\n",
      "Episode reward: 1190.6178669333458\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.71856594 -0.8349735  -0.7951943  -0.74527466 -0.74814737 -0.7638413 ]\n",
      "Reset environment\n",
      "Episode reward: 779.2482489347458\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7021545  -0.8184743  -0.7786831  -0.72606117 -0.7345734  -0.7473573 ]\n",
      "Reset environment\n",
      "Episode reward: 1075.7476991415024\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6813474  -0.79769206 -0.75763047 -0.7019848  -0.7172773  -0.72645915]\n",
      "Reset environment\n",
      "Episode reward: -14724.168106794357\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.8320357  -0.9487557  -0.9002502  -0.90131336 -0.825494   -0.8751591 ]\n",
      "Reset environment\n",
      "Episode reward: -185.223614692688\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.82711256 -0.9435257  -0.89548165 -0.8935232  -0.8226378  -0.87017184]\n",
      "Reset environment\n",
      "Episode reward: 706.7088585495949\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.8114209  -0.92789847 -0.87956566 -0.8752631  -0.8096774  -0.8544113 ]\n",
      "Reset environment\n",
      "Episode reward: 119.11667636781931\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.803668   -0.91988075 -0.8718768  -0.86587816 -0.80358166 -0.8465878 ]\n",
      "Reset environment\n",
      "Episode reward: 198.42236667871475\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.79416555 -0.9105082  -0.86205596 -0.8540049  -0.79620296 -0.83697045]\n",
      "Reset environment\n",
      "Episode reward: 572.4387705922127\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7809056  -0.89710194 -0.8487716  -0.8380337  -0.78550625 -0.8236504 ]\n",
      "Reset environment\n",
      "Episode reward: 378.95820301771164\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.76889914 -0.88559103 -0.8359823  -0.8241741  -0.77540886 -0.8115203 ]\n",
      "Reset environment\n",
      "Episode reward: -3477.1221283078194\n",
      "Total Steps: 777\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.8022622  -0.9162717  -0.869007   -0.8673853  -0.80228394 -0.84408635]\n",
      "Reset environment\n",
      "Episode reward: 576.3953308463097\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.78796107 -0.90227437 -0.85423213 -0.8513098  -0.7901149  -0.82969403]\n",
      "Reset environment\n",
      "Episode reward: -374.6634491197765\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7854844  -0.8992554  -0.85191315 -0.84778583 -0.788497   -0.8271071 ]\n",
      "Reset environment\n",
      "Episode reward: 301.83308559656143\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7746179  -0.8885052  -0.8407304  -0.8345337  -0.77987117 -0.81619674]\n",
      "Reset environment\n",
      "Episode reward: -356.3749740791973\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7720072  -0.88548833 -0.8383233  -0.8292545  -0.7790862  -0.81356966]\n",
      "Reset environment\n",
      "Episode reward: -242.9955544769764\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7680252  -0.8811339  -0.83447695 -0.8225651  -0.7770297  -0.8095763 ]\n",
      "Reset environment\n",
      "Episode reward: 634.6791009902954\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75331575 -0.8666295  -0.8194599  -0.80560505 -0.76477766 -0.79480684]\n",
      "Reset environment\n",
      "Episode reward: 2156.02002376318\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.72092444 -0.8347726  -0.78611463 -0.76982564 -0.7366636  -0.76217926]\n",
      "Reset environment\n",
      "Episode reward: 2474.8679338097572\n",
      "Total Steps: 247\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.68722093 -0.80101955 -0.7521967  -0.73190236 -0.7078157  -0.7283142 ]\n",
      "Reset environment\n",
      "Episode reward: 717.1796732544899\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.67253536 -0.7864954  -0.7372092  -0.7151313  -0.69544524 -0.7135453 ]\n",
      "Reset environment\n",
      "Episode reward: -163.2418521642685\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6678623  -0.78178746 -0.73228043 -0.7079534  -0.69249904 -0.70880395]\n",
      "Reset environment\n",
      "Episode reward: 323.92925480753183\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.65970564 -0.7731171  -0.72431976 -0.69830203 -0.68595475 -0.70051   ]\n",
      "Reset environment\n",
      "Episode reward: 768.431930642575\n",
      "Total Steps: 209\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.64091504 -0.75386125 -0.70549476 -0.6768848  -0.67016983 -0.6816105 ]\n",
      "Reset environment\n",
      "Episode reward: 480.8117155432701\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6288653  -0.7416296  -0.6934607  -0.66217166 -0.6605547  -0.6694994 ]\n",
      "Reset environment\n",
      "Episode reward: 157.43164318799973\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6204779 -0.7328915 -0.6852325 -0.6518465 -0.6540141 -0.661051 ]\n",
      "Reset environment\n",
      "Episode reward: 273.5444349050522\n",
      "Total Steps: 24\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.61932474 -0.7316903  -0.68408006 -0.6501781  -0.6532488  -0.65989584]\n",
      "Reset environment\n",
      "Episode reward: -275.0392127322266\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.61538184 -0.72845274 -0.6791179  -0.6457402  -0.6499276  -0.6559044 ]\n",
      "Reset environment\n",
      "Episode reward: -597.8057771511376\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.61564267 -0.72913283 -0.6785785  -0.6458984  -0.6502076  -0.6560872 ]\n",
      "Reset environment\n",
      "Episode reward: 479.18732157722116\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.58773124 -0.70284206 -0.6488696  -0.61604077 -0.62540007 -0.6281421 ]\n",
      "Reset environment\n",
      "Episode reward: 351.133729994297\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.577579   -0.6925585  -0.6386873  -0.60331684 -0.61751235 -0.6179298 ]\n",
      "Reset environment\n",
      "Episode reward: -304.90709137916565\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5741382  -0.6893241  -0.63473076 -0.5994083  -0.61443204 -0.6144438 ]\n",
      "Reset environment\n",
      "Episode reward: -431.85523903463036\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57059664 -0.68539834 -0.63126475 -0.5951564  -0.61153805 -0.61084   ]\n",
      "Reset environment\n",
      "Episode reward: -612.9978272267617\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57083917 -0.6860695  -0.6306883  -0.594213   -0.612419   -0.61101085]\n",
      "Reset environment\n",
      "Episode reward: -11396.363112527877\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7531811  -0.8605833  -0.8140947  -0.82141775 -0.75904924 -0.7923816 ]\n",
      "Reset environment\n",
      "Episode reward: -846.3956297636032\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75210005 -0.85968465 -0.8123638  -0.8211336  -0.7575303  -0.7911885 ]\n",
      "Reset environment\n",
      "Episode reward: 1083.4942836761475\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.73413604 -0.84148985 -0.79449993 -0.80079615 -0.7423103  -0.77315253]\n",
      "Reset environment\n",
      "Episode reward: -440.46212419867516\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7338863  -0.8404374  -0.7944972  -0.8009535  -0.74168247 -0.77287704]\n",
      "Reset environment\n",
      "Episode reward: 1201.6886940002441\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7127886  -0.81896114 -0.7734874  -0.77652717 -0.7241025  -0.7517235 ]\n",
      "Reset environment\n",
      "Episode reward: -4036.7930408644024\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7495532  -0.85523146 -0.8086569  -0.8194442  -0.75462735 -0.78807396]\n",
      "Reset environment\n",
      "Episode reward: 1098.189545094967\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.73034596 -0.8358648  -0.78956395 -0.7968591  -0.7388439  -0.76882136]\n",
      "Reset environment\n",
      "Episode reward: 244.60139220952988\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.72128594 -0.8268242  -0.7802199  -0.7855035  -0.731793   -0.7596867 ]\n",
      "Reset environment\n",
      "Episode reward: 316.96454071998596\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7112732  -0.8169258  -0.7699729  -0.7732472  -0.7237841  -0.74963236]\n",
      "Reset environment\n",
      "Episode reward: 5.513196237385273\n",
      "Total Steps: 786\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7207111  -0.8277677  -0.77604896 -0.791092   -0.7285001  -0.75879854]\n",
      "Reset environment\n",
      "Episode reward: -263.5973161458969\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7170224  -0.82405514 -0.77223766 -0.7848642  -0.7265992  -0.7550601 ]\n",
      "Reset environment\n",
      "Episode reward: -125.18408073671162\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7119694  -0.8186388  -0.76738477 -0.77818245 -0.72293156 -0.7499599 ]\n",
      "Reset environment\n",
      "Episode reward: 493.10762643814087\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.700054   -0.80694515 -0.7551603  -0.7643978  -0.7131005  -0.738037  ]\n",
      "Reset environment\n",
      "Episode reward: 1242.9900807738304\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6801091  -0.7869188  -0.73519284 -0.74124765 -0.69654197 -0.7180573 ]\n",
      "Reset environment\n",
      "Episode reward: 260.7511880546808\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6745611  -0.78072846 -0.72996366 -0.73497874 -0.69185215 -0.7124948 ]\n",
      "Reset environment\n",
      "Episode reward: 4754.357457280159\n",
      "Total Steps: 213\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.62503976 -0.7319197  -0.6796649  -0.6808412  -0.64848256 -0.66295624]\n",
      "Reset environment\n",
      "Episode reward: -14.629962235689163\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.61929435 -0.72585475 -0.67403275 -0.67272216 -0.6445513  -0.6571724 ]\n",
      "Reset environment\n",
      "Episode reward: -67.88672286272049\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.61462396 -0.72086066 -0.669596   -0.6673655  -0.64075655 -0.6524865 ]\n",
      "Reset environment\n",
      "Episode reward: -12725.063084304333\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.70253587 -0.8047747  -0.75750023 -0.79095334 -0.7059491  -0.73995143]\n",
      "Reset environment\n",
      "Episode reward: 3600.8140873908997\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.657538   -0.7597055  -0.7122265  -0.74132925 -0.6666855  -0.69471365]\n",
      "Reset environment\n",
      "Episode reward: -333.37502167839557\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.655044   -0.7575749  -0.709172   -0.73799074 -0.6648423  -0.692204  ]\n",
      "Reset environment\n",
      "Episode reward: -176.6149428775534\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.65031654 -0.7536477  -0.70343983 -0.7325659  -0.66088295 -0.68740934]\n",
      "Reset environment\n",
      "Episode reward: -67.97891995310783\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.64519286 -0.74867094 -0.6980541  -0.72522455 -0.6574001  -0.682257  ]\n",
      "Reset environment\n",
      "Episode reward: -531.4209756366909\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6470797  -0.7495939  -0.70045    -0.7279888  -0.65865946 -0.68407536]\n",
      "Reset environment\n",
      "Episode reward: 485.16907623410225\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6359171  -0.738083   -0.6894889  -0.7155522  -0.6491517  -0.67285126]\n",
      "Reset environment\n",
      "Episode reward: -346.60365891456604\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.63363373 -0.735371   -0.68749464 -0.7122958  -0.6476932  -0.6705444 ]\n",
      "Reset environment\n",
      "Episode reward: 902.9197806715965\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.626401   -0.72810316 -0.68023384 -0.70378107 -0.641771   -0.66327876]\n",
      "Reset environment\n",
      "Episode reward: -350.2185667157173\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.62414    -0.72559065 -0.67811203 -0.6991158  -0.64107025 -0.6610257 ]\n",
      "Reset environment\n",
      "Episode reward: -1024.4213654156774\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6309432  -0.7315561  -0.6852196  -0.7062538  -0.6470325  -0.66779417]\n",
      "Reset environment\n",
      "Episode reward: -165.12579995393753\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.62682414 -0.72765917 -0.6807304  -0.70039743 -0.6442156  -0.66361284]\n",
      "Reset environment\n",
      "Episode reward: -87.70785176753998\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6218536  -0.7224825  -0.6758957  -0.69302475 -0.6411532  -0.6586388 ]\n",
      "Reset environment\n",
      "Episode reward: 1495.462825536728\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.60026646 -0.7009155  -0.6541207  -0.66868544 -0.62266135 -0.63699424]\n",
      "Reset environment\n",
      "Episode reward: -850.4437040090561\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.60288405 -0.70260507 -0.65719557 -0.6724807  -0.6242913  -0.6396325 ]\n",
      "Reset environment\n",
      "Episode reward: 2393.508337020874\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57953846 -0.67903554 -0.63384086 -0.64515394 -0.6049444  -0.616239  ]\n",
      "Reset environment\n",
      "Episode reward: -238.7719248533249\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57628644 -0.67567235 -0.63060176 -0.6392311  -0.6035317  -0.6129838 ]\n",
      "Reset environment\n",
      "Episode reward: 1449.7750850915909\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.55500907 -0.65378064 -0.60965353 -0.6157815  -0.58507717 -0.5916392 ]\n",
      "Reset environment\n",
      "Episode reward: 293.07789158821106\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5463246  -0.64485973 -0.6011094  -0.6049296  -0.57832336 -0.5829174 ]\n",
      "Reset environment\n",
      "Episode reward: -11186.222798489034\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6561559  -0.74950594 -0.71131057 -0.777399   -0.6492517  -0.6915999 ]\n",
      "Reset environment\n",
      "Episode reward: 337.49947333335876\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.64685106 -0.73997056 -0.70210755 -0.76594764 -0.64190525 -0.68223107]\n",
      "Reset environment\n",
      "Episode reward: -3834.9444078551605\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6551236 -0.7448042 -0.7088706 -0.8434604 -0.6141764 -0.6898054]\n",
      "Reset environment\n",
      "Episode reward: -792.021098613739\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6657255 -0.7533488 -0.7204319 -0.8554679 -0.6226443 -0.7003349]\n",
      "Reset environment\n",
      "Episode reward: -9690.446019411087\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.74081665 -0.82734376 -0.7916766  -1.0035226  -0.6576714  -0.77371895]\n",
      "Reset environment\n",
      "Episode reward: -602.7894338965416\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7386     -0.82442933 -0.78983295 -1.0005013  -0.65606034 -0.7715301 ]\n",
      "Reset environment\n",
      "Episode reward: 53.88346368074417\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7326087 -0.8181445 -0.7840429 -0.9922739 -0.6518826 -0.7655143]\n",
      "Reset environment\n",
      "Episode reward: -617.9373383979837\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.732971   -0.81890935 -0.7838193  -0.991558   -0.6528576  -0.7658202 ]\n",
      "Reset environment\n",
      "Episode reward: 868.4169595241547\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.71885574 -0.80485475 -0.7695251  -0.97515506 -0.64113545 -0.75166416]\n",
      "Reset environment\n",
      "Episode reward: -349.8642225547228\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7167861  -0.80239666 -0.7677685  -0.97240996 -0.63981336 -0.7495626 ]\n",
      "Reset environment\n",
      "Episode reward: 4727.029059767723\n",
      "Total Steps: 222\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.67568386 -0.7614831  -0.7261351  -0.9268478  -0.60398597 -0.70839655]\n",
      "Reset environment\n",
      "Episode reward: 436.8226932287216\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6663129  -0.7519693  -0.71670485 -0.91408116 -0.5971521  -0.698939  ]\n",
      "Reset environment\n",
      "Episode reward: 6295.768853962421\n",
      "Total Steps: 340\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6074916  -0.69251066 -0.6581767  -0.8504447  -0.5457721  -0.6400343 ]\n",
      "Reset environment\n",
      "Episode reward: 1199.0762696266174\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.59096867 -0.6759224  -0.6416335  -0.8322027  -0.5315711  -0.62347645]\n",
      "Reset environment\n",
      "Episode reward: -568.6700535658747\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5918424  -0.6763955  -0.64284694 -0.83270985 -0.53283244 -0.6243312 ]\n",
      "Reset environment\n",
      "Episode reward: -341.08186184614897\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5894893  -0.6736257  -0.64081365 -0.829376   -0.5313531  -0.6219443 ]\n",
      "Reset environment\n",
      "Episode reward: 720.5294792056084\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57744193 -0.6615982  -0.6287025  -0.8154583  -0.5213564  -0.60988516]\n",
      "Reset environment\n",
      "Episode reward: 33.82307082414627\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5719703  -0.6564006  -0.6228498  -0.8082399  -0.5173125  -0.60437787]\n",
      "Reset environment\n",
      "Episode reward: -5025.32107552942\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.60037696 -0.6822518  -0.6516607  -0.9105974  -0.5110219  -0.631961  ]\n",
      "Reset environment\n",
      "Episode reward: 2463.419453263283\n",
      "Total Steps: 226\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57135475 -0.6525471  -0.62291956 -0.87655854 -0.48674646 -0.60295445]\n",
      "Reset environment\n",
      "Episode reward: 25.836713075637817\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.56597936 -0.6468168  -0.61776245 -0.8702711  -0.4822758  -0.59757304]\n",
      "Reset environment\n",
      "Episode reward: 5398.368090033531\n",
      "Total Steps: 232\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.51727134 -0.59828997 -0.5687567  -0.8163309  -0.44013223 -0.5488045 ]\n",
      "Reset environment\n",
      "Episode reward: 1001.5009599924088\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.50297916 -0.58391947 -0.5544891  -0.8002139  -0.4280524  -0.5344596 ]\n",
      "Reset environment\n",
      "Episode reward: -219.48949190974236\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.49985537 -0.58048856 -0.5515734  -0.79485047 -0.4264446  -0.5312809 ]\n",
      "Reset environment\n",
      "Episode reward: -8540.75360994041\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5698803  -0.6459334  -0.62217087 -0.89392924 -0.48068368 -0.600764  ]\n",
      "Reset environment\n",
      "Episode reward: 141.38832515478134\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.56543696 -0.64050794 -0.6184351  -0.8887635  -0.4769776  -0.5962871 ]\n",
      "Reset environment\n",
      "Episode reward: 1910.0461252331734\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.55079687 -0.6259163  -0.60369164 -0.8718361  -0.46476483 -0.58165836]\n",
      "Reset environment\n",
      "Episode reward: -11914.288497924805\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6596175 -0.7340638 -0.7116577 -1.0376174 -0.5417548 -0.6903946]\n",
      "Reset environment\n",
      "Episode reward: -4932.853539340547\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.77301645 -0.8485776  -0.8207893  -1.2230778  -0.61428225 -0.80297357]\n",
      "Reset environment\n",
      "Episode reward: 744.6186583042145\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.76133114 -0.83650976 -0.80935055 -1.209309   -0.6047047  -0.791267  ]\n",
      "Reset environment\n",
      "Episode reward: 1012.6355099081993\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7477319  -0.8230548  -0.7956219  -1.1940742  -0.5932054  -0.77766526]\n",
      "Reset environment\n",
      "Episode reward: 1464.0565565228462\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7422882  -0.8163356  -0.79105115 -1.1870922  -0.58875763 -0.77219975]\n",
      "Reset environment\n",
      "Episode reward: -261.55951803922653\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7394604 -0.8135168 -0.7880595 -1.1815885 -0.5875613 -0.7692952]\n",
      "Reset environment\n",
      "Episode reward: 602.7248375415802\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7305443  -0.8048802  -0.7786589  -1.1713842  -0.5799512  -0.76035374]\n",
      "Reset environment\n",
      "Episode reward: 64.00063800811768\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7234587  -0.79827166 -0.77094233 -1.1639622  -0.57366765 -0.7532814 ]\n",
      "Reset environment\n",
      "Episode reward: 422.82595163583755\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7146221  -0.7894588  -0.7620336  -1.1529002  -0.5668042  -0.74442136]\n",
      "Reset environment\n",
      "Episode reward: 1211.2065994143486\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6987232  -0.7735661  -0.74601066 -1.1339766  -0.55376476 -0.7285229 ]\n",
      "Reset environment\n",
      "Episode reward: 670.4471276402473\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6878797 -0.7631317 -0.7346713 -1.1217724 -0.5445267 -0.7176412]\n",
      "Reset environment\n",
      "Episode reward: 860.5326321646571\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6761266  -0.7522503  -0.7219241  -1.1091192  -0.53423834 -0.7059399 ]\n",
      "Reset environment\n",
      "Episode reward: -12.77290977910161\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.67109925 -0.7468397  -0.71716565 -1.1028856  -0.5303596  -0.7008606 ]\n",
      "Reset environment\n",
      "Episode reward: 472.8736587166786\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6622693  -0.7381829  -0.7080963  -1.0923109  -0.5231811  -0.69198817]\n",
      "Reset environment\n",
      "Episode reward: -8969.638771153986\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7082122  -0.78369135 -0.7532462  -1.1802522  -0.5498565  -0.7377313 ]\n",
      "Reset environment\n",
      "Episode reward: 675.8484725356102\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6979422 -0.7733189 -0.7430701 -1.1681772 -0.5414441 -0.7274339]\n",
      "Reset environment\n",
      "Episode reward: -496.15922942664474\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6971876  -0.7730239  -0.7417648  -1.1666     -0.5412435  -0.72667325]\n",
      "Reset environment\n",
      "Episode reward: 25.727952539920807\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6923253  -0.7677853  -0.73722434 -1.1610017  -0.53729993 -0.7217699 ]\n",
      "Reset environment\n",
      "Episode reward: 637.9448644518852\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.682087   -0.7571972  -0.7273134  -1.1495847  -0.52860487 -0.71154565]\n",
      "Reset environment\n",
      "Episode reward: 1419.4975868463516\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6649297  -0.7398968  -0.71018255 -1.12959    -0.51425934 -0.6943378 ]\n",
      "Reset environment\n",
      "Episode reward: 952.0158261060715\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6519293  -0.7267112  -0.6973528  -1.114929   -0.50330806 -0.6813088 ]\n",
      "Reset environment\n",
      "Episode reward: 1371.7761949896812\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.63495797 -0.70998716 -0.68006504 -1.0954771  -0.4890545  -0.6643471 ]\n",
      "Reset environment\n",
      "Episode reward: 1004.3889335393906\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6216378  -0.6961951  -0.6671336  -1.0807574  -0.47760427 -0.6510386 ]\n",
      "Reset environment\n",
      "Episode reward: -9508.256189852953\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6892038  -0.75637937 -0.73899966 -1.1941495  -0.52145725 -0.71779615]\n",
      "Reset environment\n",
      "Episode reward: -10411.15810803976\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.77301675 -0.83650625 -0.8235603  -1.3349036  -0.5765878  -0.80091196]\n",
      "Reset environment\n",
      "Episode reward: 488.8547440767288\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7642526  -0.8275018  -0.81495005 -1.3242787  -0.5695694  -0.7921241 ]\n",
      "Reset environment\n",
      "Episode reward: 612.3312277793884\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75439876 -0.8173802  -0.80529016 -1.3127296  -0.5614691  -0.7822337 ]\n",
      "Reset environment\n",
      "Episode reward: 869.0392810702324\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7424547  -0.8053931  -0.79332376 -1.2986612  -0.55161864 -0.7702477 ]\n",
      "Reset environment\n",
      "Episode reward: -6370.782092779875\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7633031  -0.8315663  -0.80851823 -1.3697803  -0.5561902  -0.7917452 ]\n",
      "Reset environment\n",
      "Episode reward: 427.85574519634247\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75532454 -0.82344484 -0.80064124 -1.359811   -0.54996836 -0.7837428 ]\n",
      "Reset environment\n",
      "Episode reward: -175.89711585640907\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75197315 -0.8200614  -0.7972286  -1.3537058  -0.54829925 -0.7803489 ]\n",
      "Reset environment\n",
      "Episode reward: 695.7093496322632\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7415771  -0.8096323  -0.78678846 -1.341424   -0.5397929  -0.7699407 ]\n",
      "Reset environment\n",
      "Episode reward: -5648.131552264094\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.782002   -0.8470277  -0.8276514  -1.4244202  -0.561493   -0.80955356]\n",
      "Reset environment\n",
      "Episode reward: 354.5208352804184\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.77482414 -0.8401999  -0.82001686 -1.4161781  -0.55544966 -0.8023474 ]\n",
      "Reset environment\n",
      "Episode reward: 988.3247756958008\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7626829  -0.8276913  -0.80815524 -1.4023257  -0.5452042  -0.79019153]\n",
      "Reset environment\n",
      "Episode reward: 246.68298768997192\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75626856 -0.8214499  -0.80150366 -1.3940674  -0.540281   -0.78377813]\n",
      "Reset environment\n",
      "Episode reward: 720.2400913238525\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.74621105 -0.8109393  -0.7917713  -1.3823923  -0.5318227  -0.77370137]\n",
      "Reset environment\n",
      "Episode reward: -179.31944063305855\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7486762  -0.8123543  -0.7949277  -1.3834409  -0.5346156  -0.77606404]\n",
      "Reset environment\n",
      "Episode reward: 529.771567761898\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.74018985 -0.8041047  -0.7861282  -1.3734254  -0.5276441  -0.7675433 ]\n",
      "Reset environment\n",
      "Episode reward: -174.97380444407463\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.73797923 -0.80239797 -0.78318405 -1.3712592  -0.5254482  -0.7653148 ]\n",
      "Reset environment\n",
      "Episode reward: 574.8254947662354\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.73098695 -0.79421985 -0.7768968  -1.3625017  -0.5196202  -0.7582114 ]\n",
      "Reset environment\n",
      "Episode reward: -5981.7145483493805\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.77453333 -0.8371364  -0.8186205  -1.4482763  -0.54406863 -0.8014328 ]\n",
      "Reset environment\n",
      "Episode reward: 804.8489767313004\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7636173  -0.82562685 -0.80815786 -1.435682   -0.5348627  -0.7904794 ]\n",
      "Reset environment\n",
      "Episode reward: -22.08525161445141\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7592705  -0.82167864 -0.80335796 -1.4302621  -0.5314715  -0.7861118 ]\n",
      "Reset environment\n",
      "Episode reward: 424.065044939518\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.75156283 -0.8140717  -0.7954963  -1.4206177  -0.52549225 -0.77838635]\n",
      "Reset environment\n",
      "Episode reward: 508.58792465925217\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.74335974 -0.80568814 -0.7874228  -1.410592   -0.5189723  -0.7701691 ]\n",
      "Reset environment\n",
      "Episode reward: 399.9655709974468\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.73237485 -0.79522693 -0.7757012  -1.3970864  -0.5099381  -0.75912434]\n",
      "Reset environment\n",
      "Episode reward: 1087.2748439311981\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.71968883 -0.7826673  -0.7627981  -1.3819413  -0.49946916 -0.7464202 ]\n",
      "Reset environment\n",
      "Episode reward: 1371.3966130018234\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.7066624  -0.7703447  -0.74880284 -1.3673711  -0.48789167 -0.73332137]\n",
      "Reset environment\n",
      "Episode reward: 4366.674305617809\n",
      "Total Steps: 407\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.67195153 -0.7370285  -0.7124837  -1.3310643  -0.45703745 -0.69861263]\n",
      "Reset environment\n",
      "Episode reward: -894.2301043826155\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.67791295 -0.7403419  -0.7192431  -1.365196   -0.45214874 -0.7044436 ]\n",
      "Reset environment\n",
      "Episode reward: 1004.773500919342\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.66599494 -0.7281681  -0.7075523  -1.351937   -0.4419783  -0.6925053 ]\n",
      "Reset environment\n",
      "Episode reward: 3769.697516371729\n",
      "Total Steps: 1727\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.6166834  -0.6750247  -0.66026044 -1.3248297  -0.39115214 -0.6427783 ]\n",
      "Reset environment\n",
      "Episode reward: 3418.430563390255\n",
      "Total Steps: 217\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.58629006 -0.643676   -0.63070047 -1.2912002  -0.36408013 -0.61233026]\n",
      "Reset environment\n",
      "Episode reward: -2173.032285541296\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.59634435 -0.65266746 -0.64144    -1.3011664  -0.3727847  -0.62230307]\n",
      "Reset environment\n",
      "Episode reward: -348.6867641210556\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.59467936 -0.6506941  -0.6401073  -1.2984349  -0.37192324 -0.6206546 ]\n",
      "Reset environment\n",
      "Episode reward: 200.67584557831287\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.58923477 -0.64553237 -0.63424104 -1.2918924  -0.3673106  -0.61516607]\n",
      "Reset environment\n",
      "Episode reward: 503.44208043813705\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5814015  -0.63762873 -0.62646025 -1.2821031  -0.36122167 -0.60731447]\n",
      "Reset environment\n",
      "Episode reward: 914.7959468364716\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5706759  -0.62676966 -0.6157873  -1.2697095  -0.35230508 -0.5965662 ]\n",
      "Reset environment\n",
      "Episode reward: 103.98259887099266\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5657577  -0.62204295 -0.61060643 -1.2628351  -0.34884644 -0.59164345]\n",
      "Reset environment\n",
      "Episode reward: 561.6866064071655\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5573183  -0.61373866 -0.6019601  -1.2527759  -0.34199217 -0.5832034 ]\n",
      "Reset environment\n",
      "Episode reward: -1513.5372567651793\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5677603  -0.617347   -0.6164016  -1.2834004  -0.34139848 -0.5929703 ]\n",
      "Reset environment\n",
      "Episode reward: 3536.7229758501053\n",
      "Total Steps: 226\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5397266  -0.5888565  -0.5885833  -1.2515391  -0.31699213 -0.5648936 ]\n",
      "Reset environment\n",
      "Episode reward: -10113.965290859342\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.59497535 -0.6417456  -0.645756   -1.3467948  -0.35600334 -0.62033665]\n",
      "Reset environment\n",
      "Episode reward: 783.5261496305466\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.58546853 -0.63213825 -0.6363235  -1.3356309  -0.34820098 -0.61082226]\n",
      "Reset environment\n",
      "Episode reward: 1290.8322265148163\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.57187    -0.6188137  -0.6223402  -1.3203784  -0.33644986 -0.59719545]\n",
      "Reset environment\n",
      "Episode reward: 576.0862243771553\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5639207  -0.61099654 -0.61422133 -1.3107855  -0.33002907 -0.5892642 ]\n",
      "Reset environment\n",
      "Episode reward: 204.19999480247498\n",
      "Total Steps: 3\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5638877 -0.610963  -0.6141799 -1.3106942 -0.3300222 -0.5892304]\n",
      "Reset environment\n",
      "Episode reward: 220.15516965091228\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.55843097 -0.60585165 -0.6083078  -1.3040321  -0.325592   -0.58375096]\n",
      "Reset environment\n",
      "Episode reward: 385.31231194734573\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5518784  -0.5992531  -0.6017751  -1.2955556  -0.32063568 -0.5771818 ]\n",
      "Reset environment\n",
      "Episode reward: 1490.4618111848831\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.53696245 -0.5840512  -0.587107   -1.278502   -0.30799013 -0.562248  ]\n",
      "Reset environment\n",
      "Episode reward: 508.8453389406204\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.53024065 -0.577591   -0.5799198  -1.2707293  -0.30200785 -0.55542344]\n",
      "Reset environment\n",
      "Episode reward: 947.5277261734009\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5195952  -0.5665203  -0.5696525  -1.2587403  -0.2929061  -0.54478586]\n",
      "Reset environment\n",
      "Episode reward: -657.3607012149878\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5206575  -0.56686795 -0.5713192  -1.2591698  -0.2941538  -0.5458195 ]\n",
      "Reset environment\n",
      "Episode reward: 878.4427132606506\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.5104918  -0.55690676 -0.5609055  -1.2475842  -0.2855525  -0.53564465]\n",
      "Reset environment\n",
      "Episode reward: -845.0929230339825\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.51404876 -0.5592761  -0.5652796  -1.2503985  -0.28869605 -0.53909326]\n",
      "Reset environment\n",
      "Episode reward: 147.55701734498143\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.508851   -0.5537819  -0.5603835  -1.2441406  -0.28455222 -0.53387785]\n",
      "Reset environment\n",
      "Episode reward: 1635.3389486670494\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.49351355 -0.5386181  -0.54479635 -1.2271482  -0.27123478 -0.51852685]\n",
      "Reset environment\n",
      "Episode reward: -713.7539890706539\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.49538416 -0.54073834 -0.54624915 -1.2294137  -0.27261752 -0.5203545 ]\n",
      "Reset environment\n",
      "Episode reward: 372.25669215619564\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.48900717 -0.5347338  -0.5394289  -1.2220658  -0.26719257 -0.51394147]\n",
      "Reset environment\n",
      "Episode reward: 193.07192027196288\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.48353907 -0.52885175 -0.53430927 -1.2147189  -0.26311058 -0.50846004]\n",
      "Reset environment\n",
      "Episode reward: -63.5895559489727\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.4801109  -0.5250707  -0.53123415 -1.2104678  -0.2604947  -0.5050257 ]\n",
      "Reset environment\n",
      "Episode reward: 6119.3572593025165\n",
      "Total Steps: 1258\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.4336401  -0.477205   -0.48472428 -1.166368   -0.21899599 -0.45783803]\n",
      "Reset environment\n",
      "Episode reward: -227.12440333887935\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.43192524 -0.47580594 -0.4825683  -1.1644547  -0.21733189 -0.45608974]\n",
      "Reset environment\n",
      "Episode reward: 369.68056258559227\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.42554507 -0.46911994 -0.47646612 -1.1570287  -0.2120978  -0.4496984 ]\n",
      "Reset environment\n",
      "Episode reward: 4359.9271915555\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.39673486 -0.44003785 -0.44785652 -1.1250402  -0.18725921 -0.42085814]\n",
      "Reset environment\n",
      "Episode reward: 527.4272928833961\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.38911107 -0.4321012  -0.440515   -1.1163133  -0.18089965 -0.41321293]\n",
      "Reset environment\n",
      "Episode reward: 1906.1573448777199\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.37699994 -0.4200074  -0.42833766 -1.1022243  -0.17073621 -0.4010754 ]\n",
      "Reset environment\n",
      "Episode reward: -642.3579610586166\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.37468305 -0.4180546  -0.4253705  -1.0994554  -0.16849327 -0.3986739 ]\n",
      "Reset environment\n",
      "Episode reward: 223.57846494019032\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.36933345 -0.41239065 -0.42036098 -1.093186   -0.16416158 -0.39331827]\n",
      "Reset environment\n",
      "Episode reward: -13.719547525048256\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.3655899  -0.40832686 -0.4169119  -1.088552   -0.16125302 -0.3895646 ]\n",
      "Reset environment\n",
      "Episode reward: 497.35306549072266\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.35828513 -0.4007816  -0.40972573 -1.0795971  -0.15526424 -0.38220963]\n",
      "Reset environment\n",
      "Episode reward: -93.04847767949104\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.35509288 -0.3972636  -0.40687335 -1.0755813  -0.1528565  -0.3790362 ]\n",
      "Reset environment\n",
      "Episode reward: -137.50745630264282\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.35199872 -0.39386624 -0.4040196  -1.0715047  -0.1504905  -0.37591237]\n",
      "Reset environment\n",
      "Episode reward: 5405.097783029079\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.31454262 -0.35668018 -0.3661251  -1.0300012  -0.11754306 -0.33842313]\n",
      "Reset environment\n",
      "Episode reward: 873.5971899032593\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.30483973 -0.34680298 -0.35657203 -1.0183971  -0.10958654 -0.32870615]\n",
      "Reset environment\n",
      "Episode reward: -1109.006863117218\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.30770835 -0.34978926 -0.35906777 -1.021071   -0.11194061 -0.33149752]\n",
      "Reset environment\n",
      "Episode reward: 612.4682539701462\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.29990005 -0.34200248 -0.35110486 -1.0114455  -0.10555933 -0.32364506]\n",
      "Reset environment\n",
      "Episode reward: 144.88149094581604\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.29493335 -0.33724672 -0.3458031  -1.0051531  -0.10151888 -0.3186093 ]\n",
      "Reset environment\n",
      "Episode reward: 788.3382762670517\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2856182  -0.32828632 -0.33602682 -0.9944144  -0.09358262 -0.30924544]\n",
      "Reset environment\n",
      "Episode reward: -124.22529515624046\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28280774 -0.32578537 -0.33284587 -0.99033076 -0.09165163 -0.30642843]\n",
      "Reset environment\n",
      "Episode reward: 409.8235664963722\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27629748 -0.3194637  -0.32609037 -0.98223656 -0.08649621 -0.2998913 ]\n",
      "Reset environment\n",
      "Episode reward: 3616.2371125221252\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.25316834 -0.29653496 -0.30268967 -0.9560521  -0.06647582 -0.27670866]\n",
      "Reset environment\n",
      "Episode reward: -1238.2770419207518\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2513806  -0.2980564  -0.2970132  -0.9868439  -0.05300006 -0.2741887 ]\n",
      "Reset environment\n",
      "Episode reward: 636.730673789978\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24323423 -0.2899972  -0.28877196 -0.9770595  -0.04647198 -0.2660582 ]\n",
      "Reset environment\n",
      "Episode reward: 911.6197786331177\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.23310044 -0.2800431  -0.27839476 -0.965469   -0.03788117 -0.2559069 ]\n",
      "Reset environment\n",
      "Episode reward: -4330.86679463461\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2582333  -0.3082577  -0.30030233 -1.0296496  -0.05158154 -0.28128427]\n",
      "Reset environment\n",
      "Episode reward: 957.21253657341\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24983837 -0.3007305  -0.29088008 -1.0196121  -0.04426062 -0.27284673]\n",
      "Reset environment\n",
      "Episode reward: -7194.862467574887\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.30543652 -0.3521928  -0.35037422 -1.0957518  -0.08917623 -0.3285908 ]\n",
      "Reset environment\n",
      "Episode reward: -9892.469508714974\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.3598148  -0.40494356 -0.40470526 -1.1888309  -0.12653628 -0.38253403]\n",
      "Reset environment\n",
      "Episode reward: 509.03209260106087\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.35296232 -0.3978006  -0.39811647 -1.1809058  -0.12085094 -0.37567794]\n",
      "Reset environment\n",
      "Episode reward: 1385.5767184495926\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.34040397 -0.38543713 -0.38529852 -1.1666332  -0.11007342 -0.3630911 ]\n",
      "Reset environment\n",
      "Episode reward: 1730.3285920619965\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.32550398 -0.37068847 -0.37015304 -1.1500381  -0.097117   -0.34818435]\n",
      "Reset environment\n",
      "Episode reward: 301.9499301612377\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.32006612 -0.36548057 -0.36442053 -1.1432796  -0.09278243 -0.3427219 ]\n",
      "Reset environment\n",
      "Episode reward: -8620.428638458252\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.34956872 -0.3941364  -0.3939491  -1.2230135  -0.10640704 -0.3721419 ]\n",
      "Reset environment\n",
      "Episode reward: 799.8736692667007\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.34087896 -0.3856728  -0.38500875 -1.2130817  -0.09901264 -0.36344108]\n",
      "Reset environment\n",
      "Episode reward: -1059.6238064989448\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.34430733 -0.3895282  -0.38773632 -1.2166559  -0.10140721 -0.36676526]\n",
      "Reset environment\n",
      "Episode reward: 525.5518511813134\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.33791584 -0.38177058 -0.38232946 -1.208212   -0.09640639 -0.36016324]\n",
      "Reset environment\n",
      "Episode reward: 6453.5410034656525\n",
      "Total Steps: 290\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.29837754 -0.34246075 -0.34235567 -1.164424   -0.06203775 -0.32050177]\n",
      "Reset environment\n",
      "Episode reward: 768.2406569719315\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2900864  -0.33447823 -0.33366087 -1.1547183  -0.05494735 -0.31217748]\n",
      "Reset environment\n",
      "Episode reward: 910.5529764890671\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28124988 -0.3257694  -0.32459396 -1.1446333  -0.04730215 -0.3033006 ]\n",
      "Reset environment\n",
      "Episode reward: -933.0998768378631\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28964573 -0.33855504 -0.32825798 -1.1841935  -0.04437448 -0.3114513 ]\n",
      "Reset environment\n",
      "Episode reward: 728.5892581939697\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2808163  -0.32940647 -0.3196075  -1.1734544  -0.03704664 -0.30254462]\n",
      "Reset environment\n",
      "Episode reward: -8922.062479551882\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.29188082 -0.34322762 -0.3281952  -1.2013928  -0.04305752 -0.31402954]\n",
      "Reset environment\n",
      "Episode reward: 1234.0059768557549\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28095835 -0.33228123 -0.31725892 -1.1891326  -0.03370645 -0.30308664]\n",
      "Reset environment\n",
      "Episode reward: -542.6002383287996\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28323227 -0.33437383 -0.31745398 -1.2156864  -0.02570402 -0.30422458]\n",
      "Reset environment\n",
      "Episode reward: -9012.944817453623\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.33269116 -0.38154572 -0.36801285 -1.3070276  -0.05851808 -0.35327202]\n",
      "Reset environment\n",
      "Episode reward: 617.5900113768876\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.32708782 -0.37443423 -0.36350274 -1.2989097  -0.05450414 -0.34744358]\n",
      "Reset environment\n",
      "Episode reward: 513.2734774947166\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.3208172  -0.36835977 -0.35697597 -1.2912818  -0.0493124  -0.34114262]\n",
      "Reset environment\n",
      "Episode reward: 32.04822716861963\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.31908262 -0.36773914 -0.35399726 -1.287489   -0.04807558 -0.33939338]\n",
      "Reset environment\n",
      "Episode reward: 290.40356712508947\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.31447202 -0.36373192 -0.34866285 -1.281988   -0.04385426 -0.33474228]\n",
      "Reset environment\n",
      "Episode reward: 26.336754485964775\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.31108433 -0.36060604 -0.34496555 -1.2772135  -0.04140159 -0.33134666]\n",
      "Reset environment\n",
      "Episode reward: -971.4510882943869\n",
      "Total Steps: 1351\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.3143691  -0.3657738  -0.3461623  -1.3055679  -0.0375558  -0.33480877]\n",
      "Reset environment\n",
      "Episode reward: 1571.9683246016502\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.30152386 -0.35307634 -0.33308753 -1.2908509  -0.02661738 -0.32192588]\n",
      "Reset environment\n",
      "Episode reward: -2196.05629581213\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.30921885 -0.35962954 -0.3415365  -1.2985576  -0.03285582 -0.32945836]\n",
      "Reset environment\n",
      "Episode reward: 1160.5562435984612\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.29897282 -0.3494502  -0.3311818  -1.2866386  -0.02426365 -0.31920117]\n",
      "Reset environment\n",
      "Episode reward: 68.55326014757156\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2954046  -0.3456749  -0.32783878 -1.2815912  -0.0217126  -0.3156297 ]\n",
      "Reset environment\n",
      "Episode reward: 671.3012401461601\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.28815052 -0.33867002 -0.3203022  -1.2729062  -0.01572701 -0.3083589 ]\n",
      "Reset environment\n",
      "Episode reward: 2662.935789898038\n",
      "Total Steps: 280\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2788136  -0.33099726 -0.3091338  -1.260881   -0.0080985  -0.29897407]\n",
      "Reset environment\n",
      "Episode reward: -111.62440402247012\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27589908 -0.32765245 -0.30656603 -1.257137   -0.00567033 -0.296009  ]\n",
      "Reset environment\n",
      "Episode reward: 751.1734185740352\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27110302 -0.3219054  -0.30246702 -1.2506561  -0.00184897 -0.29107004]\n",
      "Reset environment\n",
      "Episode reward: 1079.2863869667053\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.26132432 -0.31176856 -0.29303077 -1.239399    0.00647631 -0.28131443]\n",
      "Reset environment\n",
      "Episode reward: 53.66408085823059\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.25785083 -0.30870396 -0.2889769  -1.2353094   0.00974644 -0.2777705 ]\n",
      "Reset environment\n",
      "Episode reward: 853.971717953682\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24959368 -0.30045128 -0.28069693 -1.2256413   0.01659577 -0.26951775]\n",
      "Reset environment\n",
      "Episode reward: 1251.4291565418243\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.23895325 -0.29004058 -0.26972106 -1.2132739   0.0256483  -0.258843  ]\n",
      "Reset environment\n",
      "Episode reward: -4051.0735640227795\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.25989437 -0.31220672 -0.28877342 -1.2726114   0.01819022 -0.27975556]\n",
      "Reset environment\n",
      "Episode reward: -2121.3032367415726\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27665868 -0.32318836 -0.30972877 -1.3139825   0.01362324 -0.2961838 ]\n",
      "Reset environment\n",
      "Episode reward: 24.62108987569809\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27502605 -0.3202849  -0.30906677 -1.3111596   0.01510568 -0.2943664 ]\n",
      "Reset environment\n",
      "Episode reward: -721.2519693523645\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27600572 -0.32172388 -0.3094255  -1.3116603   0.01444123 -0.29526183]\n",
      "Reset environment\n",
      "Episode reward: -5109.611812813673\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2914239  -0.3368259  -0.32390416 -1.3344865   0.00444306 -0.31046423]\n",
      "Reset environment\n",
      "Episode reward: 2445.9128339886665\n",
      "Total Steps: 1101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.27193353 -0.3175217  -0.30370486 -1.3274181   0.02546337 -0.29105768]\n",
      "Reset environment\n",
      "Episode reward: 4047.978957235813\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24910255 -0.29448408 -0.2810013  -1.3017926   0.04519965 -0.26819605]\n",
      "Reset environment\n",
      "Episode reward: 825.189083814621\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24149317 -0.28692573 -0.27328432 -1.2927998   0.05154653 -0.26057678]\n",
      "Reset environment\n",
      "Episode reward: -1033.811268969439\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24512656 -0.29088497 -0.27645016 -1.2961639   0.04858351 -0.26415282]\n",
      "Reset environment\n",
      "Episode reward: -326.82632875815034\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24592425 -0.29103774 -0.2777218  -1.2964699   0.04811386 -0.2648541 ]\n",
      "Reset environment\n",
      "Episode reward: 742.9738387465477\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.23869644 -0.2838802  -0.27040997 -1.2878267   0.0540741  -0.25761232]\n",
      "Reset environment\n",
      "Episode reward: 789.8933032155037\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.23118544 -0.2765645  -0.26267365 -1.2791363   0.06049984 -0.25008452]\n",
      "Reset environment\n",
      "Episode reward: 670.9214451313019\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.22432138 -0.2694556  -0.25601935 -1.2709765   0.06615867 -0.24321249]\n",
      "Reset environment\n",
      "Episode reward: -1820.047211408615\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.24181305 -0.28832418 -0.27244496 -1.3296195   0.0643199  -0.26101914]\n",
      "Reset environment\n",
      "Episode reward: 1418.2722119688988\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2305102  -0.27737355 -0.2606002  -1.317131    0.0744093  -0.24962513]\n",
      "Reset environment\n",
      "Episode reward: 365.386901673337\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.2253022  -0.2707961  -0.25646967 -1.3096465   0.07838221 -0.24425277]\n",
      "Reset environment\n",
      "Episode reward: 1170.1944467425346\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.21579331 -0.26147422 -0.2466784  -1.2984562   0.08635078 -0.23469353]\n",
      "Reset environment\n",
      "Episode reward: 1596.0095190480351\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.20577279 -0.25053012 -0.237434   -1.2861944   0.09462066 -0.22458875]\n",
      "Reset environment\n",
      "Episode reward: 621.1789514943957\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.19862464 -0.24470003 -0.22889    -1.2767742   0.10086653 -0.21737099]\n",
      "Reset environment\n",
      "Episode reward: 269.66562312841415\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.19431695 -0.24026313 -0.22466142 -1.2708504   0.1040047  -0.21304226]\n",
      "Reset environment\n",
      "Episode reward: 3837.812501460314\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.17016692 -0.21673903 -0.19980493 -1.2436832   0.12506615 -0.18882075]\n",
      "Reset environment\n",
      "Episode reward: 311.10551127791405\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.16562061 -0.21249434 -0.19492103 -1.237997    0.12881254 -0.18426389]\n",
      "Reset environment\n",
      "Episode reward: 585.6791700124741\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.15952194 -0.2063316  -0.18886787 -1.2303892   0.1335908  -0.17815572]\n",
      "Reset environment\n",
      "Episode reward: 1049.8455911278725\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.15077223 -0.19738296 -0.18028991 -1.2204845   0.14103968 -0.16940524]\n",
      "Reset environment\n",
      "Episode reward: 2904.328830599785\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.13543974 -0.18192026 -0.16500796 -1.2033585   0.15424785 -0.15402171]\n",
      "Reset environment\n",
      "Episode reward: 2133.0681050419807\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.12042925 -0.16709732 -0.14970776 -1.1865507   0.16721664 -0.13897705]\n",
      "Reset environment\n",
      "Episode reward: 396.8094720840454\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.1153719  -0.16191578 -0.14476068 -1.1799382   0.17101339 -0.13391235]\n",
      "Reset environment\n",
      "Episode reward: 504.2614245712757\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.10971129 -0.15599951 -0.13935514 -1.173243    0.1757113  -0.12824887]\n",
      "Reset environment\n",
      "Episode reward: 793.3601152896881\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.1024462  -0.14894968 -0.13183163 -1.1648881   0.18195337 -0.1209707 ]\n",
      "Reset environment\n",
      "Episode reward: 5646.431491971016\n",
      "Total Steps: 211\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.07235613 -0.11865634 -0.10176771 -1.1321903   0.20838553 -0.0908038 ]\n",
      "Reset environment\n",
      "Episode reward: 224.74351769685745\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.06826468 -0.1143733  -0.09785721 -1.1266637   0.21140982 -0.08669768]\n",
      "Reset environment\n",
      "Episode reward: -36.00501722097397\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.06554853 -0.1121643  -0.09459417 -1.1222612   0.21323606 -0.08396614]\n",
      "Reset environment\n",
      "Episode reward: 3702.3969832360744\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.04220443 -0.08949081 -0.0704696  -1.0963117   0.23380457 -0.06056698]\n",
      "Reset environment\n",
      "Episode reward: 14.941709041595459\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.04046124 -0.08682764 -0.06941095 -1.0935323   0.23538442 -0.058686  ]\n",
      "Reset environment\n",
      "Episode reward: 641.032771229744\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.03421976 -0.08075891 -0.06300042 -1.0860534   0.24055365 -0.05244328]\n",
      "Reset environment\n",
      "Episode reward: 735.388833463192\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.02736613 -0.07366735 -0.05637534 -1.0781356   0.24631186 -0.0455925 ]\n",
      "Reset environment\n",
      "Episode reward: 390.80223697423935\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.02295353 -0.06980468 -0.05133564 -1.0728208   0.25027347 -0.04114699]\n",
      "Reset environment\n",
      "Episode reward: 1437.7653140425682\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01215273 -0.05893067 -0.0406161  -1.0604974   0.2594855  -0.03036674]\n",
      "Reset environment\n",
      "Episode reward: -64.58947815001011\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00972761 -0.05677292 -0.03789477 -1.056824    0.26121855 -0.02793719]\n",
      "Reset environment\n",
      "Episode reward: -5524.010369094438\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00517284 -0.05054609 -0.03444082 -1.076438    0.26721087 -0.02317502]\n",
      "Reset environment\n",
      "Episode reward: -498.5126477777958\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-2.7455288e-04 -4.4619869e-02 -3.0313512e-02 -1.0701537e+00\n",
      "  2.7133238e-01 -1.8130807e-02]\n",
      "Reset environment\n",
      "Episode reward: 1754.5210783481598\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01248119 -0.03204028 -0.01731054 -1.0557576   0.2823084  -0.00532846]\n",
      "Reset environment\n",
      "Episode reward: 59.41859744489193\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01504107 -0.0284649  -0.01549474 -1.0520355   0.28464922 -0.00258076]\n",
      "Reset environment\n",
      "Episode reward: 2557.7420201301575\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03881202 -0.00416681  0.00798731 -1.0622219   0.31348047  0.02092748]\n",
      "Reset environment\n",
      "Episode reward: -1365.9750241542934\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.02139842 -0.01761832 -0.01197687 -1.1027024   0.3078632   0.00363416]\n",
      "Reset environment\n",
      "Episode reward: 1288.326828300953\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03102763 -0.00761315 -0.00266131 -1.0916557   0.31611186  0.0132706 ]\n",
      "Reset environment\n",
      "Episode reward: -5654.473795935512\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.02116781 -0.05989858 -0.0549734  -1.157156    0.27888054 -0.03887995]\n",
      "Reset environment\n",
      "Episode reward: 1393.5838795900345\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01487712 -0.05353959 -0.04872466 -1.1495066   0.28398916 -0.03258753]\n",
      "Reset environment\n",
      "Episode reward: 587.9962464570999\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00908563 -0.04760062 -0.04307159 -1.1424052   0.28866687 -0.02677829]\n",
      "Reset environment\n",
      "Episode reward: -332.6538668461144\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00922181 -0.04665879 -0.04402643 -1.1409245   0.28821033 -0.02672898]\n",
      "Reset environment\n",
      "Episode reward: -399.1100156765897\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0088103  -0.04662556 -0.04323325 -1.1396112   0.28840384 -0.02630266]\n",
      "Reset environment\n",
      "Episode reward: -855.5792108625174\n",
      "Total Steps: 218\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0098849  -0.04640689 -0.04529953 -1.1387398   0.28692773 -0.02709832]\n",
      "Reset environment\n",
      "Episode reward: 466.3223659992218\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00499548 -0.0412076  -0.0406753  -1.1330204   0.29120144 -0.02220226]\n",
      "Reset environment\n",
      "Episode reward: -1824.150538213551\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.02127697 -0.05482953 -0.05880403 -1.1701378   0.2815501  -0.03826844]\n",
      "Reset environment\n",
      "Episode reward: 1095.6897809505463\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01285541 -0.04603762 -0.05069884 -1.1602836   0.28867382 -0.02983267]\n",
      "Reset environment\n",
      "Episode reward: 347.4926577806473\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00845614 -0.04138366 -0.04655164 -1.1549691   0.2923151  -0.02543057]\n",
      "Reset environment\n",
      "Episode reward: 3182.7335927784443\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.00996809 -0.02399125 -0.02708454 -1.1339679   0.3082555  -0.00697256]\n",
      "Reset environment\n",
      "Episode reward: 365.209001660347\n",
      "Total Steps: 20\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01072078 -0.0232395  -0.02633208 -1.1328492   0.3087812  -0.00622134]\n",
      "Reset environment\n",
      "Episode reward: 1023.1372016072273\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0186061  -0.01550497 -0.01829379 -1.1239363   0.3155653   0.00167236]\n",
      "Reset environment\n",
      "Episode reward: 1093.976015985012\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.02677866 -0.00716213 -0.01024804 -1.1147028   0.32259184  0.00986196]\n",
      "Reset environment\n",
      "Episode reward: -637.749190873641\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.02584572 -0.00837348 -0.01089002 -1.1148772   0.3215257   0.00891922]\n",
      "Reset environment\n",
      "Episode reward: 411.33895686268806\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0305728  -0.00379433 -0.00602523 -1.1087279   0.32518098  0.01364195]\n",
      "Reset environment\n",
      "Episode reward: 212.5207418166101\n",
      "Total Steps: 200\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03459808  0.00149013 -0.00295581 -1.1031021   0.32870847  0.01786223]\n",
      "Reset environment\n",
      "Episode reward: -3995.083006092056\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.03782939 -0.07243653 -0.07265774 -1.1776594   0.26838386 -0.0546429 ]\n",
      "Reset environment\n",
      "Episode reward: 685.7753494381905\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.03175912 -0.06662159 -0.06631581 -1.1705459   0.27359396 -0.04855872]\n",
      "Reset environment\n",
      "Episode reward: 2710.771108150482\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01882833 -0.05359422 -0.05345261 -1.1560813   0.2847262  -0.03560127]\n",
      "Reset environment\n",
      "Episode reward: 1251.7264213562012\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.00984833 -0.04472073 -0.04435153 -1.145647    0.29231095 -0.02660841]\n",
      "Reset environment\n",
      "Episode reward: 1307.8173180818558\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.00417973 -0.03152821 -0.02944411 -1.1295046   0.30428463 -0.01251871]\n",
      "Reset environment\n",
      "Episode reward: 3857.082084560767\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01291688 -0.02184775 -0.02039232 -1.1361253   0.31696048 -0.00357428]\n",
      "Reset environment\n",
      "Episode reward: -7876.01116444543\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.01903428 -0.05264613 -0.05336712 -1.2016925   0.29464316 -0.03599824]\n",
      "Reset environment\n",
      "Episode reward: 1090.7880722880363\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0110512  -0.04449667 -0.04550585 -1.1926572   0.3014982  -0.02799311]\n",
      "Reset environment\n",
      "Episode reward: 379.50843021273613\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [-0.0067357  -0.04039191 -0.04095549 -1.1871511   0.30493394 -0.02366929]\n",
      "Reset environment\n",
      "Episode reward: 947.0952355861664\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 4.9402169e-04 -3.3339389e-02 -3.3458006e-02 -1.1784866e+00\n",
      "  3.1096542e-01 -1.6424611e-02]\n",
      "Reset environment\n",
      "Episode reward: -728.5284647792578\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.00544074 -0.0311391  -0.02520633 -1.2001959   0.32273728 -0.01117491]\n",
      "Reset environment\n",
      "Episode reward: -128.6019201874733\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.00734564 -0.02958329 -0.02290175 -1.1973429   0.32419243 -0.00923714]\n",
      "Reset environment\n",
      "Episode reward: 1056.7793997526169\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01507137 -0.02184789 -0.01521163 -1.1885151   0.33077964 -0.00152282]\n",
      "Reset environment\n",
      "Episode reward: -3105.4340739399195\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01019256 -0.02771502 -0.01869623 -1.2281681   0.33762062 -0.00605662]\n",
      "Reset environment\n",
      "Episode reward: 1119.8667161464691\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.01808928 -0.01961768 -0.01098033 -1.2193105   0.3444807   0.00185057]\n",
      "Reset environment\n",
      "Episode reward: 469.2408632338047\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.02281627 -0.0151409  -0.00597333 -1.2135268   0.34846103  0.0065869 ]\n",
      "Reset environment\n",
      "Episode reward: 993.2731775045395\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03017812 -0.0076348   0.00125038 -1.2050803   0.3546995   0.01394797]\n",
      "Reset environment\n",
      "Episode reward: 477.1094423532486\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.03492637 -0.00314608  0.00624993 -1.1992617   0.3586443   0.01869091]\n",
      "Reset environment\n",
      "Episode reward: 631.2651522159576\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.04043766  0.00229604  0.01183965 -1.1925577   0.36317784  0.02420838]\n",
      "Reset environment\n",
      "Episode reward: 661.4718760251999\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.04588119  0.0080261   0.01704832 -1.1862656   0.3679816   0.02967394]\n",
      "Reset environment\n",
      "Episode reward: -0.034018563106656075\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.04831342  0.00939261  0.0204685  -1.1817238   0.36994097  0.03210217]\n",
      "Reset environment\n",
      "Episode reward: 690.2567314505577\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.05059084  0.0116711   0.02276639 -1.1788095   0.37174043  0.03438109]\n",
      "Reset environment\n",
      "Episode reward: 231.72707498073578\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.0536815   0.01512637  0.02551917 -1.1750613   0.3744627   0.03747754]\n",
      "Reset environment\n",
      "Episode reward: 1168.0787157416344\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.05830178  0.01974053  0.03015578 -1.1694735   0.37822804  0.04208766]\n",
      "Reset environment\n",
      "Episode reward: -576.7946239858866\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.05637397  0.01753142  0.02855366 -1.1714453   0.37669575  0.04019649]\n",
      "Reset environment\n",
      "Episode reward: 76.32737202942371\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.05926919  0.02068618  0.03117525 -1.1677338   0.37902167  0.04307207]\n",
      "Reset environment\n",
      "Episode reward: 975.7015722617507\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.06750952  0.02993639  0.03852297 -1.1577897   0.3857434   0.05139935]\n",
      "Reset environment\n",
      "Episode reward: 6804.932069947943\n",
      "Total Steps: 411\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.09954878  0.06129416  0.07136819 -1.1222368   0.41394556  0.0834394 ]\n",
      "Reset environment\n",
      "Episode reward: 1703.511958181858\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.11030488  0.07185952  0.08228387 -1.1102303   0.42337355  0.09420681]\n",
      "Reset environment\n",
      "Episode reward: 585.7115833163261\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.11545479  0.07718288  0.08730868 -1.1038364   0.42749196  0.09937643]\n",
      "Reset environment\n",
      "Episode reward: 591.7691900730133\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.12068882  0.08264801  0.09232045 -1.0975931   0.43188784  0.10461977]\n",
      "Reset environment\n",
      "Episode reward: 1921.1087156236172\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.12609492  0.08720413  0.09912194 -1.120213    0.4471895   0.11036154]\n",
      "Reset environment\n",
      "Episode reward: 1018.9905828237534\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.133323    0.09440026  0.1064124  -1.1119332   0.4533825   0.11760671]\n",
      "Reset environment\n",
      "Episode reward: 5929.1017644405365\n",
      "Total Steps: 224\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.16029993  0.12129503  0.13357408 -1.0824299   0.47730985  0.14466785]\n",
      "Reset environment\n",
      "Episode reward: -1547.013245716691\n",
      "Total Steps: 1904\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.1775412   0.13860275  0.1520017  -1.0647154   0.49616972  0.16215412]\n",
      "Reset environment\n",
      "Episode reward: 1469.9538562893867\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.17885244  0.14192285  0.15184306 -1.0825431   0.5048803   0.16360961]\n",
      "Reset environment\n",
      "Episode reward: 5675.104244083166\n",
      "Total Steps: 236\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.2028838   0.16499844  0.17679141 -1.0559667   0.5256873   0.18762125]\n",
      "Reset environment\n",
      "Episode reward: 6754.485551297665\n",
      "Total Steps: 288\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.23152667  0.19269994  0.20638806 -1.0244104   0.550788    0.2162682 ]\n",
      "Reset environment\n",
      "Episode reward: -27.812670320272446\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.233194    0.19527979  0.20721099 -1.021385    0.55182165  0.21806836]\n",
      "Reset environment\n",
      "Episode reward: -526.0470536537468\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.25177142  0.21108913  0.22895095 -1.0208789   0.57031167  0.23704533]\n",
      "Reset environment\n",
      "Episode reward: 755.7939896583557\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.2575379   0.21667512  0.23488899 -1.0140942   0.5752237   0.24282128]\n",
      "Reset environment\n",
      "Episode reward: 14.043492682278156\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.26024845  0.22062807  0.23771283 -1.0183961   0.58150303  0.24583653]\n",
      "Reset environment\n",
      "Episode reward: 3119.9180179983377\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.2777739   0.23753645  0.255959   -1.0195893   0.60117054  0.26339355]\n",
      "Reset environment\n",
      "Episode reward: 1613.9411612451077\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.28741455  0.24641682  0.2663268  -1.0082302   0.6093385   0.27302486]\n",
      "Reset environment\n",
      "Episode reward: 948.3923316001892\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.29395032  0.25291824  0.27290282 -1.0006284   0.6148961   0.27957   ]\n",
      "Reset environment\n",
      "Episode reward: 664.7869448363781\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.29915923  0.2578747   0.27838105 -0.99454284  0.6193798   0.28480762]\n",
      "Reset environment\n",
      "Episode reward: 677.4978036880493\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.30434597  0.26317912  0.28349027 -0.98791784  0.62348735  0.29000825]\n",
      "Reset environment\n",
      "Episode reward: 1481.6944214105606\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.31327564  0.27185366  0.29269442 -0.9777373   0.63125336  0.29894623]\n",
      "Reset environment\n",
      "Episode reward: 430.5740007162094\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.31716612  0.2760534   0.29632804 -0.9730361   0.6345488   0.30284977]\n",
      "Reset environment\n",
      "Episode reward: 5181.65815025568\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.3392814   0.29829967  0.3184247  -0.9488626   0.6541685   0.3250035 ]\n",
      "Reset environment\n",
      "Episode reward: 882.002370595932\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.345413    0.30456632  0.3244165  -0.94172275  0.659336    0.33113155]\n",
      "Reset environment\n",
      "Episode reward: 757.8696425165981\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.35130355  0.30997765  0.33081308 -0.93466073  0.66439414  0.33704528]\n",
      "Reset environment\n",
      "Episode reward: -858.5617449060082\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.32619604  0.28445122  0.30777878 -0.9631526   0.64686286  0.31225514]\n",
      "Reset environment\n",
      "Episode reward: 977.2791059613228\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.3327093   0.29123905  0.31404194 -0.95554703  0.65239894  0.31876886]\n",
      "Reset environment\n",
      "Episode reward: -625.3675676584244\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.3322293   0.2906099   0.31370202 -0.95632637  0.65201646  0.31828552]\n",
      "Reset environment\n",
      "Episode reward: 1016.2032886147499\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.33891907  0.29728186  0.32039353 -0.9486066   0.6577082   0.32498014]\n",
      "Reset environment\n",
      "Episode reward: 183.78798667527735\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.34169558  0.3009186   0.3224544  -0.9444551   0.6597447   0.32787728]\n",
      "Reset environment\n",
      "Episode reward: 1153.193653345108\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.3490133   0.30840126  0.32959345 -0.9362506   0.6661156   0.33518785]\n",
      "Reset environment\n",
      "Episode reward: 1117.6147106289864\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.35600972  0.315099    0.3368968  -0.9280935   0.6721453   0.3422    ]\n",
      "Reset environment\n",
      "Episode reward: 1103.561204135418\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.36302948  0.32218653  0.34386244 -0.92004347  0.67818904  0.34922627]\n",
      "Reset environment\n",
      "Episode reward: 1077.4564329385757\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.36994687  0.32898253  0.35088894 -0.91223353  0.68425184  0.35614672]\n",
      "Reset environment\n",
      "Episode reward: 1167.3301334381104\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.37728247  0.33655018  0.3579577  -0.9037562   0.69051915  0.36345926]\n",
      "Reset environment\n",
      "Episode reward: 3596.3016909509897\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.39521915  0.35388806  0.3764314  -0.8838587   0.70633644  0.38139814]\n",
      "Reset environment\n",
      "Episode reward: 1243.9010410904884\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.40293887  0.3616127   0.38415217 -0.8751995   0.7130421   0.38912418]\n",
      "Reset environment\n",
      "Episode reward: 1552.098383616656\n",
      "Total Steps: 478\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.40588075  0.36327976  0.38845244 -0.8713145   0.7149645   0.39210317]\n",
      "Reset environment\n",
      "Episode reward: 1482.6801989078522\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.4146321   0.37223536  0.3970051  -0.86123174  0.7224559   0.40086842]\n",
      "Reset environment\n",
      "Episode reward: 5335.7365955114365\n",
      "Total Steps: 222\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.43972185  0.3975223   0.42188495 -0.83386093  0.7446424   0.42600378]\n",
      "Reset environment\n",
      "Episode reward: 1027.5410437583923\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.44631287  0.4038854   0.4286965  -0.8262847   0.75036246  0.43261197]\n",
      "Reset environment\n",
      "Episode reward: 4542.121563434601\n",
      "Total Steps: 1114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.45734295  0.41398525  0.44044688 -0.8195854   0.7638949   0.44344327]\n",
      "Reset environment\n",
      "Episode reward: 815.6206966638565\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.4551341   0.41132376  0.43879798 -0.8546844   0.769283    0.4410032 ]\n",
      "Reset environment\n",
      "Episode reward: 2698.8052331358194\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.4686741   0.42412362  0.45306    -0.83930516  0.78097606  0.454555  ]\n",
      "Reset environment\n",
      "Episode reward: -1989.4806805886328\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.46433693  0.41952762  0.4489702  -0.87303966  0.7809482   0.4500085 ]\n",
      "Reset environment\n",
      "Episode reward: 623.4195749312639\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.4694104   0.42545074  0.45331857 -0.86667067  0.78499323  0.4551594 ]\n",
      "Reset environment\n",
      "Episode reward: 916.0818771123886\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.47537813  0.43119675  0.45948845 -0.85968083  0.7901128   0.46112305]\n",
      "Reset environment\n",
      "Episode reward: 1852.7669112682343\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.48242664  0.43819848  0.4665716  -0.8514529   0.7960552   0.46816337]\n",
      "Reset environment\n",
      "Episode reward: 818.7258498668671\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.48794064  0.44354156  0.47223157 -0.8449651   0.80073583  0.47366872]\n",
      "Reset environment\n",
      "Episode reward: 104.71857785247266\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.47891688  0.4374716   0.46125004 -0.8700415   0.7991362   0.4648545 ]\n",
      "Reset environment\n",
      "Episode reward: 177.8069642484188\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.4814308   0.43932164  0.4644448  -0.8662795   0.80131245  0.46738654]\n",
      "Reset environment\n",
      "Episode reward: 1195.230793952942\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.48859155  0.4462747   0.4717951  -0.8580076   0.8074605   0.47455025]\n",
      "Reset environment\n",
      "Episode reward: 1070.6539875268936\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.49522117  0.45310718  0.47822186 -0.850189    0.81310576  0.4811796 ]\n",
      "Reset environment\n",
      "Episode reward: 603.781294554472\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.49978635  0.45791325  0.48255128 -0.844744    0.81702816  0.4857373 ]\n",
      "Reset environment\n",
      "Episode reward: 3321.7829409241676\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5166532   0.47449118  0.49965775 -0.82586247  0.8317187   0.50258875]\n",
      "Reset environment\n",
      "Episode reward: -279.34451255202293\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.51746684  0.47557557  0.50022477 -0.82421726  0.83218116  0.50340337]\n",
      "Reset environment\n",
      "Episode reward: -1833.3625707626343\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.52827007  0.48505208  0.5124788  -0.840153    0.84546477  0.5142969 ]\n",
      "Reset environment\n",
      "Episode reward: -466.2998082488775\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.52815187  0.4846416   0.5126456  -0.83973205  0.8453      0.51418126]\n",
      "Reset environment\n",
      "Episode reward: 1353.5985993742943\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.53582644  0.4922994   0.52035624 -0.83056504  0.8517574   0.5218674 ]\n",
      "Reset environment\n",
      "Episode reward: 1511.9100489616394\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5441924   0.500732    0.5286474  -0.8206912   0.85883665  0.5302352 ]\n",
      "Reset environment\n",
      "Episode reward: 4629.375947006047\n",
      "Total Steps: 301\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5629473  0.5206161  0.5463289 -0.7988242  0.8741088  0.5490357]\n",
      "Reset environment\n",
      "Episode reward: 2305.108001381159\n",
      "Total Steps: 230\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.57107836  0.52766037  0.55549824 -0.78850776  0.88023067  0.5571356 ]\n",
      "Reset environment\n",
      "Episode reward: 1750.28851044178\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5804208   0.53678507  0.56501776 -0.77804434  0.8884182   0.5664701 ]\n",
      "Reset environment\n",
      "Episode reward: 1651.314502267167\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5902073   0.54738665  0.57407093 -0.7666259   0.8963223   0.5763356 ]\n",
      "Reset environment\n",
      "Episode reward: -748.8810141384602\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5785376   0.5387018   0.55975926 -0.79465026  0.8941705   0.56458867]\n",
      "Reset environment\n",
      "Episode reward: -67.30779210012406\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5652204   0.5251365   0.54729694 -0.8294926   0.8880524   0.55146605]\n",
      "Reset environment\n",
      "Episode reward: 471.7085957825184\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5690427   0.52826995  0.5517178  -0.82423776  0.8911137   0.55526793]\n",
      "Reset environment\n",
      "Episode reward: 899.6203563213348\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.57470006  0.5341409   0.5571754  -0.8176605   0.8960061   0.5609288 ]\n",
      "Reset environment\n",
      "Episode reward: 1306.6921792430803\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.57290393  0.53315544  0.55562484 -0.82235897  0.89041454  0.559413  ]\n",
      "Reset environment\n",
      "Episode reward: 1005.7046419531107\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5787657   0.5383044   0.56215495 -0.8149703   0.8953576   0.5652512 ]\n",
      "Reset environment\n",
      "Episode reward: 1224.0022665262222\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5856169   0.544916    0.56926346 -0.8070557   0.9012637   0.57210696]\n",
      "Reset environment\n",
      "Episode reward: 73.57514406740665\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.58821887  0.54855424  0.5709588  -0.8035483   0.9034234   0.57477313]\n",
      "Reset environment\n",
      "Episode reward: -2584.765188347781\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.57328963  0.53428227  0.55629915 -0.83908796  0.89368176  0.560017  ]\n",
      "Reset environment\n",
      "Episode reward: -543.2139849662781\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.572238    0.53305817  0.5554404  -0.8405199   0.892924    0.55898196]\n",
      "Reset environment\n",
      "Episode reward: 1757.9213377833366\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.58131236  0.5421743   0.5644651  -0.83028865  0.9008638   0.5680514 ]\n",
      "Reset environment\n",
      "Episode reward: 762.9433699250221\n",
      "Total Steps: 41\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.58339506  0.54426396  0.5665428  -0.82758564  0.90248793  0.570131  ]\n",
      "Reset environment\n",
      "Episode reward: 829.5751623511314\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.5886318   0.5498343   0.57146716 -0.8213605   0.9069822   0.5753672 ]\n",
      "Reset environment\n",
      "Episode reward: 3017.6298025250435\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.59961665  0.5609145   0.5823473  -0.8090471   0.9165162   0.5863472 ]\n",
      "Reset environment\n",
      "Episode reward: 1548.2244001924992\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.60888916  0.571103    0.59071493 -0.79812837  0.92379147  0.5956846 ]\n",
      "Reset environment\n",
      "Episode reward: 788.4742397665977\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.61388165  0.57631755  0.5954877  -0.7922236   0.92808205  0.60067147]\n",
      "Reset environment\n",
      "Episode reward: -824.3676358517259\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.62548715  0.5861863   0.60840964 -0.79546255  0.93806446  0.6119991 ]\n",
      "Reset environment\n",
      "Episode reward: 598.1336701721884\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.62355703  0.5854872   0.6065403  -0.80790323  0.9376782   0.6105593 ]\n",
      "Reset environment\n",
      "Episode reward: 1923.1356146931648\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6343514   0.59705967  0.6165782  -0.79531986  0.9461805   0.6214109 ]\n",
      "Reset environment\n",
      "Episode reward: 188.96325707435608\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.63682544  0.5993412   0.6192849  -0.79223627  0.94829714  0.6238988 ]\n",
      "Reset environment\n",
      "Episode reward: 1634.770939707756\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.64222676  0.6047706   0.6246558  -0.7859062   0.95289433  0.62929153]\n",
      "Reset environment\n",
      "Episode reward: 2094.0213401317596\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6522819   0.61496586  0.63457686 -0.77445585  0.9616067   0.6393473 ]\n",
      "Reset environment\n",
      "Episode reward: 277.37422926165164\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.65642136  0.6187084   0.6388555  -0.78934413  0.97040546  0.64335406]\n",
      "Reset environment\n",
      "Episode reward: 1329.8988390564919\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.66350996  0.625206    0.64643025 -0.7807659   0.97624767  0.6504223 ]\n",
      "Reset environment\n",
      "Episode reward: 603.1365061737597\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.66885275  0.62998086  0.6522934  -0.77410084  0.98058337  0.65575796]\n",
      "Reset environment\n",
      "Episode reward: 689.83403371647\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6730113   0.63596636  0.65595376 -0.7838408   0.98598695  0.6603423 ]\n",
      "Reset environment\n",
      "Episode reward: 1607.6876826286316\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.68096393  0.6438157   0.66396564 -0.7747927   0.9928581   0.66827863]\n",
      "Reset environment\n",
      "Episode reward: -941.1812868006527\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6742828   0.63607776  0.65762514 -0.8111063   0.9926485   0.6614458 ]\n",
      "Reset environment\n",
      "Episode reward: 916.5180030465126\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.67962646  0.641305    0.66308314 -0.8049149   0.99722373  0.66678965]\n",
      "Reset environment\n",
      "Episode reward: 1264.0466036200523\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.68631727  0.6480935   0.6696621  -0.7974465   1.0030936   0.6734749 ]\n",
      "Reset environment\n",
      "Episode reward: 3678.0716437995434\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6888009  0.6499916  0.672965  -0.8071409  1.011247   0.6758366]\n",
      "Reset environment\n",
      "Episode reward: -692.347861751914\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.68776363  0.6495241   0.67140436 -0.8077269   1.0103996   0.67483646]\n",
      "Reset environment\n",
      "Episode reward: 1628.2385724782944\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.69310033  0.65488756  0.6767337  -0.80155015  1.014986    0.6801934 ]\n",
      "Reset environment\n",
      "Episode reward: 346.99102559685707\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6913041   0.6534946   0.67477363 -0.8193944   1.0164007   0.6783161 ]\n",
      "Reset environment\n",
      "Episode reward: 1632.8942710924894\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6799827   0.64316577  0.66185135 -0.8421959   1.0119923   0.66668314]\n",
      "Reset environment\n",
      "Episode reward: 1652.4356108419597\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6733218   0.63726556  0.65455806 -0.8664783   1.0164353   0.65989643]\n",
      "Reset environment\n",
      "Episode reward: 789.0104644484818\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6765347   0.6400905   0.65765315 -0.88756776  1.0221931   0.6629079 ]\n",
      "Reset environment\n",
      "Episode reward: 1228.5193843385205\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.67763925  0.6406526   0.659115   -0.9045691   1.0273069   0.66380817]\n",
      "Reset environment\n",
      "Episode reward: 1746.1921594142914\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.68321216  0.646176    0.66473967 -0.8980676   1.0320643   0.6693724 ]\n",
      "Reset environment\n",
      "Episode reward: 1115.8374152183533\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.6889836   0.651851    0.67060274 -0.891498    1.037109    0.67514163]\n",
      "Reset environment\n",
      "Episode reward: 1570.4952570796013\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.69693345  0.66064084  0.677765   -0.8818801   1.04327     0.6831454 ]\n",
      "Reset environment\n",
      "Episode reward: 604.4415717720985\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7012386   0.6644027   0.6825735  -0.87645227  1.0468001   0.68744004]\n",
      "Reset environment\n",
      "Episode reward: -468.179321706295\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7010114   0.6640175   0.6825325  -0.8767785   1.0466485   0.68722355]\n",
      "Reset environment\n",
      "Episode reward: 2255.1004982218146\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.71141714  0.675172    0.69219255 -0.86474305  1.0552453   0.69766   ]\n",
      "Reset environment\n",
      "Episode reward: -82.54588992893696\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.71265477  0.6758887   0.6939614  -0.8628045   1.0563152   0.6989029 ]\n",
      "Reset environment\n",
      "Episode reward: -751.7728977501392\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7064526   0.66864604  0.6889664  -0.88489974  1.0542676   0.69239223]\n",
      "Reset environment\n",
      "Episode reward: 1605.180984377861\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7138377   0.6758472   0.69651353 -0.87639695  1.0606968   0.69976956]\n",
      "Reset environment\n",
      "Episode reward: -1654.5687726140022\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.70292324  0.6661563   0.68454355 -0.9090266   1.0565448   0.68856204]\n",
      "Reset environment\n",
      "Episode reward: 1796.1028356552124\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.710984    0.67414534  0.69266593 -0.89984375  1.06354     0.6966218 ]\n",
      "Reset environment\n",
      "Episode reward: 1052.1335964798927\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.71639353  0.67940825  0.6982313  -0.8936495   1.0682507   0.7020219 ]\n",
      "Reset environment\n",
      "Episode reward: 583.5775601044297\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.71801054  0.681238    0.7003037  -0.9018789   1.0700521   0.7037952 ]\n",
      "Reset environment\n",
      "Episode reward: 759.4618416726589\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.72232276  0.68576473  0.7044139  -0.89637804  1.0735633   0.70811754]\n",
      "Reset environment\n",
      "Episode reward: 1107.1306748390198\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.72796786  0.6914481   0.7100362  -0.8898582   1.0784333   0.7137642 ]\n",
      "Reset environment\n",
      "Episode reward: 1127.495113670826\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7336384   0.6971808   0.71562535 -0.88333356  1.0833123   0.7194286 ]\n",
      "Reset environment\n",
      "Episode reward: 922.0215598940849\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.73850036  0.70209813  0.7204357  -0.8775198   1.0873952   0.7242825 ]\n",
      "Reset environment\n",
      "Episode reward: 5685.686553120613\n",
      "Total Steps: 211\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7575533   0.72145927  0.73920506 -0.8565385   1.104324    0.7433371 ]\n",
      "Reset environment\n",
      "Episode reward: 898.2414084672928\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7624399   0.7265142   0.7439117  -0.85083646  1.1085544   0.7482157 ]\n",
      "Reset environment\n",
      "Episode reward: 2028.7659140825272\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7713536   0.73558205  0.7526492  -0.8407641   1.1163417   0.7571143 ]\n",
      "Reset environment\n",
      "Episode reward: 4690.348031282425\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.7866661   0.7510735   0.76775473 -0.8239611   1.1299138   0.7724102 ]\n",
      "Reset environment\n",
      "Episode reward: 4195.068486869335\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8003058   0.76406264  0.7819774  -0.8085895   1.1415169   0.7860128 ]\n",
      "Reset environment\n",
      "Episode reward: 1094.7694755196571\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8058023   0.7695484   0.78746486 -0.8022263   1.1462327   0.79151607]\n",
      "Reset environment\n",
      "Episode reward: 28.43496084213257\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8071127   0.77063763  0.78898335 -0.80097246  1.1475267   0.79282564]\n",
      "Reset environment\n",
      "Episode reward: 294.35139805823565\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8069442   0.77097625  0.78860784 -0.81871533  1.1483135   0.7926478 ]\n",
      "Reset environment\n",
      "Episode reward: 1550.1677690148354\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.81406605  0.7783506   0.79550034 -0.81033623  1.154406    0.799777  ]\n",
      "Reset environment\n",
      "Episode reward: 3347.5696711093187\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.818327    0.78190833  0.80053926 -0.82190377  1.1631173   0.803958  ]\n",
      "Reset environment\n",
      "Episode reward: 3411.1365495324135\n",
      "Total Steps: 276\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.83141625  0.79446465  0.8142297  -0.80680245  1.1742259   0.8171055 ]\n",
      "Reset environment\n",
      "Episode reward: 1082.4966299533844\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.836798    0.79991966  0.81956065 -0.800551    1.1788579   0.8224971 ]\n",
      "Reset environment\n",
      "Episode reward: 1076.4767000675201\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8422084   0.805429    0.8248708  -0.79436404  1.1835519   0.82791203]\n",
      "Reset environment\n",
      "Episode reward: 1067.5979991555214\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.84754294  0.81063783  0.83032584 -0.7883084   1.1881828   0.83326143]\n",
      "Reset environment\n",
      "Episode reward: 4809.625210804865\n",
      "Total Steps: 831\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8625335   0.8242569   0.8466929  -0.77295196  1.2005011   0.84829676]\n",
      "Reset environment\n",
      "Episode reward: 1094.535830795765\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.86791843  0.82974535  0.85196245 -0.7667875   1.2051927   0.8536766 ]\n",
      "Reset environment\n",
      "Episode reward: 1154.2983140349388\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8731668   0.83461654  0.857588   -0.7607574   1.2098484   0.85893154]\n",
      "Reset environment\n",
      "Episode reward: 418.2356618642807\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8760962   0.8377232   0.8603591  -0.7569786   1.2122608   0.86186266]\n",
      "Reset environment\n",
      "Episode reward: 63.37960625812411\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8778088   0.83903456  0.86248803 -0.7541293   1.2135497   0.8635908 ]\n",
      "Reset environment\n",
      "Episode reward: 729.5723850429058\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.88216335  0.844154    0.86612576 -0.7485547   1.2173139   0.8679764 ]\n",
      "Reset environment\n",
      "Episode reward: 1234.2373016774654\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.88752884  0.8502368   0.87080556 -0.742091    1.2222008   0.87335485]\n",
      "Reset environment\n",
      "Episode reward: 2028.6611000299454\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.8943051   0.85861117  0.8763127  -0.7465616   1.2279613   0.8802299 ]\n",
      "Reset environment\n",
      "Episode reward: 1032.5082569364458\n",
      "Total Steps: 556\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.89773166  0.86329514  0.87847733 -0.7428974   1.2293421   0.88373595]\n",
      "Reset environment\n",
      "Episode reward: 1271.494464583695\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.90329987  0.8696498   0.88323426 -0.73571026  1.23352     0.88937473]\n",
      "Reset environment\n",
      "Episode reward: 1932.3733863085508\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9114731   0.87859243  0.8906345  -0.72603524  1.2400755   0.8975794 ]\n",
      "Reset environment\n",
      "Episode reward: 771.9865037202835\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9156626   0.88292897  0.8946864  -0.72097343  1.2435985   0.9017722 ]\n",
      "Reset environment\n",
      "Episode reward: 3604.626193445176\n",
      "Total Steps: 1842\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9226006   0.8905221   0.90142435 -0.72287333  1.2492318   0.90880793]\n",
      "Reset environment\n",
      "Episode reward: 965.227148771286\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.92742944  0.8953078   0.9062997  -0.7171073   1.2532858   0.91363233]\n",
      "Reset environment\n",
      "Episode reward: 4390.393296480179\n",
      "Total Steps: 557\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9403015   0.90728503  0.9201632  -0.70224845  1.2637503   0.92651814]\n",
      "Reset environment\n",
      "Episode reward: 445.8189832866192\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9384429   0.9061858   0.9178314  -0.72729874  1.266558    0.92453283]\n",
      "Reset environment\n",
      "Episode reward: -505.729693621397\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9392713   0.9065531   0.9191505  -0.72539514  1.2670066   0.9253782 ]\n",
      "Reset environment\n",
      "Episode reward: 528.7670322805643\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9425457   0.91002184  0.92223823 -0.721388    1.2698134   0.9286559 ]\n",
      "Reset environment\n",
      "Episode reward: 2383.530400007963\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.95211035  0.9202414   0.9311361  -0.71024007  1.2774183   0.9382483 ]\n",
      "Reset environment\n",
      "Episode reward: 523.2370158135891\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9556537   0.92326343  0.9352269  -0.7056327   1.280322    0.9418011 ]\n",
      "Reset environment\n",
      "Episode reward: 748.9152255654335\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9597276   0.9275155   0.9391294  -0.7007999   1.2838601   0.94588655]\n",
      "Reset environment\n",
      "Episode reward: 847.4053629636765\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9641276   0.9320637   0.9433944  -0.69556385  1.287611    0.95029145]\n",
      "Reset environment\n",
      "Episode reward: 206.59637236595154\n",
      "Total Steps: 3\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9641219  0.9320598  0.9433882 -0.695533   1.2875981  0.9502858]\n",
      "Reset environment\n",
      "Episode reward: 5005.355354487896\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.97278106  0.94086635  0.9520642  -0.6940111   1.2989213   0.9588703 ]\n",
      "Reset environment\n",
      "Episode reward: 2607.496487258468\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9780035  0.9470316  0.9568098 -0.6844147  1.3016654  0.9642897]\n",
      "Reset environment\n",
      "Episode reward: 1626.4327611029148\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.98453087  0.9542667   0.962662   -0.6764371   1.3070885   0.97086465]\n",
      "Reset environment\n",
      "Episode reward: 2709.6802183371037\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9944991   0.96484816  0.97201747 -0.6649284   1.3155613   0.98086655]\n",
      "Reset environment\n",
      "Episode reward: 1372.1943730246276\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.99996686  0.97106993  0.9767655  -0.6578405   1.3197628   0.9864199 ]\n",
      "Reset environment\n",
      "Episode reward: -274.2981523349881\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0003893   0.9711139   0.9776292  -0.6568595   1.3201451   0.98687696]\n",
      "Reset environment\n",
      "Episode reward: -4231.4889218993485\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.98685455  0.9591447   0.9629094  -0.6605266   1.3036479   0.97354245]\n",
      "Reset environment\n",
      "Episode reward: 1567.819701731205\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 0.9933359   0.96548057  0.9695322  -0.6531465   1.3093039   0.9800375 ]\n",
      "Reset environment\n",
      "Episode reward: 6431.994787693024\n",
      "Total Steps: 244\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0126935   0.98514086  0.98858106 -0.6318288   1.3259115   0.99942225]\n",
      "Reset environment\n",
      "Episode reward: 1792.075954079628\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0201211   0.9926149   0.9959599  -0.62324876  1.3322667   1.0068592 ]\n",
      "Reset environment\n",
      "Episode reward: -638.4249060153961\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0185654   0.9911824   0.994295   -0.62473226  1.3310215   1.0053262 ]\n",
      "Reset environment\n",
      "Episode reward: -660.6542069911957\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0182474   0.9907374   0.99414575 -0.62450343  1.3305253   1.0050172 ]\n",
      "Reset environment\n",
      "Episode reward: 1274.9332988262177\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0239573   0.9963998   0.99988973 -0.618132    1.3355054   1.0107253 ]\n",
      "Reset environment\n",
      "Episode reward: -1368.0401361584663\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0185895  0.9930578  0.9928179 -0.6460998  1.3352737  1.005269 ]\n",
      "Reset environment\n",
      "Episode reward: 950.4812002778053\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0231442  0.9974969  0.9974922 -0.6408069  1.3392006  1.0098315]\n",
      "Reset environment\n",
      "Episode reward: 1127.3631268469617\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0231694  0.9991811  0.9959321 -0.6470441  1.3404033  1.0099902]\n",
      "Reset environment\n",
      "Episode reward: 124.51888389885426\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0205654  0.9962779  0.9929153 -0.6622366  1.3417729  1.007259 ]\n",
      "Reset environment\n",
      "Episode reward: 362.6099499166012\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.019305   0.9945143  0.9921059 -0.6820858  1.3439349  1.0060644]\n",
      "Reset environment\n",
      "Episode reward: 944.4738757610321\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0236852   0.99873465  0.99662846 -0.6769933   1.3477286   1.0104282 ]\n",
      "Reset environment\n",
      "Episode reward: 584.8471324443817\n",
      "Total Steps: 23\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0247728  0.9998288  0.997711  -0.6755904  1.3485707  1.0115147]\n",
      "Reset environment\n",
      "Episode reward: 1144.0958263874054\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.029906   1.0050713  1.0027337 -0.6697261  1.3531262  1.0166445]\n",
      "Reset environment\n",
      "Episode reward: 839.5897024124861\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0216447   0.99450886  0.9960619  -0.68848157  1.3496072   1.0084952 ]\n",
      "Reset environment\n",
      "Episode reward: 1070.120417892933\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0264912   0.9995052   1.0007737  -0.68296146  1.353902    1.0133417 ]\n",
      "Reset environment\n",
      "Episode reward: 152.7211568057537\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0282809  1.00097    1.0029135 -0.6802865  1.355452   1.0151355]\n",
      "Reset environment\n",
      "Episode reward: 1793.4556708335876\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0354706   1.0087148   1.0095791  -0.67169046  1.3614284   1.022373  ]\n",
      "Reset environment\n",
      "Episode reward: 1114.4869811534882\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0403873   1.0136482   1.0144845  -0.66599226  1.3656515   1.0272903 ]\n",
      "Reset environment\n",
      "Episode reward: 1697.9582487344742\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0472264   1.0204195   1.021394   -0.65823996  1.3716346   1.0341378 ]\n",
      "Reset environment\n",
      "Episode reward: 1830.971123535186\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0468757  1.02363    1.0177127 -0.6649397  1.3726887  1.0340196]\n",
      "Reset environment\n",
      "Episode reward: 933.8547549843788\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0512148   1.0278642   1.0221312  -0.65986484  1.3763937   1.0383558 ]\n",
      "Reset environment\n",
      "Episode reward: 3459.315224826336\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0607711  1.036924   1.0321528 -0.649014   1.3843275  1.0479059]\n",
      "Reset environment\n",
      "Episode reward: 1132.9930201172829\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0657679  1.0420071  1.0370702 -0.6433199  1.3886962  1.0529021]\n",
      "Reset environment\n",
      "Episode reward: 1723.691500544548\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0725858  1.0489758  1.0437303 -0.635529   1.3946557  1.0597095]\n",
      "Reset environment\n",
      "Episode reward: 1149.1269272565842\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0775871  1.0540099  1.0487154 -0.6297837  1.398956   1.0647134]\n",
      "Reset environment\n",
      "Episode reward: 334.7076156139374\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0770491  1.0529999  1.0491171 -0.6452035  1.402055   1.0646106]\n",
      "Reset environment\n",
      "Episode reward: 1675.4595617055893\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0836841   1.0595943   1.0557935  -0.63768804  1.4078151   1.071252  ]\n",
      "Reset environment\n",
      "Episode reward: 5559.252025067806\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.0995579  1.0756927  1.0714281 -0.6200672  1.4213821  1.0871419]\n",
      "Reset environment\n",
      "Episode reward: 1091.6131991147995\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1043503  1.0805786  1.0761256 -0.6145339  1.4255289  1.0919323]\n",
      "Reset environment\n",
      "Episode reward: 1185.932448387146\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1094407   1.0857633   1.0811199  -0.60875154  1.4299963   1.0970212 ]\n",
      "Reset environment\n",
      "Episode reward: 2311.7863633818924\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1174624  1.093351   1.0895747 -0.5996103  1.4369867  1.1050512]\n",
      "Reset environment\n",
      "Episode reward: 327.1749528646469\n",
      "Total Steps: 16\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1177608  1.0936521  1.0898694 -0.599108   1.4371753  1.1053464]\n",
      "Reset environment\n",
      "Episode reward: 905.3633466362953\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1219485  1.0977437  1.0941672 -0.5941995  1.4407579  1.1095365]\n",
      "Reset environment\n",
      "Episode reward: 1145.7236802577972\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1268842  1.1027725  1.0990336 -0.588549   1.4451478  1.1144819]\n",
      "Reset environment\n",
      "Episode reward: 1936.0394937694073\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.133824   1.1092633  1.10642   -0.5803206  1.4509544  1.1214372]\n",
      "Reset environment\n",
      "Episode reward: 3890.8776341080666\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1470332  1.1226746  1.1193926 -0.5656555  1.4624016  1.1346434]\n",
      "Reset environment\n",
      "Episode reward: 5189.3502740859985\n",
      "Total Steps: 214\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1623117  1.1382422  1.134338  -0.5487243  1.4750671  1.149947 ]\n",
      "Reset environment\n",
      "Episode reward: 902.0263321399689\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1664646  1.1425124  1.1383706 -0.543821   1.4786406  1.1540998]\n",
      "Reset environment\n",
      "Episode reward: 1109.9694609045982\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1712896  1.1472384  1.1432823 -0.5383695  1.4828128  1.1589147]\n",
      "Reset environment\n",
      "Episode reward: 1031.191210091114\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1758666  1.1519369  1.1477302 -0.5331217  1.4868706  1.1634955]\n",
      "Reset environment\n",
      "Episode reward: 1175.8556996583939\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1809272   1.1570952   1.1526803  -0.52737606  1.4912927   1.1685543 ]\n",
      "Reset environment\n",
      "Episode reward: 960.0472678542137\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1852566   1.1615529   1.1568766  -0.52238965  1.4950831   1.1728774 ]\n",
      "Reset environment\n",
      "Episode reward: 1809.2755197286606\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1921009  1.1690071  1.1631176 -0.5140757  1.500536   1.1797676]\n",
      "Reset environment\n",
      "Episode reward: 1114.918206334114\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.1969391  1.1737803  1.1680174 -0.508545   1.5047016  1.1846027]\n",
      "Reset environment\n",
      "Episode reward: 2251.768516123295\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2063605  1.1828852  1.1777581 -0.4979708  1.5130203  1.1940235]\n",
      "Reset environment\n",
      "Episode reward: 1751.7289564013481\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.213056   1.1895692  1.1844778 -0.4902791  1.5188037  1.2007296]\n",
      "Reset environment\n",
      "Episode reward: 1259.6243967413902\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2183799  1.1949033  1.1897835 -0.4842825  1.5234268  1.206056 ]\n",
      "Reset environment\n",
      "Episode reward: 2528.5538415014744\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2266796   1.2036427   1.1976534  -0.47470495  1.5307382   1.2143807 ]\n",
      "Reset environment\n",
      "Episode reward: 901.038179397583\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2307961   1.2076507   1.2018565  -0.46989527  1.5342426   1.2184922 ]\n",
      "Reset environment\n",
      "Episode reward: 2572.603386104107\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2398162   1.2163476   1.2111751  -0.45979986  1.5421662   1.2275158 ]\n",
      "Reset environment\n",
      "Episode reward: 1841.9487105756998\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.24044    1.2180624  1.2108337 -0.4751374  1.5488824  1.2282273]\n",
      "Reset environment\n",
      "Episode reward: 4263.556645750999\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2544637   1.2318543   1.225083   -0.45962894  1.5613686   1.2422373 ]\n",
      "Reset environment\n",
      "Episode reward: 1484.8194415718317\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2598225   1.2378227   1.229812   -0.45287117  1.5656631   1.2476374 ]\n",
      "Reset environment\n",
      "Episode reward: 3904.411191451829\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2730678   1.2506988   1.2434057  -0.43797058  1.5773032   1.2608697 ]\n",
      "Reset environment\n",
      "Episode reward: 3312.894156344235\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2780974   1.2561324   1.2485057  -0.43950808  1.5827885   1.2660513 ]\n",
      "Reset environment\n",
      "Episode reward: 1083.2522700428963\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2827536   1.2608229   1.2531244  -0.43412846  1.5867945   1.2707037 ]\n",
      "Reset environment\n",
      "Episode reward: 979.462417781353\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2870293   1.2649798   1.2575105  -0.42920998  1.5904752   1.2749656 ]\n",
      "Reset environment\n",
      "Episode reward: 2126.107573747635\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2926544   1.2705883   1.2631356  -0.42267078  1.595292    1.2805796 ]\n",
      "Reset environment\n",
      "Episode reward: 1744.6934829354286\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.2992114   1.2771997   1.2696178  -0.41528672  1.6010153   1.2871321 ]\n",
      "Reset environment\n",
      "Episode reward: 620.1957111358643\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3023889  1.2805123  1.2726754 -0.41134    1.6036699  1.2903068]\n",
      "Reset environment\n",
      "Episode reward: 1600.8807203769684\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3085498  1.2866682  1.2788324 -0.4043829  1.609039   1.2964683]\n",
      "Reset environment\n",
      "Episode reward: 1181.8611855506897\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3134806  1.2916688  1.283698  -0.3987483  1.6133204  1.3014017]\n",
      "Reset environment\n",
      "Episode reward: 1293.2985833883286\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3187078  1.2968997  1.2889137 -0.3928653  1.6178781  1.3066278]\n",
      "Reset environment\n",
      "Episode reward: 1313.4817503094673\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3240017   1.3021187   1.2942793  -0.38688573  1.6224827   1.3119309 ]\n",
      "Reset environment\n",
      "Episode reward: 4446.37369530648\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3289535  1.3061138  1.3005537 -0.3899347  1.6303722  1.3170812]\n",
      "Reset environment\n",
      "Episode reward: 2053.2297083735466\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3362705   1.3139968   1.3073093  -0.38119832  1.6362257   1.3244442 ]\n",
      "Reset environment\n",
      "Episode reward: 5847.397405385971\n",
      "Total Steps: 221\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3523113  1.3302686  1.3231405 -0.3634856  1.6501302  1.3405082]\n",
      "Reset environment\n",
      "Episode reward: 1230.9637402892113\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3572978   1.3352858   1.3281053  -0.35780513  1.6544644   1.345502  ]\n",
      "Reset environment\n",
      "Episode reward: 1205.0437639951706\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.362199   1.3402652  1.3329393 -0.3521998  1.6587372  1.3504056]\n",
      "Reset environment\n",
      "Episode reward: 4566.140291472897\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3745066  1.3521451  1.3455874 -0.338548   1.6692626  1.3626903]\n",
      "Reset environment\n",
      "Episode reward: 2768.3439779877663\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3817471   1.3593763   1.3528211  -0.33049703  1.6756123   1.3699219 ]\n",
      "Reset environment\n",
      "Episode reward: 563.6906342506409\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3846558   1.3621161   1.3559172  -0.32692707  1.6780759   1.372829  ]\n",
      "Reset environment\n",
      "Episode reward: 755.1629765407415\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3876805  1.3645866  1.3594869 -0.3227725  1.6804745  1.3758494]\n",
      "Reset environment\n",
      "Episode reward: 346.9489276409149\n",
      "Total Steps: 13\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3880194   1.3649141   1.359838   -0.32227984  1.6807272   1.3761895 ]\n",
      "Reset environment\n",
      "Episode reward: 764.3634229302406\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3915476   1.3685133   1.3633028  -0.31785998  1.6836109   1.3797184 ]\n",
      "Reset environment\n",
      "Episode reward: 990.7623958587646\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3957814   1.3726538   1.367635   -0.31293282  1.6872361   1.3839567 ]\n",
      "Reset environment\n",
      "Episode reward: 3514.9876606576145\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.3999262   1.3768212   1.3716637  -0.31631738  1.6911774   1.3880334 ]\n",
      "Reset environment\n",
      "Episode reward: 4180.006830453873\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4134376  1.3900251  1.3854814 -0.3013041  1.7031261  1.4015644]\n",
      "Reset environment\n",
      "Episode reward: 3472.7969529628754\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4249421   1.4011592   1.3973589  -0.28831825  1.7132635   1.4130722 ]\n",
      "Reset environment\n",
      "Episode reward: 2359.605002824217\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4328079  1.4086335  1.4055793 -0.2792899  1.7199904  1.4209172]\n",
      "Reset environment\n",
      "Episode reward: 1747.950215101242\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4392109   1.4147255   1.4122937  -0.27210656  1.7256633   1.4273263 ]\n",
      "Reset environment\n",
      "Episode reward: 1658.3156262636185\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.445406    1.420836    1.4185758  -0.26512697  1.7310979   1.4335195 ]\n",
      "Reset environment\n",
      "Episode reward: 1909.4921652674675\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4524279  1.427823   1.4256371 -0.257376   1.7372898  1.4405365]\n",
      "Reset environment\n",
      "Episode reward: 2818.0063560158014\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4608839   1.4367051   1.433664   -0.24767454  1.7447013   1.449008  ]\n",
      "Reset environment\n",
      "Episode reward: 1741.3965665698051\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4673288   1.4432548   1.4400014  -0.24042675  1.75035     1.4554476 ]\n",
      "Reset environment\n",
      "Episode reward: 2067.80811689049\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4740326   1.4494979   1.4471406  -0.23246458  1.7558546   1.4621462 ]\n",
      "Reset environment\n",
      "Episode reward: 992.0142188668251\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4782064   1.4535655   1.451419   -0.22759739  1.7594539   1.4663174 ]\n",
      "Reset environment\n",
      "Episode reward: 1990.4429829991423\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4750648  1.4506726  1.4480016 -0.2397186  1.756991   1.4632406]\n",
      "Reset environment\n",
      "Episode reward: 1653.5186544004828\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4809012   1.4570252   1.4533316  -0.23251286  1.7615846   1.4691288 ]\n",
      "Reset environment\n",
      "Episode reward: 5445.212039113045\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.4955437   1.4712805   1.468293   -0.21628472  1.7743788   1.4837375 ]\n",
      "Reset environment\n",
      "Episode reward: 227.30645990371704\n",
      "Total Steps: 23\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.495362    1.4711082   1.4681057  -0.21644141  1.7742202   1.4835552 ]\n",
      "Reset environment\n",
      "Episode reward: 4323.304154098034\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5067502   1.482697    1.4792682  -0.20376267  1.7839407   1.4949577 ]\n",
      "Reset environment\n",
      "Episode reward: 2750.097475681454\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5153102   1.4909163   1.4881607  -0.19408222  1.7912662   1.5035201 ]\n",
      "Reset environment\n",
      "Episode reward: 3091.933503560722\n",
      "Total Steps: 621\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5205389   1.4953507   1.4943156  -0.18740481  1.7951182   1.5088089 ]\n",
      "Reset environment\n",
      "Episode reward: 2788.1877015531063\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5296103   1.5041257   1.5036796  -0.17704666  1.802952    1.5178939 ]\n",
      "Reset environment\n",
      "Episode reward: 2012.0390959978104\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5366997   1.5111519   1.5108314  -0.16921884  1.8091887   1.5249833 ]\n",
      "Reset environment\n",
      "Episode reward: -722.7651060819626\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5352526   1.509686    1.5094178  -0.17081757  1.8077233   1.5235426 ]\n",
      "Reset environment\n",
      "Episode reward: 1352.6085308790207\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5404825   1.5149535   1.5146252  -0.16491318  1.8123393   1.5287719 ]\n",
      "Reset environment\n",
      "Episode reward: 692.9805554747581\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5437125   1.518306    1.5177386  -0.16093647  1.8150616   1.5319989 ]\n",
      "Reset environment\n",
      "Episode reward: 1823.8979382514954\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5502435   1.5247866   1.5243181  -0.15357438  1.8207494   1.538529  ]\n",
      "Reset environment\n",
      "Episode reward: 1482.5782355740666\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5552061   1.5292869   1.5297322  -0.14738652  1.8246808   1.5434867 ]\n",
      "Reset environment\n",
      "Episode reward: 1332.9916433095932\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5602391   1.5341923   1.5348922  -0.14154917  1.8290234   1.5485208 ]\n",
      "Reset environment\n",
      "Episode reward: 924.3684822320938\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5641062   1.538135    1.5386904  -0.13690938  1.8322799   1.5523868 ]\n",
      "Reset environment\n",
      "Episode reward: -512.5077489838004\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5635257   1.5377434   1.5379558  -0.13729574  1.8319213   1.5518277 ]\n",
      "Reset environment\n",
      "Episode reward: 2281.9340402036905\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5707643   1.545414    1.544752   -0.12884118  1.8383195   1.5590863 ]\n",
      "Reset environment\n",
      "Episode reward: 1191.1141641959548\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5667264   1.5418916   1.5405251  -0.15178597  1.8377587   1.5551407 ]\n",
      "Reset environment\n",
      "Episode reward: 7017.725404202938\n",
      "Total Steps: 261\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.585851    1.5607045   1.5599133  -0.13085586  1.8544506   1.5742588 ]\n",
      "Reset environment\n",
      "Episode reward: -248.59080350399017\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5860577   1.5610039   1.5600483  -0.13054216  1.8548545   1.5744668 ]\n",
      "Reset environment\n",
      "Episode reward: 1427.1569812502712\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5906725   1.5661578   1.5641016  -0.12459588  1.8585474   1.5791329 ]\n",
      "Reset environment\n",
      "Episode reward: 852.003759086132\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5942777   1.5697417   1.5677341  -0.12019141  1.8615175   1.5827366 ]\n",
      "Reset environment\n",
      "Episode reward: 921.8760042488575\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.5985888   1.5736488   1.572456   -0.11499313  1.8651055   1.5870538 ]\n",
      "Reset environment\n",
      "Episode reward: 4042.000436335802\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6114367   1.5861298   1.5856457  -0.10065679  1.8764259   1.5998858 ]\n",
      "Reset environment\n",
      "Episode reward: 1345.3533446397632\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6041814  1.5793086  1.5781726 -0.1214752  1.8716079  1.5927343]\n",
      "Reset environment\n",
      "Episode reward: 659.1057329177856\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.607245    1.5822027   1.5814229  -0.11780734  1.8742422   1.5957991 ]\n",
      "Reset environment\n",
      "Episode reward: 2742.430842936039\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6140305   1.589003    1.5881958  -0.11026014  1.8802091   1.602588  ]\n",
      "Reset environment\n",
      "Episode reward: 3591.675659865141\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.625432    1.6000671   1.5999424  -0.09744298  1.8902634   1.6140157 ]\n",
      "Reset environment\n",
      "Episode reward: 1222.9642614722252\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6302075  1.60486    1.6047065 -0.0919608  1.8944023  1.618799 ]\n",
      "Reset environment\n",
      "Episode reward: -799.8584787845612\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6286167  1.6033311  1.6031045 -0.0936299  1.8932047  1.6172231]\n",
      "Reset environment\n",
      "Episode reward: 2367.292510359548\n",
      "Total Steps: 586\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6312591   1.6057888   1.6060331  -0.08945316  1.8949403   1.6199167 ]\n",
      "Reset environment\n",
      "Episode reward: 887.9909440875053\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6349859   1.6096423   1.6096469  -0.08499369  1.8981491   1.6236478 ]\n",
      "Reset environment\n",
      "Episode reward: 1006.6127055883408\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6390431   1.6137706   1.6136193  -0.08029787  1.9016634   1.6277022 ]\n",
      "Reset environment\n",
      "Episode reward: 977.7446713596582\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6424812   1.6167318   1.6175504  -0.07569873  1.9042767   1.6311393 ]\n",
      "Reset environment\n",
      "Episode reward: 991.6140838861465\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.646455    1.6208098   1.6214279  -0.07102589  1.9077177   1.6351165 ]\n",
      "Reset environment\n",
      "Episode reward: 998.0537697076797\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6484776   1.6228385   1.6234509  -0.06850582  1.9094018   1.6371399 ]\n",
      "Reset environment\n",
      "Episode reward: 1406.3846733346581\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6448983   1.6186585   1.6203616  -0.08471703  1.9066358   1.6338099 ]\n",
      "Reset environment\n",
      "Episode reward: 995.8874557614326\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6487659   1.62283     1.6239454  -0.08014857  1.9101642   1.6376915 ]\n",
      "Reset environment\n",
      "Episode reward: 2378.256005704403\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6561385   1.630718    1.6308028  -0.07141994  1.9161345   1.6451029 ]\n",
      "Reset environment\n",
      "Episode reward: 1322.1249668002129\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6610649   1.6357267   1.6356395  -0.06580005  1.9204364   1.6500314 ]\n",
      "Reset environment\n",
      "Episode reward: 1208.9914009571075\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6656538   1.6404095   1.6401316  -0.06063253  1.9245771   1.6546155 ]\n",
      "Reset environment\n",
      "Episode reward: 620.2048716545105\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6687869   1.6430111   1.643793   -0.05650145  1.9270222   1.6577337 ]\n",
      "Reset environment\n",
      "Episode reward: 4019.407385852188\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.666293    1.6426903   1.6396637  -0.06408525  1.9267119   1.6556151 ]\n",
      "Reset environment\n",
      "Episode reward: 170.4079569876194\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.667542    1.6442215   1.6406482  -0.06226182  1.9280171   1.6568872 ]\n",
      "Reset environment\n",
      "Episode reward: 589.6183371543884\n",
      "Total Steps: 23\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6684968   1.645179    1.6415998  -0.06102604  1.928748    1.6578422 ]\n",
      "Reset environment\n",
      "Episode reward: 3914.391545209568\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.673239    1.6504999   1.646135   -0.05864625  1.9318818   1.6627729 ]\n",
      "Reset environment\n",
      "Episode reward: 1167.2781276106834\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6776742   1.6550103   1.6504922  -0.05355905  1.9357487   1.6672071 ]\n",
      "Reset environment\n",
      "Episode reward: 3364.5281945466995\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6861608   1.6636554   1.6587973  -0.04422994  1.9427661   1.6756978 ]\n",
      "Reset environment\n",
      "Episode reward: 5173.122001171112\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6919858   1.6691246   1.6654513  -0.04815971  1.9515492   1.6815736 ]\n",
      "Reset environment\n",
      "Episode reward: 1466.256209502928\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6866268   1.66681     1.6576436  -0.05356231  1.9463209   1.6767478 ]\n",
      "Reset environment\n",
      "Episode reward: 798.0171815752983\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6899301   1.6700487   1.6610299  -0.04950996  1.949079    1.6800587 ]\n",
      "Reset environment\n",
      "Episode reward: 887.6655216217041\n",
      "Total Steps: 29\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6916775   1.671792    1.6627839  -0.04747408  1.9505635   1.6818066 ]\n",
      "Reset environment\n",
      "Episode reward: 3627.6252428889275\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.690438    1.6700222   1.6626235  -0.06426066  1.9554819   1.6805545 ]\n",
      "Reset environment\n",
      "Episode reward: 1026.299951851368\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6943935   1.6740799   1.6664776  -0.05965812  1.95894     1.6845112 ]\n",
      "Reset environment\n",
      "Episode reward: 4073.5935556292534\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6858224   1.664944    1.6589876  -0.08116592  1.9550406   1.6759325 ]\n",
      "Reset environment\n",
      "Episode reward: 1312.92333060503\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6905451  1.6697313  1.6636566 -0.0758549  1.9592217  1.6806574]\n",
      "Reset environment\n",
      "Episode reward: 1256.6909021735191\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6932132   1.6723901   1.6663301  -0.07273339  1.9615126   1.6833235 ]\n",
      "Reset environment\n",
      "Episode reward: 1544.9513866305351\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6985508   1.6776634   1.6717311  -0.06672378  1.9661651   1.6886611 ]\n",
      "Reset environment\n",
      "Episode reward: 4939.282771348953\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7107824   1.689958    1.6839098  -0.05326267  1.977026    1.7008858 ]\n",
      "Reset environment\n",
      "Episode reward: -6232.8020707459655\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6826408   1.6627029   1.6553242  -0.07743968  1.947749    1.6730076 ]\n",
      "Reset environment\n",
      "Episode reward: 1008.7401106357574\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6865053   1.6664883   1.6592635  -0.07292947  1.9510509   1.6768745 ]\n",
      "Reset environment\n",
      "Episode reward: 4758.560697436333\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.6984589   1.6785876   1.6710558  -0.05966131  1.9614804   1.6888216 ]\n",
      "Reset environment\n",
      "Episode reward: 4005.8712007403374\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7082275   1.6882752   1.6808845  -0.04855409  1.9700001   1.6985873 ]\n",
      "Reset environment\n",
      "Episode reward: 3479.1431764364243\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7185507   1.6989783   1.6907835  -0.03700832  1.9788852   1.7089158 ]\n",
      "Reset environment\n",
      "Episode reward: 3365.1008933633566\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7284286   1.7091886   1.7003111  -0.02588963  1.987279    1.7188104 ]\n",
      "Reset environment\n",
      "Episode reward: 1562.496318101883\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7319303   1.7126446   1.7038664  -0.02173409  1.9902142   1.7223184 ]\n",
      "Reset environment\n",
      "Episode reward: 2120.2209379673004\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7387277   1.719491    1.7105976  -0.01410905  1.9961811   1.7291088 ]\n",
      "Reset environment\n",
      "Episode reward: 2233.082603633404\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.743761    1.7245299   1.7156165  -0.00835062  2.0005383   1.7341417 ]\n",
      "Reset environment\n",
      "Episode reward: -1014.1564750671387\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7419711   1.7228374   1.7137613  -0.01014012  1.9990727   1.7323635 ]\n",
      "Reset environment\n",
      "Episode reward: 2852.899694979191\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [ 1.7502481e+00  1.7314757e+00  1.7216412e+00 -7.1181200e-04\n",
      "  2.0063739e+00  1.7406355e+00]\n",
      "Reset environment\n",
      "Episode reward: 1098.1776938438416\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7524928e+00 1.7337070e+00 1.7238848e+00 1.9661547e-03 2.0082557e+00\n",
      " 1.7428757e+00]\n",
      "Reset environment\n",
      "Episode reward: 3921.384796321392\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7618134  1.742955   1.7332417  0.01237425 2.01632    1.752181  ]\n",
      "Reset environment\n",
      "Episode reward: 5056.719777286053\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7657503  1.7463934  1.7381154  0.00351761 2.0245805  1.7562164 ]\n",
      "Reset environment\n",
      "Episode reward: 6147.95398414135\n",
      "Total Steps: 221\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7813313  1.7618886  1.7537618  0.02063257 2.0382652  1.7717782 ]\n",
      "Reset environment\n",
      "Episode reward: 2119.1472557745874\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7801532  1.7599326  1.7534338  0.00988739 2.038641   1.7709223 ]\n",
      "Reset environment\n",
      "Episode reward: 1044.9416134357452\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7840421  1.763723   1.757412   0.01433716 2.0419946  1.7748103 ]\n",
      "Reset environment\n",
      "Episode reward: 2839.161052644253\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.791612   1.7715122  1.7647654  0.02307558 2.048738   1.7823741 ]\n",
      "Reset environment\n",
      "Episode reward: 3292.7864937587\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7950013  1.7754796  1.7676555  0.01950735 2.052838   1.7857313 ]\n",
      "Reset environment\n",
      "Episode reward: 946.5665454268456\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7985913  1.7791859  1.7711363  0.02374381 2.0559437  1.7893162 ]\n",
      "Reset environment\n",
      "Episode reward: 6020.963376715779\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8062415  1.7865297  1.7798381  0.03252055 2.0650423  1.797199  ]\n",
      "Reset environment\n",
      "Episode reward: 3509.974499925971\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8163822  1.7963631  1.7902714  0.04407622 2.073898   1.8073443 ]\n",
      "Reset environment\n",
      "Episode reward: 4385.063141887775\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8182946  1.7978189  1.7927012  0.04113409 2.0774121  1.8094219 ]\n",
      "Reset environment\n",
      "Episode reward: 1898.0436164140701\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8242742  1.8037498  1.7987378  0.04786064 2.0826821  1.8154069 ]\n",
      "Reset environment\n",
      "Episode reward: -20132.7947409926\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7762696  1.7559531  1.7510012  0.00784945 2.0332222  1.7673306 ]\n",
      "Reset environment\n",
      "Episode reward: -11.29664508253336\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7768607  1.7562139  1.7519544  0.00905702 2.033701   1.7679346 ]\n",
      "Reset environment\n",
      "Episode reward: 6275.518482685089\n",
      "Total Steps: 228\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7919157  1.7713981  1.7668518  0.02552056 2.0469067  1.7829565 ]\n",
      "Reset environment\n",
      "Episode reward: 4220.285177355632\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.795556   1.7751309  1.7702521  0.02415428 2.0518258  1.7865971 ]\n",
      "Reset environment\n",
      "Episode reward: 1182.714602779597\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.7911015  1.7701226  1.7665298  0.01356193 2.048201   1.7820753 ]\n",
      "Reset environment\n",
      "Episode reward: 1044.4652794003487\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.794835   1.7739319  1.7701805  0.01789754 2.0514543  1.7858045 ]\n",
      "Reset environment\n",
      "Episode reward: 2998.4946718215942\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8035119  1.7829033  1.7785378  0.02766454 2.0590534  1.7944865 ]\n",
      "Reset environment\n",
      "Episode reward: -372.66262131696567\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8033525  1.782562   1.7785745  0.02800756 2.0589063  1.7943261 ]\n",
      "Reset environment\n",
      "Episode reward: 4691.518983006477\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8142818  1.7934415  1.7895403  0.04010361 2.0685596  1.805243  ]\n",
      "Reset environment\n",
      "Episode reward: 1578.5045817121863\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.818709   1.7974237  1.7944266  0.04565764 2.0721247  1.8096809 ]\n",
      "Reset environment\n",
      "Episode reward: 3356.8823877722025\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.828074   1.8064919  1.8040392  0.05608406 2.0803316  1.8190264 ]\n",
      "Reset environment\n",
      "Episode reward: 2557.727189332247\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.835146   1.8139725  1.8106705  0.06431681 2.0860677  1.8261125 ]\n",
      "Reset environment\n",
      "Episode reward: 2266.665677756071\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.833419   1.8122485  1.8089631  0.05031932 2.0874732  1.8242645 ]\n",
      "Reset environment\n",
      "Episode reward: 118.75950828939676\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8344135  1.8134815  1.8097456  0.05188301 2.0884614  1.825277  ]\n",
      "Reset environment\n",
      "Episode reward: 1196.0957357138395\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8384007  1.817856   1.8133525  0.05667592 2.09199    1.8292747 ]\n",
      "Reset environment\n",
      "Episode reward: 1309.4357456564903\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8427773  1.8222723  1.8177004  0.06158227 2.0958579  1.8336527 ]\n",
      "Reset environment\n",
      "Episode reward: 4877.274592757225\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8541441  1.8334899  1.8292058  0.07411937 2.106046   1.845011  ]\n",
      "Reset environment\n",
      "Episode reward: 1689.934378489852\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8592672  1.839055   1.8338857  0.08040139 2.110153   1.8501515 ]\n",
      "Reset environment\n",
      "Episode reward: 4672.783572614193\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8719282  1.851514   1.8467169  0.09436833 2.1214674  1.8628159 ]\n",
      "Reset environment\n",
      "Episode reward: 1128.8974581956863\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8758302  1.85551    1.8505327  0.09883101 2.12491    1.8667231 ]\n",
      "Reset environment\n",
      "Episode reward: 4528.719619870186\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8863503 1.8659985 1.8610824 0.1106207 2.1341622 1.8772403]\n",
      "Reset environment\n",
      "Episode reward: 471.15403628349304\n",
      "Total Steps: 17\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8869231  1.8665644  1.8616594  0.11138205 2.1345923  1.8778154 ]\n",
      "Reset environment\n",
      "Episode reward: 462.4254318475723\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8888973  1.8688098  1.8633943  0.11386479 2.136411   1.8798078 ]\n",
      "Reset environment\n",
      "Episode reward: 4677.577885150909\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.8995247  1.8795903  1.8738556  0.12570089 2.1457107  1.8904313 ]\n",
      "Reset environment\n",
      "Episode reward: 1511.7602544277906\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9047881 1.8852292 1.8787559 0.1319121 2.1502597 1.8957036]\n",
      "Reset environment\n",
      "Episode reward: 2749.937875121832\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9125264  1.893324   1.8861235  0.14077255 2.156926   1.9034503 ]\n",
      "Reset environment\n",
      "Episode reward: 1504.6009291410446\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.915634   1.896458   1.88921    0.14430244 2.1596587  1.9065552 ]\n",
      "Reset environment\n",
      "Episode reward: 4743.342007577419\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9285555  1.9092647  1.902227   0.15849431 2.171057   1.9194682 ]\n",
      "Reset environment\n",
      "Episode reward: 1236.6997281312943\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9327364  1.9135183  1.9063362  0.16323487 2.174763   1.9236453 ]\n",
      "Reset environment\n",
      "Episode reward: 1350.8814306259155\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.937211   1.9179398  1.9108546  0.16825007 2.178696   1.9281155 ]\n",
      "Reset environment\n",
      "Episode reward: 4327.844084262848\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9470748  1.9277368  1.9207603  0.17912677 2.1873832  1.9379535 ]\n",
      "Reset environment\n",
      "Episode reward: 3747.9017091318965\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9483296  1.9297504  1.9215626  0.16957814 2.1898184  1.9393034 ]\n",
      "Reset environment\n",
      "Episode reward: 1222.4414730072021\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9524257  1.9337953  1.9257133  0.17420621 2.193391   1.9433936 ]\n",
      "Reset environment\n",
      "Episode reward: 3860.4567594081163\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9630833  1.9441639  1.9366465  0.18613705 2.202768   1.9540399 ]\n",
      "Reset environment\n",
      "Episode reward: 3878.1158323145937\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.963255   1.9437475  1.9376308  0.17895722 2.2040586  1.9543902 ]\n",
      "Reset environment\n",
      "Episode reward: 349.1653977036476\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9550407  1.936112   1.9289025  0.15215935 2.201636   1.9461254 ]\n",
      "Reset environment\n",
      "Episode reward: 3838.592037975788\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9636785  1.9445426  1.937741   0.16171329 2.20917    1.9547544 ]\n",
      "Reset environment\n",
      "Episode reward: 4558.441802561283\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9740447  1.9548414  1.9481748  0.17314872 2.218476   1.9651237 ]\n",
      "Reset environment\n",
      "Episode reward: 491.0271861553192\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9766995  1.9570495  1.9512801  0.17678553 2.2204247  1.9677753 ]\n",
      "Reset environment\n",
      "Episode reward: -992.430547952652\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9753494 1.9554195 1.9502463 0.1755871 2.2193563 1.9664307]\n",
      "Reset environment\n",
      "Episode reward: 61.862475007772446\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9758785  1.95616    1.9505795  0.17662174 2.2199674  1.9669721 ]\n",
      "Reset environment\n",
      "Episode reward: -6232.273530918872\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9478441  1.9292976  1.9217869  0.14585656 2.1925292  1.9390875 ]\n",
      "Reset environment\n",
      "Episode reward: 668.389189273119\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9500716  1.9312494  1.9242924  0.14859086 2.1944253  1.9413134 ]\n",
      "Reset environment\n",
      "Episode reward: 1309.8127313256264\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9543049  1.9355279  1.928502   0.15334249 2.198166   1.9455535 ]\n",
      "Reset environment\n",
      "Episode reward: 3999.7919389551753\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9559141  1.936836   1.9303626  0.15340784 2.1988523  1.9471587 ]\n",
      "Reset environment\n",
      "Episode reward: 1748.6185400485992\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9611719  1.9422381  1.9354866  0.15938298 2.2034857  1.9524205 ]\n",
      "Reset environment\n",
      "Episode reward: 5721.667572379112\n",
      "Total Steps: 213\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9738843  1.9550948  1.9480302  0.17343488 2.214637   1.9651233 ]\n",
      "Reset environment\n",
      "Episode reward: 3560.7987845540047\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9745182  1.9558523  1.9486051  0.16237222 2.215941   1.9657283 ]\n",
      "Reset environment\n",
      "Episode reward: 2934.479249715805\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.982316   1.9639748  1.956079   0.17113808 2.2228732  1.9735228 ]\n",
      "Reset environment\n",
      "Episode reward: -125.74560946598649\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9827722  1.9641802  1.9567969  0.17201252 2.2232625  1.9739743 ]\n",
      "Reset environment\n",
      "Episode reward: 1216.4064492583275\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9867212  1.9682035  1.9606869  0.17650326 2.226739   1.9779279 ]\n",
      "Reset environment\n",
      "Episode reward: 5086.008067369461\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [1.9981326  1.9796265  1.972086   0.18909681 2.2367995  1.9893359 ]\n",
      "Reset environment\n",
      "Episode reward: 636.7981429472566\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0006723  1.9824928  1.9743202  0.19238113 2.239009   1.9918876 ]\n",
      "Reset environment\n",
      "Episode reward: 4746.002214848995\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0113266  1.9930179  1.9851143  0.20420317 2.2484727  2.0025425 ]\n",
      "Reset environment\n",
      "Episode reward: 3924.0386620759964\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0217195  2.0032027  1.9957209  0.21570437 2.2577355  2.0129325 ]\n",
      "Reset environment\n",
      "Episode reward: 3043.417244475335\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0298254  2.0116227  2.003474   0.22490266 2.2645304  2.0210373 ]\n",
      "Reset environment\n",
      "Episode reward: 1190.2711758613586\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0336878  2.0155344  2.0072823  0.22936882 2.2678766  2.0249004 ]\n",
      "Reset environment\n",
      "Episode reward: 848.5127346515656\n",
      "Total Steps: 29\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0351026  2.0169492  2.008695   0.23105522 2.2690697  2.0263124 ]\n",
      "Reset environment\n",
      "Episode reward: 1728.1437419652939\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0402524  2.0220537  2.0138812  0.23693207 2.2735283  2.0314612 ]\n",
      "Reset environment\n",
      "Episode reward: 252.25776267051697\n",
      "Total Steps: 7\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.040312   2.0221155  2.0139391  0.23706633 2.2735493  2.0315208 ]\n",
      "Reset environment\n",
      "Episode reward: 2746.2850331773516\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0387805 2.0198786 2.0137093 0.2272718 2.2747414 2.0302389]\n",
      "Reset environment\n",
      "Episode reward: -158.53511023521423\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0390222  2.0202804  2.0138083  0.22779486 2.2751553  2.0304844 ]\n",
      "Reset environment\n",
      "Episode reward: -2120.3104852589313\n",
      "Total Steps: 854\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0286632  2.0088303  2.0047507  0.22134566 2.2634702  2.0200949 ]\n",
      "Reset environment\n",
      "Episode reward: 1104.8275986015797\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0321229  2.0126526  2.007857   0.22578858 2.2663581  2.023566  ]\n",
      "Reset environment\n",
      "Episode reward: 407.13693925738335\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0245569  2.0046015  2.0010214  0.20040719 2.2622328  2.0161068 ]\n",
      "Reset environment\n",
      "Episode reward: 3998.1401500701904\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0333595  2.013273   2.0099385  0.21033524 2.270014   2.0249004 ]\n",
      "Reset environment\n",
      "Episode reward: 4148.792489886284\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0438967  2.023995   2.0202703  0.22212738 2.2792492  2.0354288 ]\n",
      "Reset environment\n",
      "Episode reward: 3045.1799814254045\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0517373 2.0315313 2.0283864 0.2309514 2.285988  2.0432584]\n",
      "Reset environment\n",
      "Episode reward: 2875.30199236481\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0592587  2.0393844  2.0355332  0.23957229 2.2923243  2.0507658 ]\n",
      "Reset environment\n",
      "Episode reward: 5958.208185493946\n",
      "Total Steps: 215\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0722713  2.0522766  2.048682   0.25398913 2.3038886  2.063774  ]\n",
      "Reset environment\n",
      "Episode reward: 5049.349656164646\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.083414  2.063596  2.0596514 0.2663033 2.3136315 2.0749102]\n",
      "Reset environment\n",
      "Episode reward: 4681.216792881489\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.0934374 2.0735784 2.0697298 0.2773972 2.3225024 2.0849338]\n",
      "Reset environment\n",
      "Episode reward: 5198.788404941559\n",
      "Total Steps: 250\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.104366  2.084202  2.0809436 0.2896638 2.3316336 2.0958557]\n",
      "Reset environment\n",
      "Episode reward: 1062.9201214909554\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1075187  2.0870752  2.0843966  0.29335737 2.3342886  2.0990086 ]\n",
      "Reset environment\n",
      "Episode reward: 2968.6082534193993\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.115145  2.0943716 2.0923223 0.3020566 2.3408332 2.1066313]\n",
      "Reset environment\n",
      "Episode reward: 411.13292172178626\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1169348 2.0965166 2.0937665 0.3044985 2.3422947 2.1084242]\n",
      "Reset environment\n",
      "Episode reward: 7038.7945737838745\n",
      "Total Steps: 248\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1324143  2.112093   2.1091487  0.32133016 2.3560052  2.1239066 ]\n",
      "Reset environment\n",
      "Episode reward: 1209.1431593298912\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.136248   2.1159956  2.1129148  0.32569373 2.359384   2.1277413 ]\n",
      "Reset environment\n",
      "Episode reward: 1294.3441467285156\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1402416  2.1200209  2.1168835  0.33023527 2.3628964  2.131737  ]\n",
      "Reset environment\n",
      "Episode reward: 4551.890473246574\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1420088 2.1213722 2.119172  0.3266012 2.3651385 2.1335878]\n",
      "Reset environment\n",
      "Episode reward: 707.3093247786164\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1441367  2.1231263  2.1216645  0.32972804 2.366641   2.1357088 ]\n",
      "Reset environment\n",
      "Episode reward: 5756.067164897919\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.147673   2.1269352  2.1251714  0.32419604 2.3717463  2.139309  ]\n",
      "Reset environment\n",
      "Episode reward: 5018.507334887981\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1586497  2.1380796  2.13597    0.33629227 2.3814425  2.150288  ]\n",
      "Reset environment\n",
      "Episode reward: 1513.0754756841343\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.162757   2.141791   2.140475   0.34144294 2.3846245  2.154398  ]\n",
      "Reset environment\n",
      "Episode reward: 5385.92980421707\n",
      "Total Steps: 292\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1738694  2.152919   2.1515577  0.35385597 2.3944528  2.165491  ]\n",
      "Reset environment\n",
      "Episode reward: 5374.731275141239\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1854422  2.1645958  2.1630456  0.36663315 2.4048843  2.177067  ]\n",
      "Reset environment\n",
      "Episode reward: 5738.002547621727\n",
      "Total Steps: 215\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.1976779  2.1769786  2.1751223  0.38015997 2.415598   2.1893005 ]\n",
      "Reset environment\n",
      "Episode reward: 6238.8590713739395\n",
      "Total Steps: 225\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2110972 2.1905591 2.1883805 0.394922  2.427465  2.2026982]\n",
      "Reset environment\n",
      "Episode reward: 1258.004901587963\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2149694 2.1944804 2.1922104 0.399304  2.4308808 2.206572 ]\n",
      "Reset environment\n",
      "Episode reward: 1204.2636775374413\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.209492   2.188736   2.1872313  0.37409252 2.4287796  2.201258  ]\n",
      "Reset environment\n",
      "Episode reward: 1242.6351540461183\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2127738  2.191585   2.190934   0.37834936 2.4313822  2.2045283 ]\n",
      "Reset environment\n",
      "Episode reward: 1980.842104434967\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.216664  2.1954372 2.194853  0.3826912 2.4347682 2.2084146]\n",
      "Reset environment\n",
      "Episode reward: 2435.444957345724\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2230446  2.2015164  2.2015083  0.38993043 2.4403765  2.2147915 ]\n",
      "Reset environment\n",
      "Episode reward: 3794.938423658954\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2234128  2.2018356  2.2017195  0.38459682 2.4399383  2.215077  ]\n",
      "Reset environment\n",
      "Episode reward: 704.8199174476322\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2258325  2.2045918  2.2038004  0.38766834 2.4420247  2.2174895 ]\n",
      "Reset environment\n",
      "Episode reward: 2275.276388466358\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.230398  2.209135  2.2083786 0.3927913 2.4460452 2.2220523]\n",
      "Reset environment\n",
      "Episode reward: 1402.9195566475391\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2342489  2.2133505  2.211865   0.39749092 2.449216   2.2259133 ]\n",
      "Reset environment\n",
      "Episode reward: 3953.281108647585\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2426217  2.2214859  2.2204165  0.40674993 2.456607   2.234273  ]\n",
      "Reset environment\n",
      "Episode reward: -684.5638241767883\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2416437  2.2205265  2.2194319  0.40588963 2.4556146  2.233296  ]\n",
      "Reset environment\n",
      "Episode reward: 5792.9730489254\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2542436  2.2332249  2.2318974  0.41969153 2.4666984  2.2458951 ]\n",
      "Reset environment\n",
      "Episode reward: 1847.3980797827244\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.259019   2.23771    2.2369437  0.42513928 2.4708464  2.2506588 ]\n",
      "Reset environment\n",
      "Episode reward: -312.7441509962082\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2589774  2.2377584  2.2368279  0.42510423 2.471076   2.2506187 ]\n",
      "Reset environment\n",
      "Episode reward: 3219.183478832245\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2656274 2.244341  2.2435396 0.4324992 2.4769778 2.2572577]\n",
      "Reset environment\n",
      "Episode reward: 4757.565127622336\n",
      "Total Steps: 739\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2721026 2.25036   2.2504709 0.4411496 2.4822044 2.2637053]\n",
      "Reset environment\n",
      "Episode reward: 349.60961939394474\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2734797  2.2520611  2.2515347  0.44307193 2.4833274  2.265095  ]\n",
      "Reset environment\n",
      "Episode reward: 636.3953813314438\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.275992  2.2541757 2.2544556 0.4464305 2.4852965 2.2676072]\n",
      "Reset environment\n",
      "Episode reward: 5507.544317543507\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2876086  2.2658827  2.2660081  0.45919383 2.4956045  2.279234  ]\n",
      "Reset environment\n",
      "Episode reward: 2048.845181107521\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2931736  2.2715275  2.2715042  0.46543896 2.5005376  2.2848015 ]\n",
      "Reset environment\n",
      "Episode reward: 2271.04092001915\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.2975218  2.2758906  2.27584    0.47030833 2.5043578  2.2891526 ]\n",
      "Reset environment\n",
      "Episode reward: -1037.9077639579773\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.295443   2.2736845  2.2739112  0.46808332 2.5027044  2.287067  ]\n",
      "Reset environment\n",
      "Episode reward: 4149.56171309948\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3041935  2.2823079  2.2827666  0.47774342 2.5105329  2.2958052 ]\n",
      "Reset environment\n",
      "Episode reward: 2942.919087290764\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.311563  2.2899182 2.2898557 0.4860991 2.516843  2.3031642]\n",
      "Reset environment\n",
      "Episode reward: 4086.6934095323086\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3216212  2.2996807  2.3001866  0.49733046 2.5257306  2.3132172 ]\n",
      "Reset environment\n",
      "Episode reward: 2681.290194928646\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.328357  2.3060677 2.307205  0.5050615 2.531415  2.319935 ]\n",
      "Reset environment\n",
      "Episode reward: 4624.045268595219\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3396585 2.3172555 2.3186278 0.5175553 2.541424  2.3312414]\n",
      "Reset environment\n",
      "Episode reward: 5186.776762187481\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3504398 2.3281524 2.3293061 0.5294288 2.5509381 2.3420203]\n",
      "Reset environment\n",
      "Episode reward: 6441.35950666666\n",
      "Total Steps: 233\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3639674  2.3416898  2.3428624  0.54437065 2.5629463  2.3555636 ]\n",
      "Reset environment\n",
      "Episode reward: 2860.5409355536103\n",
      "Total Steps: 908\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3656185 2.3431487 2.3447936 0.5472771 2.563831  2.3572202]\n",
      "Reset environment\n",
      "Episode reward: 5943.631302416325\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3781364  2.3555615  2.3574288  0.56115633 2.5748742  2.36973   ]\n",
      "Reset environment\n",
      "Episode reward: 2766.8863277435303\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3849533 2.362015  2.3645558 0.5689676 2.5806808 2.376528 ]\n",
      "Reset environment\n",
      "Episode reward: 1864.1731377243996\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.3900495 2.3671203 2.369662  0.5746495 2.5851784 2.381626 ]\n",
      "Reset environment\n",
      "Episode reward: 4807.748620480299\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4017446 2.3786294 2.3815253 0.5876339 2.595555  2.3933039]\n",
      "Reset environment\n",
      "Episode reward: 3099.973164796829\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4078145 2.3846817 2.387621  0.5943795 2.600907  2.3993626]\n",
      "Reset environment\n",
      "Episode reward: 1431.0852415561676\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.410378  2.3872066 2.3902218 0.5972705 2.6031399 2.401923 ]\n",
      "Reset environment\n",
      "Episode reward: 4573.912039756775\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.419856   2.3966532  2.3997324  0.60769963 2.6115477  2.411406  ]\n",
      "Reset environment\n",
      "Episode reward: -400.7503084540367\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4187272 2.3957036 2.3984668 0.6069237 2.6107032 2.4102898]\n",
      "Reset environment\n",
      "Episode reward: 1930.5167261362076\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4239614 2.4009435 2.4037006 0.6128461 2.6152859 2.4155285]\n",
      "Reset environment\n",
      "Episode reward: 1294.636739373207\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.427772  2.4048002 2.4074671 0.617138  2.6186454 2.4193423]\n",
      "Reset environment\n",
      "Episode reward: 2253.887041091919\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.431978  2.4089983 2.4116642 0.6219221 2.6223097 2.423541 ]\n",
      "Reset environment\n",
      "Episode reward: -1444.5477432236075\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.430321   2.4069629  2.4104192  0.62075824 2.620666   2.4218833 ]\n",
      "Reset environment\n",
      "Episode reward: -436.6229986101389\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4291573  2.4060535  2.4090424  0.61999846 2.6197243  2.4207447 ]\n",
      "Reset environment\n",
      "Episode reward: 3717.4695906341076\n",
      "Total Steps: 756\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.433471   2.4100432  2.41374    0.62484807 2.6224022  2.4250894 ]\n",
      "Reset environment\n",
      "Episode reward: 382.52607148885727\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.435837   2.4127605  2.415796   0.62798184 2.6242824  2.427468  ]\n",
      "Reset environment\n",
      "Episode reward: 534.8227633610368\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4381695 2.414715  2.4185195 0.6310606 2.6260195 2.4298005]\n",
      "Reset environment\n",
      "Episode reward: 1356.678438782692\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4421303 2.4186683 2.4224935 0.6354629 2.6295328 2.4337604]\n",
      "Reset environment\n",
      "Episode reward: 2961.0240030288696\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4478886  2.4245243  2.428133   0.64186144 2.6346412  2.4395056 ]\n",
      "Reset environment\n",
      "Episode reward: 4981.4565824866295\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4580045 2.434674  2.43821   0.6529866 2.6436074 2.449626 ]\n",
      "Reset environment\n",
      "Episode reward: 116.45867502689362\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4586961 2.4355485 2.4387593 0.6542993 2.6442602 2.4503314]\n",
      "Reset environment\n",
      "Episode reward: 1017.7713939249516\n",
      "Total Steps: 1897\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4450088  2.4213226  2.4254317  0.63673735 2.629109   2.4365137 ]\n",
      "Reset environment\n",
      "Episode reward: 4537.300339143723\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4451075 2.421549  2.425317  0.6361588 2.628642  2.436567 ]\n",
      "Reset environment\n",
      "Episode reward: 4648.160695493221\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4562826 2.4328249 2.4363382 0.648537  2.6383965 2.4477375]\n",
      "Reset environment\n",
      "Episode reward: 2227.6415165662766\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4616313  2.4384906  2.4413328  0.65476924 2.6428106  2.4530888 ]\n",
      "Reset environment\n",
      "Episode reward: -1348.5606045294553\n",
      "Total Steps: 1575\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4477193 2.423577  2.428628  0.6368602 2.6301055 2.4392438]\n",
      "Reset environment\n",
      "Episode reward: 2213.4790169000626\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.451798  2.427668  2.4327052 0.6413849 2.633717  2.443324 ]\n",
      "Reset environment\n",
      "Episode reward: 4908.146467983723\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4632833  2.4390953  2.4442396  0.65402156 2.6439908  2.4548032 ]\n",
      "Reset environment\n",
      "Episode reward: 3961.5999849140644\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.472665  2.4486194 2.4534461 0.6643922 2.6522403 2.4641776]\n",
      "Reset environment\n",
      "Episode reward: 297.62871043384075\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4735796 2.4497883 2.4540815 0.6658713 2.6529078 2.4650855]\n",
      "Reset environment\n",
      "Episode reward: 2181.2472127377987\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4785645 2.4550745 2.4587018 0.6717713 2.6569648 2.470066 ]\n",
      "Reset environment\n",
      "Episode reward: 3635.160796701908\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4858642 2.4622455 2.4661121 0.6799162 2.6633925 2.477363 ]\n",
      "Reset environment\n",
      "Episode reward: 3163.8190010488033\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.4935925  2.4702308  2.4735496  0.68860036 2.6701205  2.485089  ]\n",
      "Reset environment\n",
      "Episode reward: 6139.387478113174\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5060668 2.4827776 2.485965  0.7022485 2.6811001 2.4975655]\n",
      "Reset environment\n",
      "Episode reward: 1867.3639925718307\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5105426  2.4875138  2.49017    0.70734817 2.6851115  2.5020354 ]\n",
      "Reset environment\n",
      "Episode reward: 2183.617530196905\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5157619 2.4930148 2.4950552 0.7134706 2.689344  2.507248 ]\n",
      "Reset environment\n",
      "Episode reward: -727.3269871473312\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.514735   2.4920824  2.4939487  0.71268326 2.6884348  2.506219  ]\n",
      "Reset environment\n",
      "Episode reward: 577.646183308214\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.516221  2.493248  2.4957554 0.7148434 2.6896617 2.50771  ]\n",
      "Reset environment\n",
      "Episode reward: 4169.939795285463\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5260942  2.5029116  2.5058253  0.72590023 2.698381   2.5175703 ]\n",
      "Reset environment\n",
      "Episode reward: 1320.2743691802025\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5298724  2.5066886  2.5096235  0.73014855 2.701716   2.5213528 ]\n",
      "Reset environment\n",
      "Episode reward: 2169.250917226076\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5348496 2.5120077 2.5141962 0.7361689 2.7056272 2.5263371]\n",
      "Reset environment\n",
      "Episode reward: 4712.653391122818\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.545931  2.5228913 2.5254526 0.7485104 2.7154815 2.5374126]\n",
      "Reset environment\n",
      "Episode reward: 825.370176076889\n",
      "Total Steps: 27\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.54718   2.5241363 2.526701  0.7499682 2.7165594 2.5386589]\n",
      "Reset environment\n",
      "Episode reward: 5444.394098810852\n",
      "Total Steps: 1646\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5490718  2.526122   2.5284638  0.74910367 2.7177367  2.5405467 ]\n",
      "Reset environment\n",
      "Episode reward: 3519.1357246637344\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.556021   2.5330827  2.5354223  0.75677323 2.7239664  2.5474956 ]\n",
      "Reset environment\n",
      "Episode reward: 1313.7848165631294\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5597827  2.5368726  2.539153   0.76099443 2.7272816  2.5512564 ]\n",
      "Reset environment\n",
      "Episode reward: 2281.2417045012116\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5651443  2.542575   2.5441332  0.76735884 2.7316015  2.5566342 ]\n",
      "Reset environment\n",
      "Episode reward: 2161.7752454578876\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5578866  2.5349786  2.53728    0.75108063 2.7280545  2.5493858 ]\n",
      "Reset environment\n",
      "Episode reward: -143.78894168138504\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5576916 2.534635  2.5372555 0.7508741 2.7281342 2.549184 ]\n",
      "Reset environment\n",
      "Episode reward: 2034.0105657577515\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5629587 2.539923  2.5424967 0.7567567 2.732794  2.5544498]\n",
      "Reset environment\n",
      "Episode reward: 5308.288856387138\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.573413  2.5504882 2.552844  0.7682552 2.7420177 2.5648975]\n",
      "Reset environment\n",
      "Episode reward: 4500.067399412394\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5837967  2.5610359  2.5630574  0.77965844 2.7512684  2.5752616 ]\n",
      "Reset environment\n",
      "Episode reward: 4639.929678440094\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5946488  2.572032   2.5737832  0.79162973 2.7608283  2.586121  ]\n",
      "Reset environment\n",
      "Episode reward: -261.51474906504154\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.5946834 2.5721774 2.5737133 0.79204   2.7608423 2.5861568]\n",
      "Reset environment\n",
      "Episode reward: 583.5504196286201\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.596244  2.573342  2.5756607 0.7945156 2.761905  2.5877123]\n",
      "Reset environment\n",
      "Episode reward: 2265.0133645534515\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6020038 2.579095  2.5814278 0.8008476 2.7670522 2.593462 ]\n",
      "Reset environment\n",
      "Episode reward: -26.6980117559433\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6020994 2.5793958 2.5813246 0.8012679 2.7672296 2.5935585]\n",
      "Reset environment\n",
      "Episode reward: 2043.621523320675\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6073513  2.5847368  2.5864868  0.80716556 2.771903   2.5988038 ]\n",
      "Reset environment\n",
      "Episode reward: 1831.4946098923683\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6122172  2.5896268  2.5913317  0.81260246 2.7762039  2.6036687 ]\n",
      "Reset environment\n",
      "Episode reward: 1217.21695125103\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6156816 2.5930583 2.5948257 0.8165558 2.779231  2.6071312]\n",
      "Reset environment\n",
      "Episode reward: 1213.3617972135544\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.619158   2.5965445  2.5983007  0.82051516 2.782266   2.6106045 ]\n",
      "Reset environment\n",
      "Episode reward: 3136.810244590044\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.625086  2.6023588 2.6043396 0.8270927 2.7874167 2.616529 ]\n",
      "Reset environment\n",
      "Episode reward: 159.6230926513672\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.625566  2.6030908 2.6045754 0.8280204 2.787765  2.6169975]\n",
      "Reset environment\n",
      "Episode reward: 3571.546579360962\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6325557 2.6098828 2.6117294 0.8358218 2.7939143 2.6239767]\n",
      "Reset environment\n",
      "Episode reward: 4455.428429484367\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6413348  2.618712   2.6204622  0.84552354 2.8017304  2.6327546 ]\n",
      "Reset environment\n",
      "Episode reward: 1703.3919729441404\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6452014  2.6221814  2.6246982  0.85043305 2.804807   2.6366086 ]\n",
      "Reset environment\n",
      "Episode reward: 3199.9117500185966\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6527042  2.6299207  2.6319714  0.85887307 2.8114402  2.644103  ]\n",
      "Reset environment\n",
      "Episode reward: 3951.7772708535194\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6605084 2.6375074 2.639964  0.8674667 2.8184056 2.651885 ]\n",
      "Reset environment\n",
      "Episode reward: 4387.414186895825\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6611812 2.6383834 2.6407468 0.8648373 2.8197072 2.652661 ]\n",
      "Reset environment\n",
      "Episode reward: 6819.423980653286\n",
      "Total Steps: 241\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6753051  2.6524074  2.6550095  0.88029903 2.8323686  2.6667995 ]\n",
      "Reset environment\n",
      "Episode reward: 2536.366927647963\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.67101   2.647864  2.650872  0.8760381 2.8277917 2.662481 ]\n",
      "Reset environment\n",
      "Episode reward: 3448.2437224388123\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6774418 2.6543183 2.6572807 0.8831966 2.8334653 2.6689062]\n",
      "Reset environment\n",
      "Episode reward: 2058.986377298832\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.681198  2.6580563 2.6610572 0.8874107 2.8367498 2.6726615]\n",
      "Reset environment\n",
      "Episode reward: 3709.0965381860733\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6893797  2.666456   2.66904    0.89646953 2.8442607  2.680844  ]\n",
      "Reset environment\n",
      "Episode reward: 4649.991149961948\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.6997607  2.677012   2.6792355  0.90816766 2.8534844  2.6912255 ]\n",
      "Reset environment\n",
      "Episode reward: 3117.858607470989\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7070038 2.6844962 2.6862102 0.9163595 2.8598232 2.6984649]\n",
      "Reset environment\n",
      "Episode reward: 5528.57690268755\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7178547  2.6953905  2.6970603  0.92826587 2.8694808  2.7093153 ]\n",
      "Reset environment\n",
      "Episode reward: 4661.031524062157\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7269182 2.7046    2.7059798 0.9382795 2.8775983 2.718382 ]\n",
      "Reset environment\n",
      "Episode reward: 6295.81792396307\n",
      "Total Steps: 221\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7391715  2.7167983  2.7182992  0.95176077 2.8884692  2.7306294 ]\n",
      "Reset environment\n",
      "Episode reward: 2824.5880272984505\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.744284   2.72195    2.7233882  0.95752007 2.89297    2.7357352 ]\n",
      "Reset environment\n",
      "Episode reward: 2197.958393037319\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7497184 2.727462  2.7287478 0.9635985 2.8977857 2.741164 ]\n",
      "Reset environment\n",
      "Episode reward: 3401.5150344483554\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7575345 2.7354977 2.7363029 0.9723462 2.9044933 2.748971 ]\n",
      "Reset environment\n",
      "Episode reward: 4263.805281639099\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7670958  2.7452254  2.745686   0.98292494 2.912961   2.758524  ]\n",
      "Reset environment\n",
      "Episode reward: 5254.911290943623\n",
      "Total Steps: 213\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7788765  2.7569768  2.7575078  0.99586856 2.923381   2.7703066 ]\n",
      "Reset environment\n",
      "Episode reward: 3044.196318089962\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7844388 2.7625635 2.7630546 1.0020568 2.928298  2.7758684]\n",
      "Reset environment\n",
      "Episode reward: 1810.825864136219\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.7890944 2.767206  2.767742  1.0072849 2.9324079 2.7805336]\n",
      "Reset environment\n",
      "Episode reward: 6235.485803067684\n",
      "Total Steps: 218\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8012674 2.7793736 2.7799194 1.0205566 2.9432764 2.7926958]\n",
      "Reset environment\n",
      "Episode reward: 3875.7787803411484\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8086355 2.7866442 2.7873588 1.0287298 2.9498422 2.8000479]\n",
      "Reset environment\n",
      "Episode reward: 1125.5902619361877\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8115263 2.7897038 2.790087  1.0321041 2.9524624 2.802935 ]\n",
      "Reset environment\n",
      "Episode reward: 715.6914343833923\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8133762 2.7912965 2.7922027 1.0344356 2.9540215 2.8047907]\n",
      "Reset environment\n",
      "Episode reward: 4678.772690415382\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.822455  2.8003356 2.8013244 1.0444458 2.9620888 2.813866 ]\n",
      "Reset environment\n",
      "Episode reward: 2570.2708586091176\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8282762 2.8064642 2.8068295 1.051006  2.967241  2.819669 ]\n",
      "Reset environment\n",
      "Episode reward: 3282.674767136574\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.834423  2.8124924 2.813081  1.0578668 2.9726658 2.8258078]\n",
      "Reset environment\n",
      "Episode reward: 928.7038882374763\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8370318 2.8153803 2.815413  1.0610807 2.9748256 2.828406 ]\n",
      "Reset environment\n",
      "Episode reward: 4888.679587841034\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.848015  2.826336  2.8264236 1.0732607 2.9846    2.8393815]\n",
      "Reset environment\n",
      "Episode reward: 1387.2524281144142\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8512988 2.8294468 2.829883  1.0769575 2.9875202 2.842659 ]\n",
      "Reset environment\n",
      "Episode reward: 2597.850007414818\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.856016  2.834102  2.8346546 1.0822502 2.991654  2.84737  ]\n",
      "Reset environment\n",
      "Episode reward: 3787.865455299616\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.86326   2.841198  2.8420265 1.0902755 2.998094  2.8546028]\n",
      "Reset environment\n",
      "Episode reward: 786.7873628139496\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.865383  2.8431585 2.8443086 1.0927607 2.9999804 2.8567216]\n",
      "Reset environment\n",
      "Episode reward: 841.3980520963669\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8677778 2.8456798 2.84659   1.0955422 3.0021544 2.8591168]\n",
      "Reset environment\n",
      "Episode reward: -504.527551651001\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.866913  2.8446436 2.8459213 1.0946493 3.00152   2.8582573]\n",
      "Reset environment\n",
      "Episode reward: 1354.8574666976929\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8705919 2.848322  2.8496115 1.0987341 3.0047712 2.8619337]\n",
      "Reset environment\n",
      "Episode reward: 1358.8962796479464\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8734963 2.851517  2.8521903 1.1025038 3.0069675 2.8648398]\n",
      "Reset environment\n",
      "Episode reward: 1875.1149773597717\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8781908 2.8561792 2.856926  1.1077523 3.0111434 2.8695314]\n",
      "Reset environment\n",
      "Episode reward: 1820.272682160139\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.882482  2.8601472 2.861516  1.1128262 3.0146403 2.8738077]\n",
      "Reset environment\n",
      "Episode reward: 2994.1566494107246\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8893116 2.867222  2.868047  1.1205838 3.020261  2.8806329]\n",
      "Reset environment\n",
      "Episode reward: 1269.6870249509811\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8927743 2.8707151 2.8714962 1.1245128 3.023303  2.884104 ]\n",
      "Reset environment\n",
      "Episode reward: 3664.945438146591\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8994708 2.8772845 2.8782945 1.1318787 3.0292275 2.8908033]\n",
      "Reset environment\n",
      "Episode reward: 4572.673874258995\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9082167 2.8859959 2.8870933 1.1414435 3.037097  2.899556 ]\n",
      "Reset environment\n",
      "Episode reward: 2719.3766229748726\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.914479  2.8925164 2.89308   1.1485764 3.0424097 2.9058223]\n",
      "Reset environment\n",
      "Episode reward: -5061.356606613845\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.8917449 2.8705642 2.8699148 1.1229434 3.0198965 2.883265 ]\n",
      "Reset environment\n",
      "Episode reward: 3585.516641020775\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.898369  2.8771992 2.876532  1.1303273 3.0257204 2.8898828]\n",
      "Reset environment\n",
      "Episode reward: 3380.1666119396687\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9047062 2.8833702 2.8830116 1.1373547 3.0313148 2.896215 ]\n",
      "Reset environment\n",
      "Episode reward: 4669.289437830448\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.913591  2.8923187 2.8918262 1.147204  3.0392141 2.9050999]\n",
      "Reset environment\n",
      "Episode reward: 2226.456354126334\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.918654  2.8970923 2.897156  1.1530157 3.0435264 2.9101627]\n",
      "Reset environment\n",
      "Episode reward: 1397.4127691984177\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9221144 2.9007304 2.9004416 1.1570104 3.0466027 2.9136152]\n",
      "Reset environment\n",
      "Episode reward: 3889.403202176094\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9307778 2.909189  2.9092991 1.1665756 3.0544448 2.9222698]\n",
      "Reset environment\n",
      "Episode reward: 2493.3865243792534\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9351614 2.913606  2.9136481 1.1714836 3.058338  2.9266484]\n",
      "Reset environment\n",
      "Episode reward: 5381.129242062569\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.945429  2.9239097 2.923893  1.1827064 3.06746   2.9369156]\n",
      "Reset environment\n",
      "Episode reward: 3568.9605474472046\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9518542 2.930334  2.930325  1.1898341 3.0731366 2.9433403]\n",
      "Reset environment\n",
      "Episode reward: 610.6356214284897\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9536128 2.9322488 2.931948  1.191938  3.0747223 2.9450953]\n",
      "Reset environment\n",
      "Episode reward: 1313.408765077591\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9557583 2.934401  2.934087  1.1943944 3.0765808 2.9472387]\n",
      "Reset environment\n",
      "Episode reward: 3413.5045412778854\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.961847  2.9404845 2.9401808 1.2011327 3.0819926 2.9533243]\n",
      "Reset environment\n",
      "Episode reward: 3988.2152875065804\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9691012 2.947709  2.9474514 1.2091831 3.088446  2.9605777]\n",
      "Reset environment\n",
      "Episode reward: 1244.8138047456741\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.972497  2.9510906 2.9508615 1.2130396 3.0914047 2.9639785]\n",
      "Reset environment\n",
      "Episode reward: 1267.9013661146164\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9759166 2.954529  2.9542572 1.2169092 3.0943968 2.9673948]\n",
      "Reset environment\n",
      "Episode reward: 2457.7038500905037\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9817615 2.9604466 2.9600248 1.2233776 3.0996125 2.9732356]\n",
      "Reset environment\n",
      "Episode reward: 4909.151432394981\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.992639  2.9712625 2.9709504 1.2352148 3.1092732 2.984103 ]\n",
      "Reset environment\n",
      "Episode reward: 651.1322681903839\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9944    2.9728944 2.9728477 1.237108  3.1109674 2.9858575]\n",
      "Reset environment\n",
      "Episode reward: 1753.4853819012642\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [2.9987047 2.976854  2.9774656 1.2421755 3.114467  2.9901516]\n",
      "Reset environment\n",
      "Episode reward: 5319.119206130505\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0087402 2.9868855 2.9874897 1.2532455 3.1233706 3.0001833]\n",
      "Reset environment\n",
      "Episode reward: 3204.5646536946297\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0144434 2.9926293 2.993155  1.2595586 3.1284468 3.0058835]\n",
      "Reset environment\n",
      "Episode reward: 3524.0259052813053\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0208368 2.998883  2.9996712 1.2666273 3.1339717 3.0122678]\n",
      "Reset environment\n",
      "Episode reward: 2974.651188187301\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0274487 3.0057318 3.0060031 1.274065  3.1396363 3.0188696]\n",
      "Reset environment\n",
      "Episode reward: 4883.409142315388\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.038176  3.0163107 3.016839  1.2858124 3.1492023 3.0295897]\n",
      "Reset environment\n",
      "Episode reward: 6409.07624322176\n",
      "Total Steps: 232\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.050249  3.028426  3.0288599 1.2991295 3.159756  3.0416553]\n",
      "Reset environment\n",
      "Episode reward: 4807.136325001717\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0592618 3.0372992 3.0379767 1.3092489 3.1676712 3.0506613]\n",
      "Reset environment\n",
      "Episode reward: 4614.699055701494\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0680122 3.0461986 3.0465565 1.3189021 3.175332  3.0593934]\n",
      "Reset environment\n",
      "Episode reward: 6203.276778399944\n",
      "Total Steps: 217\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0796964 3.0579412 3.0582073 1.3317214 3.1857524 3.0710764]\n",
      "Reset environment\n",
      "Episode reward: 4061.634333014488\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0885997 3.0670261 3.0669084 1.3415176 3.193682  3.0799594]\n",
      "Reset environment\n",
      "Episode reward: 7380.715865433216\n",
      "Total Steps: 543\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.0993268 3.0777268 3.077725  1.3517442 3.2042322 3.0907326]\n",
      "Reset environment\n",
      "Episode reward: 3869.2148519456387\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.107952  3.0865428 3.0861464 1.3613259 3.2118828 3.099353 ]\n",
      "Reset environment\n",
      "Episode reward: 1482.145808428526\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1113498 3.0902193 3.0892084 1.3655226 3.2145348 3.1027436]\n",
      "Reset environment\n",
      "Episode reward: 714.9162645936012\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1133852 3.092028  3.0914726 1.3679193 3.2163253 3.1047788]\n",
      "Reset environment\n",
      "Episode reward: 1027.3806390166283\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.114927  3.0935743 3.0930097 1.3697402 3.217648  3.1063201]\n",
      "Reset environment\n",
      "Episode reward: 837.0000665187836\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1171641 3.09605   3.0950203 1.3723762 3.2196748 3.1085598]\n",
      "Reset environment\n",
      "Episode reward: 3076.8353532254696\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1239595 3.1030607 3.1015468 1.3800383 3.2253568 3.1153471]\n",
      "Reset environment\n",
      "Episode reward: 4660.795310139656\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1326694 3.1116848 3.1103482 1.3896557 3.2331727 3.1240602]\n",
      "Reset environment\n",
      "Episode reward: 2787.228411793709\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1387477 3.1180415 3.1161232 1.396565  3.2384834 3.1301258]\n",
      "Reset environment\n",
      "Episode reward: 554.3824989795685\n",
      "Total Steps: 23\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1393263 3.1186252 3.1166987 1.3973624 3.2389169 3.130703 ]\n",
      "Reset environment\n",
      "Episode reward: 4949.541324526072\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1485364 3.1279707 3.125755  1.4075434 3.2471247 3.1399035]\n",
      "Reset environment\n",
      "Episode reward: 3503.331965394318\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.154848  3.134041  3.1322782 1.4146874 3.2525911 3.1462073]\n",
      "Reset environment\n",
      "Episode reward: 3883.4409603774548\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1619337 3.14133   3.1391358 1.4225917 3.258765  3.1532829]\n",
      "Reset environment\n",
      "Episode reward: 1180.703260421753\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1643834 3.1434314 3.1419134 1.4258993 3.260625  3.15572  ]\n",
      "Reset environment\n",
      "Episode reward: 2750.659474015236\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.170432  3.149739  3.1476812 1.4327879 3.2658792 3.161762 ]\n",
      "Reset environment\n",
      "Episode reward: 2328.326549053192\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1745586 3.1538079 3.1518576 1.4373765 3.2695367 3.1658874]\n",
      "Reset environment\n",
      "Episode reward: 4507.753991127014\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1843042 3.1635013 3.1616633 1.4480364 3.2782605 3.1756272]\n",
      "Reset environment\n",
      "Episode reward: 4252.602321565151\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1919565 3.1712785 3.169184  1.456509  3.2849324 3.1832669]\n",
      "Reset environment\n",
      "Episode reward: 2303.2032005693763\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.1968873 3.1764662 3.1737864 1.4623455 3.2888439 3.1881876]\n",
      "Reset environment\n",
      "Episode reward: 4965.3743641376495\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2076368 3.1871386 3.1845942 1.4740494 3.2983723 3.1989293]\n",
      "Reset environment\n",
      "Episode reward: 4346.814198851585\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2171392 3.196771  3.1939874 1.4845414 3.3067682 3.2084298]\n",
      "Reset environment\n",
      "Episode reward: 4944.069854199886\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.227764  3.207353  3.2046583 1.4960417 3.3162346 3.2190397]\n",
      "Reset environment\n",
      "Episode reward: 1854.2463816804811\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.231717  3.2115734 3.208281  1.5008733 3.319322  3.2229953]\n",
      "Reset environment\n",
      "Episode reward: 5427.954752385616\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2419133 3.2217257 3.2185192 1.511972  3.3284817 3.2331924]\n",
      "Reset environment\n",
      "Episode reward: -696.3306175470352\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.240749  3.2205756 3.2173636 1.5109049 3.327363  3.232027 ]\n",
      "Reset environment\n",
      "Episode reward: 4462.674261212349\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2487268 3.228475  3.2254252 1.519816  3.3344345 3.2399988]\n",
      "Reset environment\n",
      "Episode reward: 1652.8194136023521\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2515035 3.231241  3.2282062 1.5229175 3.3368855 3.242771 ]\n",
      "Reset environment\n",
      "Episode reward: 1495.6710520088673\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.254733  3.234728  3.2311466 1.5268954 3.339388  3.2460065]\n",
      "Reset environment\n",
      "Episode reward: 4906.9580100774765\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2652485 3.245153  3.2416945 1.538419  3.3487363 3.256507 ]\n",
      "Reset environment\n",
      "Episode reward: 3724.392400212586\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2719681 3.2516432 3.248593  1.5458937 3.3545504 3.2632082]\n",
      "Reset environment\n",
      "Episode reward: 3473.205505669117\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2780952 3.2577643 3.254719  1.5526273 3.3599794 3.2693257]\n",
      "Reset environment\n",
      "Episode reward: 2193.969861485064\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2826746 3.2626157 3.2589657 1.5581056 3.363551  3.2739127]\n",
      "Reset environment\n",
      "Episode reward: 4611.236098349094\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.2909694 3.27079   3.2673583 1.5673156 3.370878  3.2821937]\n",
      "Reset environment\n",
      "Episode reward: 5417.917650163174\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3011398 3.2810118 3.2774723 1.5784125 3.3799975 3.2923572]\n",
      "Reset environment\n",
      "Episode reward: 3135.0411815047264\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.306574  3.2864728 3.2828777 1.5844502 3.3848238 3.2977827]\n",
      "Reset environment\n",
      "Episode reward: 4909.002213299274\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.315715  3.295724  3.291901  1.594479  3.3929534 3.306919 ]\n",
      "Reset environment\n",
      "Episode reward: -114.91229200363159\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.315919  3.2960665 3.291993  1.5947694 3.3932834 3.307128 ]\n",
      "Reset environment\n",
      "Episode reward: 2615.1153202950954\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3216226 3.3020184 3.2974017 1.6012633 3.398041  3.312841 ]\n",
      "Reset environment\n",
      "Episode reward: 1818.2806565761566\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3246753 3.3049998 3.3005154 1.604668  3.4007323 3.315889 ]\n",
      "Reset environment\n",
      "Episode reward: 5358.753931045532\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3346586 3.3148887 3.3105834 1.6155636 3.4097385 3.3258672]\n",
      "Reset environment\n",
      "Episode reward: 3361.025851815939\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3406558 3.3207808 3.3166497 1.6221994 3.4150128 3.331849 ]\n",
      "Reset environment\n",
      "Episode reward: 6214.695393502712\n",
      "Total Steps: 221\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3523695 3.3325202 3.328336  1.6350493 3.4254742 3.343558 ]\n",
      "Reset environment\n",
      "Episode reward: 5050.4100642204285\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3613822 3.3414483 3.3374102 1.6449866 3.4335074 3.352561 ]\n",
      "Reset environment\n",
      "Episode reward: 795.59945499897\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3624117 3.3424788 3.3384402 1.6462603 3.434366  3.3535912]\n",
      "Reset environment\n",
      "Episode reward: 1252.3836036920547\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3656542 3.3457372 3.3416743 1.6499463 3.4372063 3.3568356]\n",
      "Reset environment\n",
      "Episode reward: 5695.818653106689\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3760612 3.3561442 3.3521066 1.6613302 3.446487  3.3672369]\n",
      "Reset environment\n",
      "Episode reward: 4201.042926847935\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.38365   3.363691  3.3597386 1.6696743 3.4532552 3.3748198]\n",
      "Reset environment\n",
      "Episode reward: 1351.511040687561\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3857777 3.3658295 3.3618577 1.6720968 3.4551146 3.376948 ]\n",
      "Reset environment\n",
      "Episode reward: 4992.946300327778\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.3962162 3.3763447 3.3722239 1.6835663 3.4644165 3.3873785]\n",
      "Reset environment\n",
      "Episode reward: 4152.543933451176\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4037328 3.3837767 3.3798044 1.6918485 3.4710848 3.394874 ]\n",
      "Reset environment\n",
      "Episode reward: 1257.4355509281158\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.406986  3.3870232 3.3830683 1.6955358 3.4739387 3.3981268]\n",
      "Reset environment\n",
      "Episode reward: 3692.452340364456\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4133499 3.3934872 3.3893528 1.7026616 3.4795666 3.4044833]\n",
      "Reset environment\n",
      "Episode reward: 1214.067739725113\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.416498  3.3966713 3.392468  1.7062286 3.4823492 3.4076276]\n",
      "Reset environment\n",
      "Episode reward: 6163.423909544945\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4276674 3.407869  3.403617  1.7185069 3.4922657 3.4187853]\n",
      "Reset environment\n",
      "Episode reward: -205.74961459636688\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4274042 3.4076993 3.4032807 1.7182143 3.492255  3.4185224]\n",
      "Reset environment\n",
      "Episode reward: 2463.705385191366\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.419075  3.3997316 3.3947618 1.7080173 3.4847994 3.410206 ]\n",
      "Reset environment\n",
      "Episode reward: 1167.4630173444748\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4220998 3.4027991 3.3977616 1.7115276 3.4874332 3.4132347]\n",
      "Reset environment\n",
      "Episode reward: 2172.3394051790237\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4270751 3.4078221 3.4026926 1.7170646 3.491868  3.4182014]\n",
      "Reset environment\n",
      "Episode reward: 4996.470428645611\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4361596 3.4169958 3.4116886 1.7270399 3.499943  3.4272828]\n",
      "Reset environment\n",
      "Episode reward: -692.3607335090637\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4349494 3.4158034 3.4104815 1.7259111 3.4988055 3.4260707]\n",
      "Reset environment\n",
      "Episode reward: 2097.0170938670635\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4390216 3.4201307 3.4142315 1.730863  3.5019171 3.4301314]\n",
      "Reset environment\n",
      "Episode reward: 2252.0999000668526\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4428818 3.4239268 3.4181488 1.735163  3.5053327 3.4339852]\n",
      "Reset environment\n",
      "Episode reward: 6887.076140999794\n",
      "Total Steps: 1498\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4448829 3.425905  3.4201236 1.7279136 3.508663  3.4359298]\n",
      "Reset environment\n",
      "Episode reward: 2845.781370408833\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4383202 3.4190996 3.4137886 1.7179692 3.5023956 3.4293258]\n",
      "Reset environment\n",
      "Episode reward: 142.72435631603003\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4386039 3.4190986 3.4143653 1.7185991 3.5025976 3.429608 ]\n",
      "Reset environment\n",
      "Episode reward: 2524.0601073503494\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4426844 3.4231591 3.4184759 1.723159  3.506175  3.4336867]\n",
      "Reset environment\n",
      "Episode reward: -451.0134752616286\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4417248 3.4220014 3.4177306 1.7224243 3.50527   3.4327261]\n",
      "Reset environment\n",
      "Episode reward: 1863.1205503940582\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.444807  3.425053  3.4208412 1.7258527 3.5079923 3.4358082]\n",
      "Reset environment\n",
      "Episode reward: -5.483535051345825\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4449816 3.4253924 3.4208708 1.7262248 3.5082467 3.4359825]\n",
      "Reset environment\n",
      "Episode reward: 1985.0217552669346\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4489975 3.4296756 3.4245508 1.7310959 3.511412  3.4399934]\n",
      "Reset environment\n",
      "Episode reward: 3535.685080625117\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4564075 3.437237  3.4317553 1.7393266 3.5178883 3.4473839]\n",
      "Reset environment\n",
      "Episode reward: 2434.5465158736333\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4504998 3.4308007 3.4263418 1.7287635 3.512114  3.441443 ]\n",
      "Reset environment\n",
      "Episode reward: 4237.820882201195\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4578695 3.4381504 3.433738  1.7369075 3.5186322 3.4488091]\n",
      "Reset environment\n",
      "Episode reward: 4463.852099895477\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.467069  3.447237  3.4430294 1.7470887 3.52683   3.4580092]\n",
      "Reset environment\n",
      "Episode reward: 1277.0728299096227\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4699304 3.4503388 3.4455817 1.7507067 3.528977  3.4608576]\n",
      "Reset environment\n",
      "Episode reward: 1712.2689215540886\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4734573 3.4535325 3.4494257 1.7549686 3.5319288 3.4643822]\n",
      "Reset environment\n",
      "Episode reward: 3341.8819898813963\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4789708 3.4588542 3.4551125 1.7611941 3.536701  3.469896 ]\n",
      "Reset environment\n",
      "Episode reward: 5459.1650540828705\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.488568  3.4684846 3.4646652 1.7717727 3.545202  3.4794936]\n",
      "Reset environment\n",
      "Episode reward: 5500.299613595009\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.4982831 3.478184  3.4743922 1.782425  3.553852  3.4891968]\n",
      "Reset environment\n",
      "Episode reward: 2285.1313759945333\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5031066 3.4832373 3.478919  1.7879933 3.5577848 3.494004 ]\n",
      "Reset environment\n",
      "Episode reward: 3630.255922138691\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5094543 3.489397  3.4854155 1.7950493 3.5633645 3.5003405]\n",
      "Reset environment\n",
      "Episode reward: 2770.194046020508\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.514187  3.494076  3.4901946 1.8003272 3.5675397 3.5050602]\n",
      "Reset environment\n",
      "Episode reward: 590.1313762664795\n",
      "Total Steps: 19\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5148199 3.494709  3.490827  1.8011087 3.568061  3.5056942]\n",
      "Reset environment\n",
      "Episode reward: 1541.0667984485626\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.518537  3.4984307 3.4945483 1.8052764 3.5713387 3.5094113]\n",
      "Reset environment\n",
      "Episode reward: 1242.6208947896957\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5216641 3.501578  3.497661  1.8088337 3.5740752 3.5125353]\n",
      "Reset environment\n",
      "Episode reward: 994.3683162033558\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5238402 3.503463  3.5000982 1.8115796 3.5757487 3.5146952]\n",
      "Reset environment\n",
      "Episode reward: 214.76059544086456\n",
      "Total Steps: 7\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5238118 3.503439  3.5000665 1.8116106 3.575711  3.5146668]\n",
      "Reset environment\n",
      "Episode reward: 1429.0504107773304\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5267622 3.5060813 3.5033178 1.8151972 3.578184  3.5176148]\n",
      "Reset environment\n",
      "Episode reward: -690.9332022666931\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5255032 3.5048356 3.5020688 1.8139129 3.5770035 3.51636  ]\n",
      "Reset environment\n",
      "Episode reward: 1487.6634988486767\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5284412 3.507458  3.5053158 1.8175696 3.5794113 3.5192907]\n",
      "Reset environment\n",
      "Episode reward: 5378.375907540321\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5379891 3.517082  3.514777  1.8280759 3.5879142 3.5288386]\n",
      "Reset environment\n",
      "Episode reward: 1935.2187470197678\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.542429  3.5215394 3.5191963 1.8330392 3.591833  3.5332797]\n",
      "Reset environment\n",
      "Episode reward: 3806.5248787226155\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5491304 3.5280793 3.5260236 1.840431  3.5976872 3.5399742]\n",
      "Reset environment\n",
      "Episode reward: 4115.1941803097725\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5564241 3.5352178 3.5334306 1.8484333 3.604182  3.5472565]\n",
      "Reset environment\n",
      "Episode reward: 4802.42780905962\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5648117 3.5435867 3.5418324 1.8575712 3.6116464 3.5556476]\n",
      "Reset environment\n",
      "Episode reward: 4949.252825021744\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5735714 3.5523152 3.550639  1.8671794 3.6195145 3.564415 ]\n",
      "Reset environment\n",
      "Episode reward: 287.1918227672577\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5745008 3.553401  3.5514212 1.8682727 3.620408  3.5653367]\n",
      "Reset environment\n",
      "Episode reward: 1284.7598569989204\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.577703  3.55659   3.5546448 1.8718944 3.6232123 3.5685472]\n",
      "Reset environment\n",
      "Episode reward: 1585.2434012293816\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5810983 3.5597994 3.5582247 1.875688  3.6262338 3.5719438]\n",
      "Reset environment\n",
      "Episode reward: 499.9235405921936\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.5822997 3.5608153 3.5596368 1.8771925 3.6273124 3.5731525]\n",
      "Reset environment\n",
      "Episode reward: 5395.855318307877\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.591781  3.5703225 3.5691025 1.8875723 3.6357698 3.5826333]\n",
      "Reset environment\n",
      "Episode reward: 3674.284174978733\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.598143  3.576636  3.5755162 1.8946522 3.6414163 3.5889952]\n",
      "Reset environment\n",
      "Episode reward: 3272.313232704997\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6049473 3.5836008 3.582133  1.9022429 3.647387  3.5957935]\n",
      "Reset environment\n",
      "Episode reward: 1151.2540314346552\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6073403 3.5856934 3.5848193 1.9052598 3.6494062 3.5981805]\n",
      "Reset environment\n",
      "Episode reward: 2748.492086827755\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6117861 3.5901494 3.5892522 1.9101746 3.653367  3.6026251]\n",
      "Reset environment\n",
      "Episode reward: 3506.844362318516\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6179104 3.5963628 3.5952787 1.9169027 3.6588624 3.6087394]\n",
      "Reset environment\n",
      "Episode reward: 4440.166304558516\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6257372 3.6043167 3.6029725 1.9255196 3.6658258 3.6165633]\n",
      "Reset environment\n",
      "Episode reward: 2197.205927670002\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6291826 3.607759  3.6064272 1.9293422 3.6688728 3.620015 ]\n",
      "Reset environment\n",
      "Episode reward: 3269.4389449357986\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6346064 3.6132116 3.6118214 1.935337  3.6736887 3.625442 ]\n",
      "Reset environment\n",
      "Episode reward: -210.79724448919296\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6340632 3.6127872 3.6111727 1.9350787 3.6733685 3.6248991]\n",
      "Reset environment\n",
      "Episode reward: 1841.6710198521614\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6378062 3.6167557 3.6146812 1.939364  3.6767275 3.6286354]\n",
      "Reset environment\n",
      "Episode reward: 695.8984247595072\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6394513 3.6180484 3.6166768 1.94175   3.6778798 3.6302671]\n",
      "Reset environment\n",
      "Episode reward: 1338.0472804903984\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6427546 3.6213427 3.6199949 1.9454647 3.6807897 3.6335716]\n",
      "Reset environment\n",
      "Episode reward: 2400.102993309498\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.646738  3.6252723 3.6240282 1.9498885 3.684331  3.637549 ]\n",
      "Reset environment\n",
      "Episode reward: 2765.8869457393885\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6524925 3.6312582 3.629504  1.9563823 3.6891863 3.6432962]\n",
      "Reset environment\n",
      "Episode reward: 3594.485510110855\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.65847   3.637336  3.6353793 1.9630098 3.6944063 3.6492753]\n",
      "Reset environment\n",
      "Episode reward: 3101.6689831912518\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6635883 3.6422732 3.6406486 1.968718  3.69879   3.6543927]\n",
      "Reset environment\n",
      "Episode reward: 4441.566380679607\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6714153 3.6502175 3.6483366 1.9773029 3.705804  3.6622057]\n",
      "Reset environment\n",
      "Episode reward: 4941.390191137791\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6801825 3.6591043 3.6569583 1.9868784 3.7136562 3.670962 ]\n",
      "Reset environment\n",
      "Episode reward: 2091.4079819321632\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.6848445 3.6638157 3.6615627 1.9920813 3.7178006 3.6756206]\n",
      "Reset environment\n",
      "Episode reward: 715.7716176509857\n",
      "Total Steps: 24\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.685703  3.6646814 3.662412  1.9931216 3.7185283 3.6764784]\n",
      "Reset environment\n",
      "Episode reward: 4275.5858390033245\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.693125  3.6722164 3.6696944 2.0013533 3.7250483 3.6838894]\n",
      "Reset environment\n",
      "Episode reward: 5256.189758360386\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.702175  3.681274  3.6787367 2.011295  3.733073  3.6929443]\n",
      "Reset environment\n",
      "Episode reward: 4670.47858017683\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.710333  3.6893158 3.687007  2.0203607 3.740372  3.7011118]\n",
      "Reset environment\n",
      "Episode reward: 4581.02819776535\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7183359 3.6974142 3.6948988 2.0291896 3.7474728 3.7091105]\n",
      "Reset environment\n",
      "Episode reward: -700.9763685464859\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7169225 3.6959987 3.6934972 2.0275347 3.746286  3.7076888]\n",
      "Reset environment\n",
      "Episode reward: 6889.040816664696\n",
      "Total Steps: 249\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7288036 3.7078388 3.7054045 2.0406454 3.7567766 3.7195635]\n",
      "Reset environment\n",
      "Episode reward: 3088.8912184387445\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7338438 3.7126796 3.7105904 2.0463045 3.7610233 3.7245827]\n",
      "Reset environment\n",
      "Episode reward: 4896.012334883213\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7437086 3.722393  3.7206023 2.0571454 3.7698576 3.7344522]\n",
      "Reset environment\n",
      "Episode reward: 2991.4646865725517\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7487085 3.727371  3.7256248 2.062711  3.774292  3.7394512]\n",
      "Reset environment\n",
      "Episode reward: 2447.9804177880287\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7526257 3.7312264 3.7295847 2.0670702 3.7777221 3.7433589]\n",
      "Reset environment\n",
      "Episode reward: 2136.7025079727173\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7573624 3.7359736 3.7343159 2.07231   3.7819483 3.7480886]\n",
      "Reset environment\n",
      "Episode reward: 2098.691409766674\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.761515  3.7398367 3.7387512 2.0772042 3.78541   3.75224  ]\n",
      "Reset environment\n",
      "Episode reward: 2412.4136941432953\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7655249 3.7438767 3.742723  2.0816822 3.7889786 3.7562385]\n",
      "Reset environment\n",
      "Episode reward: 2977.111778855324\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.771481  3.750059  3.7484357 2.0883057 3.7942638 3.7621813]\n",
      "Reset environment\n",
      "Episode reward: 4819.333170235157\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.779677  3.7582858 3.7566097 2.097323  3.8015666 3.7703745]\n",
      "Reset environment\n",
      "Episode reward: 4138.889091901481\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.786881  3.7653    3.7639718 2.1052425 3.8079228 3.7775648]\n",
      "Reset environment\n",
      "Episode reward: 5976.93479269743\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.7972252 3.7755911 3.7743788 2.1165879 3.8171296 3.7878997]\n",
      "Reset environment\n",
      "Episode reward: 2224.6502991318703\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8006744 3.7790844 3.7777762 2.1204185 3.82024   3.7913394]\n",
      "Reset environment\n",
      "Episode reward: 2762.735203742981\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.805235  3.7836208 3.7823577 2.1254845 3.8242815 3.7958984]\n",
      "Reset environment\n",
      "Episode reward: 4021.4063679277897\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8134663 3.7917724 3.79066   2.1347816 3.8315716 3.8041325]\n",
      "Reset environment\n",
      "Episode reward: 4735.495008647442\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8214805 3.799818  3.798664  2.1435723 3.8387287 3.8121455]\n",
      "Reset environment\n",
      "Episode reward: 1795.7134905457497\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8256028 3.8039513 3.8027744 2.148113  3.8424058 3.816265 ]\n",
      "Reset environment\n",
      "Episode reward: 4555.981165468693\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8343897 3.8127947 3.8115015 2.1579442 3.8502116 3.8250592]\n",
      "Reset environment\n",
      "Episode reward: 2647.2417332679033\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8394537 3.8175495 3.8168554 2.1638734 3.854409  3.8301256]\n",
      "Reset environment\n",
      "Episode reward: 4296.642859280109\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8468764 3.8249097 3.8243418 2.1720016 3.8610785 3.8375452]\n",
      "Reset environment\n",
      "Episode reward: 3015.5105879306793\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8517222 3.82978   3.8291464 2.1774013 3.8653367 3.8423853]\n",
      "Reset environment\n",
      "Episode reward: 4283.727668642998\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8589013 3.8369744 3.8363142 2.1852689 3.8717153 3.849562 ]\n",
      "Reset environment\n",
      "Episode reward: 2362.0167549848557\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8627224 3.8407896 3.8401432 2.1895306 3.875094  3.8533788]\n",
      "Reset environment\n",
      "Episode reward: 1589.9027718305588\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8664    3.8444753 3.8438191 2.1936126 3.8783588 3.8570552]\n",
      "Reset environment\n",
      "Episode reward: -687.7188076972961\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.865126  3.8432157 3.842553  2.1924436 3.8771493 3.8557918]\n",
      "Reset environment\n",
      "Episode reward: 5553.453045725822\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8745651 3.8526535 3.851997  2.202806  3.885552  3.8652396]\n",
      "Reset environment\n",
      "Episode reward: -362.7215917110443\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8740184 3.8522859 3.851291  2.2024906 3.8850818 3.8646927]\n",
      "Reset environment\n",
      "Episode reward: 605.9693910479546\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8748133 3.8527427 3.8524277 2.2039547 3.8856556 3.8654835]\n",
      "Reset environment\n",
      "Episode reward: -712.314732670784\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8733342 3.8512552 3.8509667 2.202186  3.8844354 3.864003 ]\n",
      "Reset environment\n",
      "Episode reward: 439.5574414730072\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.874271  3.8519812 3.8521223 2.2033203 3.8852367 3.864936 ]\n",
      "Reset environment\n",
      "Episode reward: 1737.4448605775833\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8778641 3.8558433 3.85544   2.2074451 3.888342  3.8685188]\n",
      "Reset environment\n",
      "Episode reward: 1873.4020375609398\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.881973  3.85998   3.8595283 2.212103  3.8919694 3.872627 ]\n",
      "Reset environment\n",
      "Episode reward: 4680.214836359024\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8898528 3.867861  3.867408  2.220744  3.898966  3.8805032]\n",
      "Reset environment\n",
      "Episode reward: 1517.0759913921356\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8921003 3.8700578 3.869701  2.2232652 3.9009326 3.8827453]\n",
      "Reset environment\n",
      "Episode reward: 3924.663334608078\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.8987558 3.8767838 3.8762963 2.2305994 3.9068646 3.8894007]\n",
      "Reset environment\n",
      "Episode reward: 1237.5737823843956\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9015584 3.8797019 3.8789732 2.23374   3.9094107 3.8921974]\n",
      "Reset environment\n",
      "Episode reward: 5142.884404301643\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9101372 3.8884108 3.8874328 2.2432635 3.9170024 3.9007766]\n",
      "Reset environment\n",
      "Episode reward: 1318.200365960598\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.913312  3.8915713 3.8906343 2.2468214 3.9198115 3.903953 ]\n",
      "Reset environment\n",
      "Episode reward: 1938.962584555149\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9175823 3.8958313 3.8949137 2.2516003 3.9235725 3.9082208]\n",
      "Reset environment\n",
      "Episode reward: 5139.326072633266\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9264548 3.9047256 3.9037595 2.2613437 3.9314985 3.9170952]\n",
      "Reset environment\n",
      "Episode reward: 4449.0211091041565\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9340036 3.912316  3.9112508 2.2696595 3.9382496 3.9246414]\n",
      "Reset environment\n",
      "Episode reward: 4037.96256929636\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.940648  3.918964  3.9179032 2.2770054 3.9441206 3.9312906]\n",
      "Reset environment\n",
      "Episode reward: 4486.877735137939\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9495866 3.9279797 3.9267457 2.286777  3.9521003 3.9402246]\n",
      "Reset environment\n",
      "Episode reward: 660.7030098438263\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.951011  3.929564  3.9280097 2.2884672 3.9533625 3.9416368]\n",
      "Reset environment\n",
      "Episode reward: 1976.3072392940521\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.955379  3.933958  3.9323525 2.293296  3.957248  3.9460027]\n",
      "Reset environment\n",
      "Episode reward: 2799.2067451179028\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.959866  3.9383624 3.936904  2.2983084 3.9611597 3.950488 ]\n",
      "Reset environment\n",
      "Episode reward: 436.3120057582855\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9606216 3.9393065 3.9374738 2.2992365 3.9618592 3.9512331]\n",
      "Reset environment\n",
      "Episode reward: 2018.030622601509\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9637985 3.9424968 3.940627  2.3027773 3.9646752 3.954408 ]\n",
      "Reset environment\n",
      "Episode reward: 2855.424592077732\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9684641 3.9471273 3.9453208 2.3079195 3.9688196 3.9590719]\n",
      "Reset environment\n",
      "Episode reward: 1435.47974473238\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9718392 3.9505439 3.9486556 2.3117065 3.971851  3.9624436]\n",
      "Reset environment\n",
      "Episode reward: 1338.0906720161438\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9750369 3.9537513 3.951847  2.3152711 3.9746802 3.965638 ]\n",
      "Reset environment\n",
      "Episode reward: 4112.4790679216385\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9819925 3.9606326 3.958875  2.3228703 3.9809263 3.9725838]\n",
      "Reset environment\n",
      "Episode reward: 5628.060770988464\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9915042 3.9701538 3.9683747 2.3332434 3.989485  3.9821074]\n",
      "Reset environment\n",
      "Episode reward: 3709.6799560189247\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [3.9976585 3.9762285 3.9746108 2.3400912 3.9949594 3.9882667]\n",
      "Reset environment\n",
      "Episode reward: 3499.107384979725\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0032644 3.9818602 3.980199  2.346313  3.9999323 3.9938755]\n",
      "Reset environment\n",
      "Episode reward: 3605.836489662528\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0092893 3.987739  3.9863424 2.35295   4.005232  3.9998918]\n",
      "Reset environment\n",
      "Episode reward: 3424.2433707267046\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.016105  3.9947245 3.9929569 2.3604937 4.0111413 4.006705 ]\n",
      "Reset environment\n",
      "Episode reward: 2744.9364317432046\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.021399  4.000247  3.9979553 2.3665926 4.015428  4.012005 ]\n",
      "Reset environment\n",
      "Episode reward: 2080.7943130135536\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0259323 4.004812  4.00245   2.3716202 4.0194516 4.0165353]\n",
      "Reset environment\n",
      "Episode reward: 4554.642494857311\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.033482  4.012356  4.01001   2.3799236 4.026151  4.024084 ]\n",
      "Reset environment\n",
      "Episode reward: 2486.5721163749695\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0373106 4.0161734 4.013845  2.3841443 4.029536  4.027908 ]\n",
      "Reset environment\n",
      "Episode reward: 4764.093742072582\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0465183 4.0254626 4.022982  2.3942163 4.0378137 4.0371275]\n",
      "Reset environment\n",
      "Episode reward: 1296.1153333187103\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.049594  4.028563  4.0260425 2.3976812 4.0405273 4.040203 ]\n",
      "Reset environment\n",
      "Episode reward: 1035.0050796866417\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0519447 4.0311847 4.028108  2.4005153 4.0423765 4.0425534]\n",
      "Reset environment\n",
      "Episode reward: 2114.7965639829636\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.055565  4.035044  4.03144   2.4049017 4.045156  4.0461736]\n",
      "Reset environment\n",
      "Episode reward: 3704.2510377168655\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0617347 4.0413055 4.037523  2.41177   4.0506344 4.0523353]\n",
      "Reset environment\n",
      "Episode reward: 1242.2285875082016\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.06471   4.044276  4.0405107 2.4151647 4.0532346 4.055312 ]\n",
      "Reset environment\n",
      "Episode reward: 2739.415338218212\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0689206 4.0484967 4.044713  2.4198592 4.0569525 4.059522 ]\n",
      "Reset environment\n",
      "Episode reward: 383.46405816078186\n",
      "Total Steps: 15\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0691395 4.048717  4.0449295 2.420213  4.0570874 4.05974  ]\n",
      "Reset environment\n",
      "Episode reward: 1472.3704543113708\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.072588  4.0521374 4.048401  2.4240415 4.0601654 4.0631795]\n",
      "Reset environment\n",
      "Episode reward: 3193.306749880314\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0778413 4.0573177 4.0537133 2.4298785 4.064812  4.068425 ]\n",
      "Reset environment\n",
      "Episode reward: 3938.299376606941\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0844564 4.063999  4.060265  2.4372034 4.0707045 4.0750422]\n",
      "Reset environment\n",
      "Episode reward: 2562.970105007291\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0896053 4.069356  4.0651765 2.4430182 4.075041  4.0801997]\n",
      "Reset environment\n",
      "Episode reward: 5008.252346634865\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0981426 4.077965  4.073645  2.4523976 4.082635  4.088738 ]\n",
      "Reset environment\n",
      "Episode reward: -429.49383294582367\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.0969453 4.0766573 4.0725846 2.450775  4.0818825 4.087537 ]\n",
      "Reset environment\n",
      "Episode reward: 6310.372704982758\n",
      "Total Steps: 218\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.10772   4.0874724 4.0833077 2.4625304 4.0915413 4.0983114]\n",
      "Reset environment\n",
      "Episode reward: 1546.9003066886216\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1108236 4.0908284 4.0861187 2.4662716 4.094004  4.101407 ]\n",
      "Reset environment\n",
      "Episode reward: 3254.9072771817446\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.116889  4.097084  4.0919495 2.4730453 4.0992045 4.107473 ]\n",
      "Reset environment\n",
      "Episode reward: 1973.4100267887115\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1211643 4.101355  4.096223  2.477794  4.1029983 4.1117435]\n",
      "Reset environment\n",
      "Episode reward: 669.4423466920853\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1223516 4.1027246 4.09724   2.4791887 4.104075  4.1129246]\n",
      "Reset environment\n",
      "Episode reward: 4063.874034702778\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.128897  4.1092615 4.103784  2.486461  4.109859  4.119468 ]\n",
      "Reset environment\n",
      "Episode reward: 5996.963512480259\n",
      "Total Steps: 210\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1391463 4.119579  4.113965  2.4976602 4.119002  4.129712 ]\n",
      "Reset environment\n",
      "Episode reward: 844.4037389159203\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1409717 4.1212378 4.1159616 2.4996903 4.1205397 4.131539 ]\n",
      "Reset environment\n",
      "Episode reward: 246.76578211784363\n",
      "Total Steps: 12\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1409607 4.121229  4.1159477 2.499777  4.1204925 4.1315274]\n",
      "Reset environment\n",
      "Episode reward: 1360.3162528797984\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1435814 4.123591  4.118804  2.502863  4.1226945 4.134132 ]\n",
      "Reset environment\n",
      "Episode reward: 3406.476780474186\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.148949  4.1289487 4.124187  2.5087929 4.127481  4.139495 ]\n",
      "Reset environment\n",
      "Episode reward: 2307.394642278552\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.153571  4.1337705 4.128576  2.5140345 4.131428  4.144109 ]\n",
      "Reset environment\n",
      "Episode reward: 3987.4568704366684\n",
      "Total Steps: 215\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1604433 4.1409225 4.1351814 2.5218616 4.137736  4.15099  ]\n",
      "Reset environment\n",
      "Episode reward: 3130.4363802671432\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.165237  4.145697  4.13999   2.5272288 4.141985  4.155783 ]\n",
      "Reset environment\n",
      "Episode reward: 1743.157104730606\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.167901  4.148397  4.1426134 2.5302243 4.1443706 4.158439 ]\n",
      "Reset environment\n",
      "Episode reward: 1357.2771554142237\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.170854  4.151597  4.1452947 2.5337389 4.1467705 4.1613855]\n",
      "Reset environment\n",
      "Episode reward: 2518.2158905267715\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.174629  4.1553674 4.149082  2.5379524 4.150081  4.1651616]\n",
      "Reset environment\n",
      "Episode reward: 3614.6689554154873\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1815705 4.1624885 4.155839  2.5457535 4.156223  4.1721015]\n",
      "Reset environment\n",
      "Episode reward: 4559.598294734955\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.189174  4.170069  4.1634703 2.5540192 4.163057  4.179701 ]\n",
      "Reset environment\n",
      "Episode reward: 4900.989892184734\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1974    4.178277  4.1717143 2.5629063 4.1704335 4.187932 ]\n",
      "Reset environment\n",
      "Episode reward: 1066.8831214308739\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.1994696 4.1805234 4.173604  2.5652413 4.172263  4.1899934]\n",
      "Reset environment\n",
      "Episode reward: 3221.229178905487\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.204704  4.185719  4.178878  2.571009  4.1769295 4.195223 ]\n",
      "Reset environment\n",
      "Episode reward: 1925.2288843393326\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.207562  4.1885543 4.1817493 2.574212  4.179428  4.1980786]\n",
      "Reset environment\n",
      "Episode reward: 5513.172278761864\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2167993 4.197838  4.190952  2.5843322 4.1877546 4.2073216]\n",
      "Reset environment\n",
      "Episode reward: 2065.877277135849\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.221207  4.2022142 4.1953835 2.5892072 4.1916714 4.2117248]\n",
      "Reset environment\n",
      "Episode reward: 2110.273426294327\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.225623  4.206651  4.1997867 2.594117  4.195613  4.2161427]\n",
      "Reset environment\n",
      "Episode reward: 1766.5366078615189\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2282662 4.2092905 4.2024355 2.597098  4.1979294 4.2187862]\n",
      "Reset environment\n",
      "Episode reward: 4088.3151527941227\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2348943 4.216068  4.2089047 2.6044543 4.2038536 4.2254114]\n",
      "Reset environment\n",
      "Episode reward: 3744.1086723208427\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2410445 4.222059  4.2151856 2.611259  4.2093244 4.231559 ]\n",
      "Reset environment\n",
      "Episode reward: 3320.6383772194386\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2463427 4.227301  4.2205286 2.6171627 4.214015  4.2368507]\n",
      "Reset environment\n",
      "Episode reward: 1889.4096138328314\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.24987   4.2310715 4.223761  2.621352  4.216839  4.240376 ]\n",
      "Reset environment\n",
      "Episode reward: 2474.621737718582\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.253781  4.2349625 4.2276797 2.6256838 4.2202997 4.24428  ]\n",
      "Reset environment\n",
      "Episode reward: 4564.882961630821\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.262517  4.2436986 4.2363873 2.6353338 4.2280607 4.253017 ]\n",
      "Reset environment\n",
      "Episode reward: 3805.0686965212226\n",
      "Total Steps: 748\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.263185  4.244787  4.2366157 2.6377912 4.227904  4.2536936]\n",
      "Reset environment\n",
      "Episode reward: 1922.154416024685\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2669053 4.2487245 4.2400513 2.6420932 4.2309484 4.257401 ]\n",
      "Reset environment\n",
      "Episode reward: 2346.7797718942165\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2704906 4.252244  4.2436767 2.6460671 4.234073  4.260977 ]\n",
      "Reset environment\n",
      "Episode reward: 3139.599751472473\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2756066 4.257359  4.2487955 2.6516569 4.238657  4.266086 ]\n",
      "Reset environment\n",
      "Episode reward: 2726.2834859490395\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2806945 4.262145  4.254153  2.657498  4.2428865 4.27117  ]\n",
      "Reset environment\n",
      "Episode reward: 4798.442681252956\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2886486 4.2700706 4.2621303 2.6661305 4.2499743 4.279117 ]\n",
      "Reset environment\n",
      "Episode reward: 3280.9713299274445\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2937427 4.2752085 4.2671742 2.6717572 4.254506  4.284217 ]\n",
      "Reset environment\n",
      "Episode reward: -1124.464482575655\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2913566 4.272682  4.2649403 2.6695936 4.252222  4.28182  ]\n",
      "Reset environment\n",
      "Episode reward: 4315.667212843895\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.2981825 4.279447  4.271807  2.6770914 4.2582994 4.288641 ]\n",
      "Reset environment\n",
      "Episode reward: 2166.078415468335\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3019576 4.283434  4.2752805 2.68155   4.2612433 4.2924066]\n",
      "Reset environment\n",
      "Episode reward: 1933.462965786457\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3060718 4.2875314 4.2794194 2.6861403 4.2648764 4.2965198]\n",
      "Reset environment\n",
      "Episode reward: 1291.8953446149826\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.309081  4.290521  4.2824416 2.689525  4.2675176 4.299523 ]\n",
      "Reset environment\n",
      "Episode reward: 272.20140421390533\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3095384 4.2911606 4.282722  2.690172  4.267898  4.2999783]\n",
      "Reset environment\n",
      "Episode reward: 4650.592707633972\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3169622 4.298679  4.2900395 2.6983378 4.2744484 4.307395 ]\n",
      "Reset environment\n",
      "Episode reward: 859.4328877329826\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3179536 4.299919  4.2907786 2.699677  4.2751746 4.308386 ]\n",
      "Reset environment\n",
      "Episode reward: 2856.0332733392715\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.322277  4.3042603 4.2950954 2.7044756 4.2790136 4.312711 ]\n",
      "Reset environment\n",
      "Episode reward: 4578.554644346237\n",
      "Total Steps: 225\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.330947  4.3129416 4.303742  2.7139826 4.2868476 4.3213725]\n",
      "Reset environment\n",
      "Episode reward: 4546.173137307167\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3383975 4.3204546 4.311129  2.7221317 4.293537  4.328831 ]\n",
      "Reset environment\n",
      "Episode reward: 1258.762893974781\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.341311  4.323341  4.3140674 2.725438  4.2960863 4.3317413]\n",
      "Reset environment\n",
      "Episode reward: 4283.565702259541\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3482995 4.3302894 4.321103  2.7331178 4.3023663 4.338735 ]\n",
      "Reset environment\n",
      "Episode reward: 2528.543278425932\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.352099  4.334018  4.324954  2.7373078 4.305684  4.342527 ]\n",
      "Reset environment\n",
      "Episode reward: 3267.182227343321\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3572764 4.3391323 4.330187  2.7430482 4.3102846 4.3477054]\n",
      "Reset environment\n",
      "Episode reward: 1233.343452513218\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3601384 4.34197   4.3330603 2.746264  4.3127966 4.3505597]\n",
      "Reset environment\n",
      "Episode reward: 4052.3487104177475\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.366513  4.348422  4.3393593 2.7532954 4.31848   4.3569293]\n",
      "Reset environment\n",
      "Episode reward: 4642.07869464159\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.373957  4.3558354 4.3468423 2.7614572 4.3251157 4.3643703]\n",
      "Reset environment\n",
      "Episode reward: -21.02504050731659\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.373727  4.355305  4.3469267 2.7616816 4.3247886 4.3641367]\n",
      "Reset environment\n",
      "Episode reward: 3023.8740422725677\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.378481  4.3600273 4.3517175 2.7669296 4.3290234 4.3688955]\n",
      "Reset environment\n",
      "Episode reward: 6747.2454280257225\n",
      "Total Steps: 238\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.3898826 4.3714404 4.3630924 2.7793472 4.3392415 4.3803005]\n",
      "Reset environment\n",
      "Episode reward: 3565.6609902381897\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.395537  4.3769097 4.3688955 2.7856092 4.3441386 4.385953 ]\n",
      "Reset environment\n",
      "Episode reward: 4486.775649428368\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4028788 4.384323  4.3761725 2.7937028 4.3506813 4.3932986]\n",
      "Reset environment\n",
      "Episode reward: 4582.790712296963\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4104204 4.3919525 4.3836246 2.801968  4.357372  4.4008374]\n",
      "Reset environment\n",
      "Episode reward: 6053.350483298302\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.420383  4.401912  4.393609  2.8129244 4.366317  4.410807 ]\n",
      "Reset environment\n",
      "Episode reward: 3101.47065359354\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4252877 4.4068036 4.3985286 2.8183115 4.3707047 4.4157143]\n",
      "Reset environment\n",
      "Episode reward: 1653.0754549503326\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4276867 4.409224  4.4009185 2.821029  4.372829  4.4181147]\n",
      "Reset environment\n",
      "Episode reward: 2615.5885377526283\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.432677  4.414437  4.4056096 2.8267517 4.3768487 4.4231005]\n",
      "Reset environment\n",
      "Episode reward: 1948.4868687987328\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.435569  4.417306  4.408524  2.8300192 4.3793864 4.425993 ]\n",
      "Reset environment\n",
      "Episode reward: 131.76242817938328\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.435821  4.4177866 4.4085646 2.8307755 4.379443  4.4262505]\n",
      "Reset environment\n",
      "Episode reward: 2026.03932005167\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.440023  4.4219666 4.4127893 2.8354375 4.383159  4.43045  ]\n",
      "Reset environment\n",
      "Episode reward: 2111.6268486380577\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.443198  4.425128  4.415976  2.838997  4.3859487 4.433622 ]\n",
      "Reset environment\n",
      "Episode reward: 6395.771515011787\n",
      "Total Steps: 221\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4538875 4.435879  4.426568  2.8505704 4.395497  4.444298 ]\n",
      "Reset environment\n",
      "Episode reward: 2106.0060591101646\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4582324 4.440226  4.4309096 2.8553634 4.3993692 4.44864  ]\n",
      "Reset environment\n",
      "Episode reward: 3992.6860340237617\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.464524  4.446689  4.4369726 2.862359  4.4046874 4.4549274]\n",
      "Reset environment\n",
      "Episode reward: 3818.0931637883186\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.470398  4.4523287 4.4430623 2.8689482 4.409664  4.460804 ]\n",
      "Reset environment\n",
      "Episode reward: 5167.520503520966\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.48012   4.4620156 4.452833  2.8794358 4.4183807 4.470539 ]\n",
      "Reset environment\n",
      "Episode reward: 1305.2168154716492\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.48308   4.464967  4.455802  2.8827536 4.4209843 4.4734955]\n",
      "Reset environment\n",
      "Episode reward: 1221.826400578022\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4853854 4.467528  4.4578514 2.8855474 4.422977  4.475801 ]\n",
      "Reset environment\n",
      "Episode reward: 4162.5741969347\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.491982 4.474091 4.464469 2.892809 4.428852 4.482398]\n",
      "Reset environment\n",
      "Episode reward: -902.2062571197748\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.490484  4.4722376 4.463346  2.8917785 4.4272    4.48089  ]\n",
      "Reset environment\n",
      "Episode reward: 3873.7284983992577\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.496628  4.4782662 4.469588  2.8985314 4.432706  4.4870267]\n",
      "Reset environment\n",
      "Episode reward: 5291.740641053766\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.490538  4.4720325 4.4637966 2.8830225 4.4291162 4.4810405]\n",
      "Reset environment\n",
      "Episode reward: 3350.6006770431995\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.4956717 4.4770956 4.4689994 2.8887289 4.4336796 4.4861665]\n",
      "Reset environment\n",
      "Episode reward: 5002.119691669941\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5049663 4.486318  4.478354  2.898913  4.442008  4.4954457]\n",
      "Reset environment\n",
      "Episode reward: 4686.331051886082\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5122795 4.493665  4.485641  2.9069767 4.4485016 4.502774 ]\n",
      "Reset environment\n",
      "Episode reward: 1214.1428663730621\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5150433 4.49641   4.488429  2.9101195 4.4509106 4.505542 ]\n",
      "Reset environment\n",
      "Episode reward: 4320.503510326147\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.521948  4.50343   4.4952135 2.917748  4.457035  4.512438 ]\n",
      "Reset environment\n",
      "Episode reward: -100.0548586845398\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.521924  4.503379  4.4952245 2.9176002 4.4572473 4.512408 ]\n",
      "Reset environment\n",
      "Episode reward: 1317.6939496994019\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5248437 4.506337  4.4981008 2.9209254 4.4598317 4.51532  ]\n",
      "Reset environment\n",
      "Episode reward: 2863.9551081061363\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5301304 4.5113616 4.5036287 2.9269793 4.464251  4.520606 ]\n",
      "Reset environment\n",
      "Episode reward: 508.7175717353821\n",
      "Total Steps: 18\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5305376 4.511773  4.5040336 2.9275393 4.464557  4.5210137]\n",
      "Reset environment\n",
      "Episode reward: 1217.618455171585\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5333023 4.5145283 4.506817  2.9306767 4.4669743 4.5237827]\n",
      "Reset environment\n",
      "Episode reward: 2575.2051892876625\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5372643 4.5184774 4.5107937 2.9350333 4.4705167 4.527742 ]\n",
      "Reset environment\n",
      "Episode reward: 2758.697568356991\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5413322 4.522559  4.5148463 2.9395137 4.4741507 4.531804 ]\n",
      "Reset environment\n",
      "Episode reward: -581.8472179174423\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5402393 4.5211387 4.5141115 2.9387963 4.4730477 4.5307097]\n",
      "Reset environment\n",
      "Episode reward: 4317.191969156265\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5471215 4.527992  4.5210266 2.9462636 4.4792266 4.537596 ]\n",
      "Reset environment\n",
      "Episode reward: 1163.8683260083199\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5497575 4.5306473 4.52365   2.9492993 4.481523  4.5402374]\n",
      "Reset environment\n",
      "Episode reward: 4992.6654769182205\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5575943 4.5384593 4.5315094 2.9579422 4.4884677 4.5480657]\n",
      "Reset environment\n",
      "Episode reward: 131.49645936489105\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.557762  4.538491  4.5318317 2.9580524 4.4887295 4.5482306]\n",
      "Reset environment\n",
      "Episode reward: 2168.5086684823036\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.562169  4.542879  4.536254  2.9629178 4.4926405 4.552627 ]\n",
      "Reset environment\n",
      "Episode reward: 3684.8584321141243\n",
      "Total Steps: 210\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5683928 4.549335  4.5422626 2.9701087 4.4983735 4.5588517]\n",
      "Reset environment\n",
      "Episode reward: 1083.0442497134209\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5705733 4.5516443 4.5443254 2.9727426 4.500272  4.5610313]\n",
      "Reset environment\n",
      "Episode reward: 4339.63335454464\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.57729   4.5584927 4.5508876 2.9801948 4.5062094 4.5677404]\n",
      "Reset environment\n",
      "Episode reward: 1628.4318606853485\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5795565 4.560737  4.5531707 2.9827607 4.508207  4.570006 ]\n",
      "Reset environment\n",
      "Episode reward: 1866.9602298140526\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.582274  4.563411  4.5559216 2.9857876 4.510602  4.57272  ]\n",
      "Reset environment\n",
      "Episode reward: 1425.4725459516048\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.584761  4.565587  4.558712  2.9889348 4.512598  4.5752053]\n",
      "Reset environment\n",
      "Episode reward: 796.8241198062897\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5859013 4.566989  4.5595937 2.9905047 4.513361  4.5763526]\n",
      "Reset environment\n",
      "Episode reward: 2165.1830040216446\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.5889187 4.570034  4.5625887 2.993903  4.5160546 4.5793686]\n",
      "Reset environment\n",
      "Episode reward: 2183.742136180401\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.592116  4.5731926 4.565805  2.9975166 4.5188365 4.582563 ]\n",
      "Reset environment\n",
      "Episode reward: 2733.2323096990585\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.596075  4.577159  4.5697722 3.00196   4.522314  4.586532 ]\n",
      "Reset environment\n",
      "Episode reward: 2232.0344347953796\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6005387 4.5816507 4.5742207 3.0069249 4.5262966 4.5909967]\n",
      "Reset environment\n",
      "Episode reward: 4617.404855281115\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6079164 4.5891323 4.5814996 3.0149925 4.53287   4.5983763]\n",
      "Reset environment\n",
      "Episode reward: 1541.446261882782\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6100307 4.591203  4.5836544 3.0174098 4.5347285 4.600483 ]\n",
      "Reset environment\n",
      "Episode reward: 2871.5681985020638\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.614233  4.5954204 4.587854  3.0221019 4.5384364 4.6046834]\n",
      "Reset environment\n",
      "Episode reward: 2725.739032328129\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.618295  4.599428  4.591969  3.0266232 4.542029  4.608745 ]\n",
      "Reset environment\n",
      "Episode reward: 1950.009456396103\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.621095  4.6021748 4.594816  3.0297773 4.544484  4.611542 ]\n",
      "Reset environment\n",
      "Episode reward: 2891.251378953457\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6264687 4.607738  4.599935  3.0358148 4.548908  4.6169143]\n",
      "Reset environment\n",
      "Episode reward: 3804.522018522024\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6323037 4.613738  4.6055536 3.0422935 4.5537825 4.6227474]\n",
      "Reset environment\n",
      "Episode reward: 4052.3376562595367\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.638461  4.619926  4.6116724 3.0490832 4.5592637 4.6289015]\n",
      "Reset environment\n",
      "Episode reward: 3587.091413348913\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.643853  4.625106  4.6172447 3.0550835 4.5638475 4.634289 ]\n",
      "Reset environment\n",
      "Episode reward: 2811.250390827656\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6480665 4.629233  4.6215153 3.0597315 4.5675774 4.6384926]\n",
      "Reset environment\n",
      "Episode reward: 2028.5267345905304\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6509414 4.6320505 4.6244235 3.062946  4.569978  4.6413665]\n",
      "Reset environment\n",
      "Episode reward: 6294.279160320759\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.661132  4.642221  4.634634  3.0739908 4.5791483 4.6515517]\n",
      "Reset environment\n",
      "Episode reward: 3161.120959699154\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.66586   4.6469474 4.6393657 3.0791779 4.5833616 4.6562786]\n",
      "Reset environment\n",
      "Episode reward: 3691.5718383789062\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.671445  4.6525817 4.6448903 3.085318  4.5883393 4.661846 ]\n",
      "Reset environment\n",
      "Episode reward: 1618.3831495456398\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6744447 4.655806  4.6476393 3.0888536 4.590726  4.6648483]\n",
      "Reset environment\n",
      "Episode reward: 5260.051515340805\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6828976 4.6642594 4.656083  3.0980835 4.598316  4.6733017]\n",
      "Reset environment\n",
      "Episode reward: 5509.834230959415\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6917467 4.673169  4.664845  3.1077504 4.606191  4.6821394]\n",
      "Reset environment\n",
      "Episode reward: 1939.902123272419\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.6955943 4.6770425 4.668673  3.1121192 4.6095734 4.6859913]\n",
      "Reset environment\n",
      "Episode reward: 3790.4180661439896\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.701436  4.682923  4.6744742 3.1185665 4.6148    4.691832 ]\n",
      "Reset environment\n",
      "Episode reward: 5375.5986223220825\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7100387 4.6915236 4.683065  3.1278503 4.6225586 4.7004323]\n",
      "Reset environment\n",
      "Episode reward: -632.57250893116\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7086806 4.690232  4.68167   3.1263509 4.6214776 4.6990776]\n",
      "Reset environment\n",
      "Episode reward: 4819.913410631474\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7162504 4.6976466 4.689371  3.1346765 4.6281853 4.7066436]\n",
      "Reset environment\n",
      "Episode reward: 1072.3467134833336\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.717533  4.6989374 4.6906424 3.136225  4.62928   4.707924 ]\n",
      "Reset environment\n",
      "Episode reward: 5116.353088080883\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7256985 4.707082  4.698823  3.1450672 4.6365676 4.716094 ]\n",
      "Reset environment\n",
      "Episode reward: 5273.071411281824\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7338943 4.7151675 4.707097  3.1540058 4.643842  4.724279 ]\n",
      "Reset environment\n",
      "Episode reward: 4284.113421320915\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7406073 4.7218637 4.713831  3.1613564 4.6498613 4.7309895]\n",
      "Reset environment\n",
      "Episode reward: -53.27030473947525\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7404237 4.721879  4.71346   3.161664  4.649534  4.73081  ]\n",
      "Reset environment\n",
      "Episode reward: 5895.936704218388\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.749659  4.731181  4.72263   3.1717079 4.657776  4.740043 ]\n",
      "Reset environment\n",
      "Episode reward: 4157.935201764107\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7559614 4.737509  4.7289143 3.178593  4.6634097 4.746351 ]\n",
      "Reset environment\n",
      "Episode reward: 4931.79951530695\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7637153 4.745344  4.7365737 3.1871407 4.67032   4.7540994]\n",
      "Reset environment\n",
      "Episode reward: 1261.5491263270378\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7664795 4.748126  4.739333  3.190267  4.6727543 4.756866 ]\n",
      "Reset environment\n",
      "Episode reward: 487.1306154355407\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7673583 4.749149  4.7400923 3.1913345 4.673545  4.757751 ]\n",
      "Reset environment\n",
      "Episode reward: 1283.987122297287\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7701774 4.751968  4.7429137 3.194499  4.676026  4.760572 ]\n",
      "Reset environment\n",
      "Episode reward: 2887.5662419199944\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7751923 4.756693  4.7481933 3.2002707 4.680168  4.7655835]\n",
      "Reset environment\n",
      "Episode reward: 3625.9235742092133\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.780611  4.7621136 4.753615  3.2061791 4.6850004 4.7710047]\n",
      "Reset environment\n",
      "Episode reward: 1937.4151465296745\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7845063 4.7660255 4.7574997 3.2104917 4.6884766 4.7748966]\n",
      "Reset environment\n",
      "Episode reward: 3162.5667976140976\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7896495 4.7712784 4.7625194 3.2164128 4.693115  4.7800384]\n",
      "Reset environment\n",
      "Episode reward: 3688.8369864821434\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7953024 4.7769012 4.7681994 3.222647  4.69815   4.78569  ]\n",
      "Reset environment\n",
      "Episode reward: 813.6385259628296\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.7965484 4.778378  4.769225  3.2241201 4.699265  4.7869334]\n",
      "Reset environment\n",
      "Episode reward: 1285.2320029139519\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.799377  4.7811937 4.772072  3.2273054 4.7017546 4.7897625]\n",
      "Reset environment\n",
      "Episode reward: 1330.9487425088882\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8022923 4.784096  4.7749934 3.2305527 4.7043347 4.7926764]\n",
      "Reset environment\n",
      "Episode reward: 1838.7291318774223\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.804894  4.7867236 4.7775636 3.2334979 4.7066627 4.795274 ]\n",
      "Reset environment\n",
      "Episode reward: 2994.619028866291\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8093266 4.7910566 4.782085  3.2384422 4.7105675 4.799705 ]\n",
      "Reset environment\n",
      "Episode reward: 2309.910313606262\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.812612  4.794308  4.7853923 3.2421353 4.713463  4.802985 ]\n",
      "Reset environment\n",
      "Episode reward: 4685.565702617168\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8198094 4.8016133 4.792465  3.2500427 4.7199497 4.8101754]\n",
      "Reset environment\n",
      "Episode reward: 3042.0917185544968\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.824357  4.8061266 4.7970386 3.2550955 4.723986  4.8147187]\n",
      "Reset environment\n",
      "Episode reward: 2480.477710366249\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.828041  4.809785  4.8007517 3.2591648 4.7272625 4.818407 ]\n",
      "Reset environment\n",
      "Episode reward: 664.8995487689972\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8292074 4.8110924 4.8017793 3.2605064 4.7282767 4.8195696]\n",
      "Reset environment\n",
      "Episode reward: 1253.940884232521\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8319445 4.813816  4.8045335 3.2636251 4.730672  4.82231  ]\n",
      "Reset environment\n",
      "Episode reward: 2052.6415190398693\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.82004   4.8018136 4.792825  3.2400103 4.7205195 4.81046  ]\n",
      "Reset environment\n",
      "Episode reward: 1239.1097600758076\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.821948  4.803463  4.7949753 3.2424355 4.722098  4.8123665]\n",
      "Reset environment\n",
      "Episode reward: 1771.425675895065\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.824966  4.8066926 4.797734  3.2460477 4.724538  4.815382 ]\n",
      "Reset environment\n",
      "Episode reward: 2996.9319202303886\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.829432  4.811105  4.8022456 3.250987  4.7285066 4.81984  ]\n",
      "Reset environment\n",
      "Episode reward: 1825.3955369535834\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8328404 4.8147216 4.805426  3.2548978 4.731317  4.8232536]\n",
      "Reset environment\n",
      "Episode reward: 3574.9537976384163\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8395896 4.8213763 4.8122554 3.262517  4.737244  4.8300023]\n",
      "Reset environment\n",
      "Episode reward: 5542.565645813942\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8483667 4.8302317 4.8209505 3.2720468 4.7450976 4.8387756]\n",
      "Reset environment\n",
      "Episode reward: 5106.190955996513\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.856348  4.838191  4.8289576 3.2807386 4.7522864 4.8467684]\n",
      "Reset environment\n",
      "Episode reward: 4770.741172730923\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.863771  4.8456783 4.836338  3.2888982 4.758936  4.8541985]\n",
      "Reset environment\n",
      "Episode reward: 2731.4925978779793\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8677835 4.849669  4.8403735 3.2933474 4.7625127 4.8582096]\n",
      "Reset environment\n",
      "Episode reward: 1958.5817237496376\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8705726 4.8524723 4.8431487 3.2964587 4.765004  4.860998 ]\n",
      "Reset environment\n",
      "Episode reward: 5324.659075498581\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.878852  4.8608055 4.851354  3.3054729 4.77242   4.8692713]\n",
      "Reset environment\n",
      "Episode reward: 5956.023910820484\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8882327 4.870256  4.8606606 3.3156786 4.7808065 4.878654 ]\n",
      "Reset environment\n",
      "Episode reward: 1719.5883937478065\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.8906145 4.872605  4.8630714 3.318347  4.7829113 4.8810334]\n",
      "Reset environment\n",
      "Episode reward: 1800.0053207343444\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.893469  4.8756695 4.8656836 3.3218446 4.78509   4.883889 ]\n",
      "Reset environment\n",
      "Episode reward: 5501.023700535297\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.90202   4.8842335 4.8742175 3.3311872 4.792748  4.892431 ]\n",
      "Reset environment\n",
      "Episode reward: 1106.8326257169247\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.903536  4.8855033 4.87595   3.333235  4.793724  4.89395  ]\n",
      "Reset environment\n",
      "Episode reward: 3993.70709477365\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.910604  4.892547  4.883047  3.3411934 4.800061  4.9010186]\n",
      "Reset environment\n",
      "Episode reward: 2797.1763301491737\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9147215 4.8966994 4.887135  3.3457537 4.8037257 4.905136 ]\n",
      "Reset environment\n",
      "Episode reward: 5261.308025717735\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9226584 4.904638  4.895073  3.3544018 4.8108454 4.9130692]\n",
      "Reset environment\n",
      "Episode reward: 1290.8995038866997\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.925429  4.907415  4.8978367 3.3575144 4.813281  4.915836 ]\n",
      "Reset environment\n",
      "Episode reward: 2431.95478361845\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.929631  4.911828  4.9018226 3.3623004 4.8170047 4.920042 ]\n",
      "Reset environment\n",
      "Episode reward: 1254.4698437154293\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.932027  4.914465  4.9039564 3.3651326 4.818827  4.922447 ]\n",
      "Reset environment\n",
      "Episode reward: 2659.8491680696607\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9367304 4.9193816 4.9083896 3.3705165 4.822483  4.9271593]\n",
      "Reset environment\n",
      "Episode reward: 4285.335018247366\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.943261  4.9259944 4.9148207 3.3777122 4.82826   4.933687 ]\n",
      "Reset environment\n",
      "Episode reward: 1046.2895573973656\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9445205 4.92725   4.916081  3.3792002 4.829339  4.9349456]\n",
      "Reset environment\n",
      "Episode reward: 3793.977402202785\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9502354 4.9328213 4.921907  3.385492  4.8343945 4.9406543]\n",
      "Reset environment\n",
      "Episode reward: -695.6378164291382\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9488134 4.931402  4.9204936 3.3840387 4.833065  4.9392304]\n",
      "Reset environment\n",
      "Episode reward: 1263.731320925057\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.950832  4.933631  4.922272  3.386442  4.8346424 4.941244 ]\n",
      "Reset environment\n",
      "Episode reward: 2904.091742694378\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.954933  4.937784  4.9263263 3.3910131 4.8382816 4.9453487]\n",
      "Reset environment\n",
      "Episode reward: 4591.0052689909935\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9617863 4.944627  4.933189  3.3984835 4.8443985 4.952205 ]\n",
      "Reset environment\n",
      "Episode reward: -701.5386340618134\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.960488 4.943398 4.931827 3.397107 4.843295 4.950901]\n",
      "Reset environment\n",
      "Episode reward: 3326.3491953760386\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.96636   4.9494505 4.9374995 3.4036605 4.8485255 4.9567795]\n",
      "Reset environment\n",
      "Episode reward: 5063.12302929163\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9739594 4.957038  4.9451146 3.4119503 4.855345  4.964376 ]\n",
      "Reset environment\n",
      "Episode reward: 1251.1174367666245\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9760113 4.9593353 4.946887  3.4144375 4.8569727 4.9664335]\n",
      "Reset environment\n",
      "Episode reward: 3602.0801405012608\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.981304  4.964739  4.952075  3.420349  4.861664  4.9717326]\n",
      "Reset environment\n",
      "Episode reward: 2377.587552547455\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.984565  4.96799   4.955347  3.423956  4.8645477 4.974993 ]\n",
      "Reset environment\n",
      "Episode reward: 2486.3623809218407\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9889593 4.972498  4.959642  3.4288695 4.868483  4.979391 ]\n",
      "Reset environment\n",
      "Episode reward: 1238.6107403039932\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9905157 4.9740553 4.9611955 3.430646  4.869832  4.980943 ]\n",
      "Reset environment\n",
      "Episode reward: 3064.4910856485367\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.9950137 4.978498  4.965746  3.4356177 4.8738503 4.985448 ]\n",
      "Reset environment\n",
      "Episode reward: 1826.8933812379837\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [4.997548  4.9810853 4.968234  3.438464  4.8761325 4.987978 ]\n",
      "Reset environment\n",
      "Episode reward: 4861.336236357689\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0050464 4.988647  4.975676  3.4466555 4.8828487 4.995476 ]\n",
      "Reset environment\n",
      "Episode reward: 1202.8539404273033\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.007153  4.990919  4.97762   3.4490929 4.884762  4.997582 ]\n",
      "Reset environment\n",
      "Episode reward: 1464.1329828612506\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0096426 4.993598  4.9799147 3.4519463 4.886943  5.000065 ]\n",
      "Reset environment\n",
      "Episode reward: 2770.592857301235\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.013657  4.9976363 4.983901  3.4564264 4.8905106 5.004076 ]\n",
      "Reset environment\n",
      "Episode reward: 2443.9560528695583\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.017754  5.0014534 4.9882483 3.461156  4.893906  5.008165 ]\n",
      "Reset environment\n",
      "Episode reward: 3694.841465830803\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0233254 5.007077  4.993755  3.4672816 4.8988676 5.013734 ]\n",
      "Reset environment\n",
      "Episode reward: 4090.9647159576416\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.029508  5.0132217 4.9999743 3.4740708 4.9044185 5.0199137]\n",
      "Reset environment\n",
      "Episode reward: 4972.326121330261\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.03829   5.0219884 5.008791  3.4835784 4.9123006 5.028702 ]\n",
      "Reset environment\n",
      "Episode reward: -29.491209536790848\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0379825 5.0215425 5.008641  3.4831219 4.9121943 5.0283947]\n",
      "Reset environment\n",
      "Episode reward: 1633.0354007482529\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0404444 5.0237184 5.011384  3.4862673 4.9140577 5.0308566]\n",
      "Reset environment\n",
      "Episode reward: 476.14594435691833\n",
      "Total Steps: 19\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0407596 5.024037  5.0116973 3.486739  4.9142723 5.0311713]\n",
      "Reset environment\n",
      "Episode reward: 2286.5963349938393\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.043958  5.0272117 5.0149198 3.4902782 4.9171214 5.0343695]\n",
      "Reset environment\n",
      "Episode reward: 2026.0060644745827\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0478096 5.0310955 5.018744  3.494584  4.920543  5.038223 ]\n",
      "Reset environment\n",
      "Episode reward: 1750.1698548793793\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0502    5.033474  5.021142  3.4972746 4.922655  5.040609 ]\n",
      "Reset environment\n",
      "Episode reward: 1625.5998168587685\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.052925  5.0359044 5.0241265 3.5005324 4.9249387 5.0433226]\n",
      "Reset environment\n",
      "Episode reward: 1658.0295935869217\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.055185  5.038195  5.026358  3.503074  4.9269543 5.0455794]\n",
      "Reset environment\n",
      "Episode reward: 2269.7955722799525\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0592923 5.04248   5.0302505 3.5077252 4.9303913 5.049684 ]\n",
      "Reset environment\n",
      "Episode reward: 6084.974425435066\n",
      "Total Steps: 209\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0687003 5.051906  5.0396566 3.5178986 4.9388757 5.0590916]\n",
      "Reset environment\n",
      "Episode reward: 3123.787482023239\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.07308   5.0563035 5.0440197 3.5227447 4.942765  5.0634637]\n",
      "Reset environment\n",
      "Episode reward: 3492.9409831762314\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0783296 5.0616236 5.0491996 3.5285044 4.9474554 5.0687084]\n",
      "Reset environment\n",
      "Episode reward: 1461.3339743614197\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.080236  5.063492  5.0511346 3.5306334 4.949122  5.070611 ]\n",
      "Reset environment\n",
      "Episode reward: 6121.60511547327\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.089722  5.072998  5.060585  3.5409272 4.957633  5.080089 ]\n",
      "Reset environment\n",
      "Episode reward: 2087.5673057436943\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.0926566 5.0759315 5.06352   3.544183  4.960237  5.0830245]\n",
      "Reset environment\n",
      "Episode reward: 3459.081546444446\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.097716  5.0808372 5.068702  3.5497549 4.9646883 5.0880737]\n",
      "Reset environment\n",
      "Episode reward: 3253.142468869686\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.102489  5.0855927 5.073484  3.5549903 4.968961  5.092851 ]\n",
      "Reset environment\n",
      "Episode reward: 2646.2148829102516\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.106123  5.08925   5.077094  3.5590284 4.9721923 5.0964847]\n",
      "Reset environment\n",
      "Episode reward: 6484.217183351517\n",
      "Total Steps: 224\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.116046  5.0991516 5.0870414 3.5697389 4.9811444 5.1064057]\n",
      "Reset environment\n",
      "Episode reward: 4097.170693516731\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.121987  5.105093  5.0929804 3.5762687 4.98645   5.112339 ]\n",
      "Reset environment\n",
      "Episode reward: 3851.102762207389\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.127636  5.110587  5.0987616 3.5824964 4.9914975 5.1179867]\n",
      "Reset environment\n",
      "Episode reward: 2102.5002772808075\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.13169   5.1146116 5.1028466 3.5869627 4.995099  5.122037 ]\n",
      "Reset environment\n",
      "Episode reward: 4326.059572458267\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1382093 5.121125  5.1093597 3.5940845 5.000948  5.128557 ]\n",
      "Reset environment\n",
      "Episode reward: 2251.088339328766\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1412396 5.1241803 5.112369  3.5974236 5.003672  5.131586 ]\n",
      "Reset environment\n",
      "Episode reward: 3575.9882743880153\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.14628   5.1293426 5.1172442 3.6030586 5.008045  5.1366153]\n",
      "Reset environment\n",
      "Episode reward: 1386.8524174690247\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1480737 5.131167  5.1190085 3.6050904 5.0096397 5.1384077]\n",
      "Reset environment\n",
      "Episode reward: 1498.5631001591682\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.151102  5.1342154 5.1220164 3.6084933 5.0123053 5.1414394]\n",
      "Reset environment\n",
      "Episode reward: 2637.6504456996918\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.154796  5.1378455 5.1257677 3.612591  5.0155644 5.1451335]\n",
      "Reset environment\n",
      "Episode reward: 1248.4257057085633\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1567264 5.139507  5.12794   3.6149952 5.0171385 5.147052 ]\n",
      "Reset environment\n",
      "Episode reward: 1546.0154981687665\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.159216  5.1417513 5.130653  3.6179109 5.0192723 5.149541 ]\n",
      "Reset environment\n",
      "Episode reward: 3300.0305267572403\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1640983 5.146682  5.1354756 3.623262  5.023653  5.154417 ]\n",
      "Reset environment\n",
      "Episode reward: 2280.2599197626114\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1678734 5.1506367 5.1390567 3.6275458 5.027023  5.1581845]\n",
      "Reset environment\n",
      "Episode reward: 1664.0841120705009\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.170256  5.1527467 5.1417117 3.63056   5.028906  5.1605606]\n",
      "Reset environment\n",
      "Episode reward: 279.34211707115173\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1697836 5.152504  5.141024  3.6303754 5.028392  5.1601005]\n",
      "Reset environment\n",
      "Episode reward: 4890.518865287304\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.177171  5.1598606 5.148439  3.6383502 5.035048  5.1674867]\n",
      "Reset environment\n",
      "Episode reward: 2850.8120031654835\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1811767 5.1637836 5.1525207 3.642804  5.03862   5.1714926]\n",
      "Reset environment\n",
      "Episode reward: 2104.7831659317017\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.18408   5.1666512 5.155449  3.6460342 5.0411925 5.1743937]\n",
      "Reset environment\n",
      "Episode reward: 2903.20122808218\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1881065 5.1705394 5.1595845 3.6505108 5.0446844 5.178411 ]\n",
      "Reset environment\n",
      "Episode reward: 5258.4614470005035\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1958776 5.1782913 5.1673694 3.658962  5.051626  5.186185 ]\n",
      "Reset environment\n",
      "Episode reward: -691.9861447811127\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.1944666 5.176891  5.1659703 3.6575034 5.0502872 5.1847763]\n",
      "Reset environment\n",
      "Episode reward: 6043.5058426856995\n",
      "Total Steps: 207\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.203576  5.186054  5.175038  3.6674783 5.0585423 5.1938953]\n",
      "Reset environment\n",
      "Episode reward: 5405.43870973587\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.211806  5.1942654 5.1832952 3.6763444 5.0659103 5.20213  ]\n",
      "Reset environment\n",
      "Episode reward: 1277.0961077213287\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.214464  5.1969275 5.1859555 3.6793487 5.068242  5.2047944]\n",
      "Reset environment\n",
      "Episode reward: 1615.5134528130293\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2167835 5.198979  5.1885424 3.6822367 5.070103  5.207117 ]\n",
      "Reset environment\n",
      "Episode reward: 5288.182214617729\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.22447   5.2067013 5.196176  3.6906881 5.077043  5.2148037]\n",
      "Reset environment\n",
      "Episode reward: 1609.9353483319283\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.226562  5.208753  5.1982903 3.6929972 5.0788956 5.216888 ]\n",
      "Reset environment\n",
      "Episode reward: 3336.8078364133835\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.231388  5.2135487 5.2031407 3.6983082 5.0832095 5.221718 ]\n",
      "Reset environment\n",
      "Episode reward: 1739.6020055115223\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2338886 5.215774  5.2058873 3.7014108 5.085139  5.224219 ]\n",
      "Reset environment\n",
      "Episode reward: -119.13538908958435\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2335434 5.21522   5.2057586 3.7011027 5.0848565 5.2238765]\n",
      "Reset environment\n",
      "Episode reward: 2783.3998090326786\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.237445  5.219053  5.209724  3.7054234 5.088317  5.2277765]\n",
      "Reset environment\n",
      "Episode reward: 3920.424940109253\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2432065 5.22485   5.2154427 3.711749  5.093455  5.233533 ]\n",
      "Reset environment\n",
      "Episode reward: 1617.5425298213959\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2453294 5.226953  5.2175803 3.7141273 5.0953317 5.235651 ]\n",
      "Reset environment\n",
      "Episode reward: 2383.6283577382565\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2486053 5.2301807 5.220902  3.7177856 5.0981956 5.2389317]\n",
      "Reset environment\n",
      "Episode reward: 1590.7886866033077\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2512474 5.2330303 5.223338  3.720891  5.100523  5.241582 ]\n",
      "Reset environment\n",
      "Episode reward: 1375.9144304990768\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.252977  5.234757  5.225073  3.7228603 5.1020465 5.24331  ]\n",
      "Reset environment\n",
      "Episode reward: 2549.50213377364\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.257338  5.239314  5.2292094 3.72778   5.1056376 5.2476835]\n",
      "Reset environment\n",
      "Episode reward: 3386.5844579190016\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2632985 5.245407  5.234982  3.7343879 5.1107655 5.2536397]\n",
      "Reset environment\n",
      "Episode reward: 1216.9616386890411\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.264867  5.247189  5.236331  3.7363577 5.111985  5.2552195]\n",
      "Reset environment\n",
      "Episode reward: 387.1003898382187\n",
      "Total Steps: 15\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2650294 5.247349  5.236493  3.7366314 5.112092  5.255379 ]\n",
      "Reset environment\n",
      "Episode reward: 2447.033123642206\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2684126 5.2506747 5.2399125 3.740355  5.115083  5.2587576]\n",
      "Reset environment\n",
      "Episode reward: 5231.060454189777\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.276078  5.258315  5.247597  3.7486525 5.1219654 5.2664237]\n",
      "Reset environment\n",
      "Episode reward: 2741.7126585245132\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2799344 5.2621703 5.2514496 3.7529094 5.125433  5.2702765]\n",
      "Reset environment\n",
      "Episode reward: 2026.576216518879\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.283781  5.265997  5.255316  3.757168  5.1288543 5.274128 ]\n",
      "Reset environment\n",
      "Episode reward: 1286.893363237381\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2864347 5.2686176 5.258006  3.7601655 5.131187  5.276782 ]\n",
      "Reset environment\n",
      "Episode reward: 1771.8269820958376\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.289474  5.271858  5.2608356 3.7636595 5.133765  5.2798295]\n",
      "Reset environment\n",
      "Episode reward: 2164.261969447136\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.2923956 5.27476   5.2637625 3.7669454 5.1363277 5.2827487]\n",
      "Reset environment\n",
      "Episode reward: 4802.087835252285\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.299333 5.2818   5.270587 3.77458  5.142336 5.289683]\n",
      "Reset environment\n",
      "Episode reward: 3690.517572760582\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3046665 5.2871003 5.2759495 3.780459  5.1471004 5.29502  ]\n",
      "Reset environment\n",
      "Episode reward: 5772.921168804169\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3133063 5.295794  5.2845416 3.7898128 5.154902  5.303664 ]\n",
      "Reset environment\n",
      "Episode reward: 2166.2612991034985\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.316217  5.298653  5.287485  3.7930322 5.157461  5.306576 ]\n",
      "Reset environment\n",
      "Episode reward: 5545.300467431545\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3244967 5.3069882 5.29569   3.8020587 5.164887  5.3148484]\n",
      "Reset environment\n",
      "Episode reward: 3013.082273632288\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.329318  5.311992  5.3002753 3.8074818 5.1689563 5.319673 ]\n",
      "Reset environment\n",
      "Episode reward: 2168.8705651164055\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.333354  5.31604   5.304303  3.8119256 5.1725755 5.323712 ]\n",
      "Reset environment\n",
      "Episode reward: 2122.19126868248\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.337335  5.3200026 5.3082986 3.8163176 5.1761136 5.3276844]\n",
      "Reset environment\n",
      "Episode reward: 5523.461665391922\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3453336 5.328041  5.316233  3.8251007 5.183198  5.3356705]\n",
      "Reset environment\n",
      "Episode reward: 4087.8687847852707\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.351266  5.3340597 5.3220644 3.8316455 5.1884837 5.3416038]\n",
      "Reset environment\n",
      "Episode reward: 2482.059455215931\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3547    5.337447  5.325534  3.8354878 5.19151   5.3450403]\n",
      "Reset environment\n",
      "Episode reward: 3891.427818428725\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3603034 5.3431883 5.3309608 3.8416617 5.196339  5.3506413]\n",
      "Reset environment\n",
      "Episode reward: 2373.71765512228\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.363446  5.3463216 5.334107  3.845155  5.1991167 5.3537784]\n",
      "Reset environment\n",
      "Episode reward: 4991.776268571615\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3707957 5.3536453 5.341454  3.8532312 5.205631  5.36112  ]\n",
      "Reset environment\n",
      "Episode reward: 1907.6384224891663\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.373591  5.356652  5.343991  3.8565838 5.2077317 5.363918 ]\n",
      "Reset environment\n",
      "Episode reward: 752.1541830301285\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.374293  5.3573484 5.3446927 3.8574946 5.2082963 5.364618 ]\n",
      "Reset environment\n",
      "Episode reward: 2115.71574395895\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.377862  5.3607397 5.3484206 3.8614283 5.2114286 5.368178 ]\n",
      "Reset environment\n",
      "Episode reward: 6387.374532878399\n",
      "Total Steps: 217\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.387486  5.37035   5.3580627 3.871833  5.2201314 5.3778157]\n",
      "Reset environment\n",
      "Episode reward: 1976.2277002334595\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3902006 5.3730946 5.360742  3.8748534 5.222557  5.380525 ]\n",
      "Reset environment\n",
      "Episode reward: 5995.476509153843\n",
      "Total Steps: 211\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.3989496 5.3819222 5.369427  3.8844144 5.230329  5.3892694]\n",
      "Reset environment\n",
      "Episode reward: 2567.1485196202993\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4023714 5.3852625 5.372903  3.8882306 5.233239  5.392682 ]\n",
      "Reset environment\n",
      "Episode reward: 1309.671944141388\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.405028  5.3879347 5.375552  3.891245  5.2355657 5.3953376]\n",
      "Reset environment\n",
      "Episode reward: 4645.540014028549\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.41188   5.394829  5.38236   3.8987107 5.2416987 5.402185 ]\n",
      "Reset environment\n",
      "Episode reward: 3127.3911017477512\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4161572 5.399253  5.3864536 3.903499  5.2452583 5.406463 ]\n",
      "Reset environment\n",
      "Episode reward: 4052.9717719852924\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4220195 5.4050326 5.392384  3.9099417 5.250534  5.4123306]\n",
      "Reset environment\n",
      "Episode reward: 3784.709041953087\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.427555  5.4106393 5.397854  3.91598   5.255512  5.4178605]\n",
      "Reset environment\n",
      "Episode reward: 3192.601694405079\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4320436 5.4151754 5.4022903 3.921035  5.2594523 5.422348 ]\n",
      "Reset environment\n",
      "Episode reward: 2085.9226927757263\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4349203 5.4180436 5.405171  3.924233  5.2620254 5.425219 ]\n",
      "Reset environment\n",
      "Episode reward: 1718.8003494739532\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4372234 5.4203644 5.407447  3.9268034 5.2640867 5.4275165]\n",
      "Reset environment\n",
      "Episode reward: 4727.396933853626\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4439616 5.427128  5.4141636 3.9341288 5.270116  5.4342504]\n",
      "Reset environment\n",
      "Episode reward: 3252.8920242786407\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.449711  5.4327807 5.419987  3.9405944 5.2753053 5.440006 ]\n",
      "Reset environment\n",
      "Episode reward: 3890.852918624878\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4552474 5.4382815 5.42555   3.9467046 5.2802234 5.4455476]\n",
      "Reset environment\n",
      "Episode reward: 1005.7961773276329\n",
      "Total Steps: 41\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4563026 5.439337  5.4266157 3.9479918 5.2811112 5.446607 ]\n",
      "Reset environment\n",
      "Episode reward: 1571.0887564122677\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.458441  5.4411926 5.429021  3.9507902 5.282746  5.4487367]\n",
      "Reset environment\n",
      "Episode reward: 1364.7205051779747\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.461201  5.4439416 5.4317994 3.95386   5.285181  5.4515014]\n",
      "Reset environment\n",
      "Episode reward: 4979.033724069595\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.468521  5.451244  5.439133  3.9618225 5.2917423 5.458815 ]\n",
      "Reset environment\n",
      "Episode reward: 4760.334625959396\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4755073 5.458285  5.446076  3.969475  5.298003  5.4658003]\n",
      "Reset environment\n",
      "Episode reward: 1565.4124034643173\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4785542 5.461323  5.4491267 3.9728806 5.3006825 5.468845 ]\n",
      "Reset environment\n",
      "Episode reward: 2485.9895460009575\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4819145 5.4647427 5.452423  3.9766676 5.3036556 5.4722023]\n",
      "Reset environment\n",
      "Episode reward: 3420.9957472085953\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.486814  5.469693  5.4572663 3.9820647 5.3080244 5.477094 ]\n",
      "Reset environment\n",
      "Episode reward: 5211.076807081699\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.4942384 5.4771347 5.464674  3.9901848 5.3146334 5.4845147]\n",
      "Reset environment\n",
      "Episode reward: 3073.0809049010277\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.49835   5.481384  5.4686313 3.9947858 5.3182726 5.4886203]\n",
      "Reset environment\n",
      "Episode reward: 1663.6441700458527\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.500515  5.483567  5.4707694 3.997235  5.320196  5.490787 ]\n",
      "Reset environment\n",
      "Episode reward: 6317.874016225338\n",
      "Total Steps: 217\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5097017 5.492735  5.479967  4.0071406 5.328518  5.499974 ]\n",
      "Reset environment\n",
      "Episode reward: 5056.969982504845\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.517166  5.500222  5.487408  4.0152173 5.3352385 5.507427 ]\n",
      "Reset environment\n",
      "Episode reward: 3980.5945624113083\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.522797  5.5058155 5.4930663 4.021424  5.340286  5.5130544]\n",
      "Reset environment\n",
      "Episode reward: 4003.808368086815\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.528546  5.511621  5.4987392 4.0277147 5.3453765 5.5187964]\n",
      "Reset environment\n",
      "Episode reward: 1345.0681334137917\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5312347 5.5143247 5.5014224 4.030739  5.3477554 5.5214887]\n",
      "Reset environment\n",
      "Episode reward: 3673.6649649739265\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5362864 5.519396  5.506455  4.0362887 5.352281  5.5265355]\n",
      "Reset environment\n",
      "Episode reward: 568.7270547151566\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.536648  5.5194764 5.5071115 4.037178  5.3524485 5.526899 ]\n",
      "Reset environment\n",
      "Episode reward: -422.30067604780197\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5351105 5.517703  5.5058208 4.035881  5.3509345 5.5253625]\n",
      "Reset environment\n",
      "Episode reward: 3218.9551715254784\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5394607 5.52205   5.510174  4.0406876 5.354779  5.529712 ]\n",
      "Reset environment\n",
      "Episode reward: 2556.826420545578\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.543012  5.5255666 5.513755  4.044609  5.3579335 5.5332575]\n",
      "Reset environment\n",
      "Episode reward: 197.07579165697098\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.543054  5.5258055 5.5135975 4.0451016 5.357831  5.533302 ]\n",
      "Reset environment\n",
      "Episode reward: 4264.935477733612\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5492287 5.5320344 5.519704  4.0518303 5.3633313 5.5394697]\n",
      "Reset environment\n",
      "Episode reward: 1399.6636123657227\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5520334 5.5348325 5.522519  4.054927  5.365808  5.542271 ]\n",
      "Reset environment\n",
      "Episode reward: 2482.814380168915\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5553036 5.5381794 5.5257006 4.0586257 5.368713  5.545538 ]\n",
      "Reset environment\n",
      "Episode reward: 1365.190543204546\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.557557  5.540672  5.5276976 4.0613055 5.370537  5.547797 ]\n",
      "Reset environment\n",
      "Episode reward: 3677.39994174242\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5627265 5.545821  5.532886  4.0669923 5.375133  5.552965 ]\n",
      "Reset environment\n",
      "Episode reward: 1368.042824268341\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.565472  5.5485826 5.535614  4.070058  5.3775573 5.555708 ]\n",
      "Reset environment\n",
      "Episode reward: 5634.111466109753\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.573775  5.556895  5.543927  4.0790553 5.3850675 5.564014 ]\n",
      "Reset environment\n",
      "Episode reward: 2724.8782698512077\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5773168 5.560455  5.547454  4.082988  5.3882065 5.5675535]\n",
      "Reset environment\n",
      "Episode reward: -315.9327461672947\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5761    5.5589204 5.546555  4.082259  5.3868318 5.566334 ]\n",
      "Reset environment\n",
      "Episode reward: 1243.7617896795273\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.578603  5.561412  5.5490627 4.085104  5.389028  5.568833 ]\n",
      "Reset environment\n",
      "Episode reward: 1950.9986171722412\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.581194  5.564055  5.5515957 4.0880103 5.391339  5.571418 ]\n",
      "Reset environment\n",
      "Episode reward: 3711.418196976185\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.586431  5.569286  5.5568438 4.093749  5.396039  5.576653 ]\n",
      "Reset environment\n",
      "Episode reward: 2137.037795841694\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5892262 5.572061  5.559645  4.096891  5.398483  5.579446 ]\n",
      "Reset environment\n",
      "Episode reward: 24.627898812294006\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5889087 5.571644  5.5594335 4.0962915 5.3983817 5.57913  ]\n",
      "Reset environment\n",
      "Episode reward: 1674.9168343544006\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.5910087 5.5736976 5.5615706 4.09863   5.4001937 5.5812287]\n",
      "Reset environment\n",
      "Episode reward: 4588.454605817795\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.597644  5.5803547 5.5681825 4.105884  5.406137  5.5878596]\n",
      "Reset environment\n",
      "Episode reward: 1378.1256835460663\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.600384  5.5830975 5.5709324 4.108906  5.408573  5.5906014]\n",
      "Reset environment\n",
      "Episode reward: 1398.5253578722477\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6023927 5.5853    5.572716  4.111241  5.4101496 5.5926094]\n",
      "Reset environment\n",
      "Episode reward: 1285.6535939872265\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6039696 5.5870886 5.574057  4.11322   5.4114027 5.5941873]\n",
      "Reset environment\n",
      "Episode reward: 6363.215363144875\n",
      "Total Steps: 227\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6130075 5.596156  5.583069  4.123131  5.419427  5.6032195]\n",
      "Reset environment\n",
      "Episode reward: 3993.2462127804756\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6185155 5.6016665 5.5885773 4.129131  5.424345  5.608729 ]\n",
      "Reset environment\n",
      "Episode reward: 3672.4363276660442\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6234765 5.606451  5.593679  4.134708  5.4285855 5.6136785]\n",
      "Reset environment\n",
      "Episode reward: -513.2740239202976\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6222305 5.605066  5.592585  4.1332026 5.42761   5.6124344]\n",
      "Reset environment\n",
      "Episode reward: 2163.314491689205\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6251106 5.6079206 5.5954924 4.1364117 5.43016   5.6153164]\n",
      "Reset environment\n",
      "Episode reward: 6377.385220468044\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6344204 5.617231  5.6048074 4.146478  5.438564  5.624625 ]\n",
      "Reset environment\n",
      "Episode reward: 2002.0764519833028\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.63752   5.620541  5.6076937 4.15003   5.4412723 5.6277275]\n",
      "Reset environment\n",
      "Episode reward: 2127.420405268669\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6413517 5.624411  5.6114855 4.154302  5.4446635 5.6315517]\n",
      "Reset environment\n",
      "Episode reward: 4364.3531076312065\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6474066 5.630604  5.6173816 4.1609645 5.450097  5.637597 ]\n",
      "Reset environment\n",
      "Episode reward: 837.8724111318588\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.64844   5.631833  5.6182294 4.1621995 5.4509892 5.638627 ]\n",
      "Reset environment\n",
      "Episode reward: 1962.8666855096817\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6510406 5.634454  5.6208158 4.165088  5.453313  5.6412244]\n",
      "Reset environment\n",
      "Episode reward: 2794.5006957650185\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.655719  5.638935  5.6256323 4.170311  5.457202  5.645888 ]\n",
      "Reset environment\n",
      "Episode reward: 3077.7068874388933\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6597505 5.6431136 5.6294847 4.1748657 5.4606194 5.6499166]\n",
      "Reset environment\n",
      "Episode reward: 3662.515808403492\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6647525 5.648128  5.6344857 4.1803174 5.465094  5.654916 ]\n",
      "Reset environment\n",
      "Episode reward: 5702.776500225067\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.67286   5.6562257 5.6425896 4.1891217 5.472345  5.6630216]\n",
      "Reset environment\n",
      "Episode reward: -209.98130059242249\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6719723 5.65521   5.6418543 4.1880503 5.4716988 5.662138 ]\n",
      "Reset environment\n",
      "Episode reward: 4511.646215260029\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.678388  5.6616387 5.6482654 4.1950493 5.477479  5.668555 ]\n",
      "Reset environment\n",
      "Episode reward: 2623.8069383502007\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6819096 5.6651134 5.651813  4.1989374 5.4806075 5.6720653]\n",
      "Reset environment\n",
      "Episode reward: 2398.7437866330147\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.684956  5.6681867 5.6548285 4.202353  5.483333  5.675105 ]\n",
      "Reset environment\n",
      "Episode reward: 2728.2957052588463\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6886325 5.671876  5.6584935 4.2064013 5.4866214 5.678779 ]\n",
      "Reset environment\n",
      "Episode reward: -62.293710112571716\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.6881394 5.671572  5.6578293 4.205975  5.486119  5.6782875]\n",
      "Reset environment\n",
      "Episode reward: 2602.677435398102\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.69146   5.6749086 5.661126  4.2096815 5.4890614 5.6816006]\n",
      "Reset environment\n",
      "Episode reward: 1373.3673321008682\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.694162  5.677637  5.663809  4.212689  5.491461  5.6843014]\n",
      "Reset environment\n",
      "Episode reward: 5041.04919975996\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.702487  5.685957  5.6721435 4.2216363 5.4989076 5.692622 ]\n",
      "Reset environment\n",
      "Episode reward: 2280.0284111499786\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.706554 5.690047 5.676197 4.22613  5.502549 5.696693]\n",
      "Reset environment\n",
      "Episode reward: 2818.6752029657364\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7101846 5.6936746 5.6798277 4.2301574 5.5057607 5.700321 ]\n",
      "Reset environment\n",
      "Episode reward: 2287.8770860135555\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.713188  5.6966376 5.68286   4.2334356 5.508418  5.70332  ]\n",
      "Reset environment\n",
      "Episode reward: 658.1347984671593\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.713851  5.6970205 5.6837983 4.234596  5.5088153 5.7039747]\n",
      "Reset environment\n",
      "Episode reward: 2661.7307863533497\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7174354 5.7005515 5.6874213 4.2385154 5.512011  5.7075562]\n",
      "Reset environment\n",
      "Episode reward: -404.47499181889\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7158914 5.6988273 5.686069  4.236936  5.5105824 5.7060094]\n",
      "Reset environment\n",
      "Episode reward: 3011.162391066551\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.720025  5.7030125 5.6901536 4.241508  5.514252  5.710146 ]\n",
      "Reset environment\n",
      "Episode reward: 2202.0066380500793\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.722825  5.7057633 5.6929803 4.244644  5.5166373 5.7129354]\n",
      "Reset environment\n",
      "Episode reward: 2042.8964117765427\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7265244 5.7094913 5.696654  4.2487707 5.5199075 5.716635 ]\n",
      "Reset environment\n",
      "Episode reward: 3290.4540568590164\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7310925 5.7140813 5.701184  4.2538004 5.5239573 5.721204 ]\n",
      "Reset environment\n",
      "Episode reward: 315.0607855319977\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7312922 5.71414   5.701522  4.253886  5.5242305 5.7214026]\n",
      "Reset environment\n",
      "Episode reward: 2118.401043832302\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7350726 5.7179227 5.705305  4.25807   5.5276084 5.7251863]\n",
      "Reset environment\n",
      "Episode reward: -691.2724351882935\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7335253 5.7163763 5.703775  4.256413  5.526183  5.7236447]\n",
      "Reset environment\n",
      "Episode reward: 5947.313898086548\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7421293 5.724964  5.7123876 4.26571   5.5339317 5.732255 ]\n",
      "Reset environment\n",
      "Episode reward: 2529.573891058564\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.746141  5.7291255 5.7162256 4.2702527 5.537353  5.736268 ]\n",
      "Reset environment\n",
      "Episode reward: 2864.73033285141\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7500367 5.7330465 5.7200794 4.2745743 5.540781  5.7401595]\n",
      "Reset environment\n",
      "Episode reward: 1079.04993891716\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7511463 5.733893  5.7214622 4.2763076 5.541515  5.7412715]\n",
      "Reset environment\n",
      "Episode reward: 2726.37713457644\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.755607  5.7385645 5.7256756 4.2813644 5.5450726 5.745744 ]\n",
      "Reset environment\n",
      "Episode reward: 2734.282610476017\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7592936 5.742171  5.729415  4.285383  5.5483217 5.7494254]\n",
      "Reset environment\n",
      "Episode reward: 4005.0095441937447\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7653112 5.74827   5.7353644 4.292147  5.553823  5.7554564]\n",
      "Reset environment\n",
      "Episode reward: 1986.9982183277607\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.767839  5.750746  5.73793   4.294917  5.556042  5.7579823]\n",
      "Reset environment\n",
      "Episode reward: 1768.488474741578\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.770327  5.753453  5.7401896 4.2978625 5.558008  5.7604785]\n",
      "Reset environment\n",
      "Episode reward: 4309.425958126783\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.777156  5.7602735 5.7470245 4.3054867 5.564168  5.767306 ]\n",
      "Reset environment\n",
      "Episode reward: 4766.066065490246\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7849274 5.768042  5.754769  4.3139877 5.5711823 5.7750635]\n",
      "Reset environment\n",
      "Episode reward: 1773.2607227563858\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.7875443 5.770423  5.757603  4.3170424 5.5733576 5.777671 ]\n",
      "Reset environment\n",
      "Episode reward: 4482.013971209526\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.793646  5.776518  5.7637134 4.323683  5.5788064 5.7837734]\n",
      "Reset environment\n",
      "Episode reward: 2070.0301528424025\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.796603  5.7796707 5.766439  4.3271403 5.5810757 5.7867446]\n",
      "Reset environment\n",
      "Episode reward: 5217.520976185799\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.804043  5.787099  5.7739124 4.3352065 5.587808  5.794201 ]\n",
      "Reset environment\n",
      "Episode reward: 5468.891635417938\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8118873 5.7949457 5.781767  4.3436885 5.5948853 5.8020597]\n",
      "Reset environment\n",
      "Episode reward: 4425.316546201706\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.817976  5.8010035 5.787877  4.35041   5.6004186 5.808151 ]\n",
      "Reset environment\n",
      "Episode reward: 4333.278369188309\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8240438 5.807095  5.7939305 4.3570576 5.605858  5.8142247]\n",
      "Reset environment\n",
      "Episode reward: 6545.251170516014\n",
      "Total Steps: 224\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.833394  5.816467  5.803242  4.3671403 5.6142874 5.8235745]\n",
      "Reset environment\n",
      "Episode reward: -216.79816696047783\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8326483 5.8155985 5.8026276 4.366372  5.6136622 5.8228283]\n",
      "Reset environment\n",
      "Episode reward: 2456.5009495019913\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.835891  5.8188286 5.8058753 4.369962  5.616532  5.8260655]\n",
      "Reset environment\n",
      "Episode reward: 6131.894996523857\n",
      "Total Steps: 210\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.844717  5.827664  5.814666  4.3794684 5.624471  5.83488  ]\n",
      "Reset environment\n",
      "Episode reward: 2024.7223103046417\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8472714 5.8302574 5.8171806 4.3823743 5.626723  5.8374367]\n",
      "Reset environment\n",
      "Episode reward: 1870.078835427761\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8495865 5.8325496 5.8195    4.384976  5.628737  5.839741 ]\n",
      "Reset environment\n",
      "Episode reward: 4987.682948172092\n",
      "Total Steps: 229\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.85693   5.8400073 5.8267264 4.3930755 5.6354103 5.8470902]\n",
      "Reset environment\n",
      "Episode reward: 1355.4892426729202\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.859563  5.8426313 5.8293724 4.3959923 5.6377344 5.849724 ]\n",
      "Reset environment\n",
      "Episode reward: -584.9766190145165\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8582897 5.841512  5.827964  4.394781  5.636594  5.8484526]\n",
      "Reset environment\n",
      "Episode reward: 3125.9419769495726\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.862418  5.8455515 5.8321667 4.399342  5.640255  5.8525763]\n",
      "Reset environment\n",
      "Episode reward: 2357.50394487381\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8664804 5.8496375 5.8362107 4.403864  5.6438637 5.8566394]\n",
      "Reset environment\n",
      "Episode reward: 1347.46791523695\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8690953 5.852236  5.8388357 4.406769  5.6461616 5.859251 ]\n",
      "Reset environment\n",
      "Episode reward: 2023.1901569366455\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.872708  5.855859  5.8424506 4.410742  5.649391  5.862871 ]\n",
      "Reset environment\n",
      "Episode reward: 1886.925717562437\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8750825 5.858198  5.8448577 4.413365  5.651488  5.865246 ]\n",
      "Reset environment\n",
      "Episode reward: 2614.540758907795\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8786182 5.861763  5.848356  4.4172406 5.654643  5.8687787]\n",
      "Reset environment\n",
      "Episode reward: 4512.946967303753\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.884932  5.8681417 5.854607  4.4241223 5.66031   5.875096 ]\n",
      "Reset environment\n",
      "Episode reward: 4369.7722871899605\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8909426 5.874246  5.8605056 4.430734  5.665667  5.881108 ]\n",
      "Reset environment\n",
      "Episode reward: 2141.0398491621017\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8936644 5.8769407 5.863253  4.4337597 5.6680694 5.883832 ]\n",
      "Reset environment\n",
      "Episode reward: 3478.4448414072394\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.8993955 5.882802  5.868819  4.4400535 5.673079  5.8895574]\n",
      "Reset environment\n",
      "Episode reward: 5463.9397567510605\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9071126 5.8905754 5.876464  4.4484453 5.6799955 5.8972716]\n",
      "Reset environment\n",
      "Episode reward: 2109.207999572158\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.910104  5.8937244 5.8792505 4.452042  5.682239  5.900266 ]\n",
      "Reset environment\n",
      "Episode reward: 1376.5822466015816\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9127507 5.896361  5.8819046 4.454979  5.684574  5.9029117]\n",
      "Reset environment\n",
      "Episode reward: -143.15239489078522\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9122114 5.8957543 5.881426  4.4541497 5.6842947 5.902366 ]\n",
      "Reset environment\n",
      "Episode reward: 726.1139576435089\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9128733 5.896629  5.881899  4.4549623 5.6848407 5.903031 ]\n",
      "Reset environment\n",
      "Episode reward: 5278.950345516205\n",
      "Total Steps: 207\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.921223  5.904992  5.890214  4.4641147 5.6923933 5.911378 ]\n",
      "Reset environment\n",
      "Episode reward: 3709.89061832428\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.926066  5.9099193 5.8949547 4.4695315 5.696656  5.9162135]\n",
      "Reset environment\n",
      "Episode reward: 3771.3098335266113\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9311895 5.915014  5.9001083 4.475113  5.701277  5.9213433]\n",
      "Reset environment\n",
      "Episode reward: 1374.0048537109978\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9332485 5.9169044 5.90232   4.47733   5.703021  5.923408 ]\n",
      "Reset environment\n",
      "Episode reward: 1270.068308506161\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9349103 5.9187765 5.903782  4.479255  5.7044873 5.925074 ]\n",
      "Reset environment\n",
      "Episode reward: 5363.52665412426\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9424186 5.9263263 5.911241  4.487409  5.711252  5.932588 ]\n",
      "Reset environment\n",
      "Episode reward: 2254.121952533722\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.94523   5.9291863 5.914003  4.49059   5.713715  5.9354005]\n",
      "Reset environment\n",
      "Episode reward: 6275.429879426956\n",
      "Total Steps: 215\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9539165 5.937903  5.922672  4.500001  5.721525  5.944098 ]\n",
      "Reset environment\n",
      "Episode reward: 1318.85118830204\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.956445  5.9404225 5.9252143 4.502828  5.7237515 5.946634 ]\n",
      "Reset environment\n",
      "Episode reward: 4448.7938650250435\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.962608  5.9465795 5.931378  4.509567  5.7292423 5.952809 ]\n",
      "Reset environment\n",
      "Episode reward: 624.8709321022034\n",
      "Total Steps: 19\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.963133  5.9471045 5.9319034 4.5101995 5.7296824 5.9533343]\n",
      "Reset environment\n",
      "Episode reward: 533.366626739502\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.963497  5.947244  5.93249   4.5108466 5.7297864 5.953701 ]\n",
      "Reset environment\n",
      "Episode reward: 1316.9362580776215\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.966018  5.9497805 5.934992  4.5136666 5.732017  5.9562216]\n",
      "Reset environment\n",
      "Episode reward: 4744.769196152687\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9734087 5.9571996 5.942335  4.5218477 5.7386794 5.963618 ]\n",
      "Reset environment\n",
      "Episode reward: -199.52851817011833\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.972657  5.9562855 5.941762  4.521269  5.737823  5.962868 ]\n",
      "Reset environment\n",
      "Episode reward: 1715.8564584851265\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9747047 5.958308  5.9438267 4.523582  5.7396245 5.9649186]\n",
      "Reset environment\n",
      "Episode reward: 2072.247971713543\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9783373 5.9619155 5.9474807 4.527603  5.742837  5.9685483]\n",
      "Reset environment\n",
      "Episode reward: 1971.7054030299187\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.980788  5.9643283 5.9499507 4.5303025 5.7449965 5.9709964]\n",
      "Reset environment\n",
      "Episode reward: 2599.801353275776\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9842224 5.9677396 5.953399  4.5340657 5.74805   5.9744287]\n",
      "Reset environment\n",
      "Episode reward: 2315.7982109189034\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9881945 5.971739  5.9573345 4.5384407 5.7515984 5.9784026]\n",
      "Reset environment\n",
      "Episode reward: 2570.1197289526463\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.992364  5.9760575 5.961327  4.5430303 5.755077  5.9825764]\n",
      "Reset environment\n",
      "Episode reward: 3275.181460440159\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.996535  5.980212  5.9655113 4.547667  5.758757  5.986742 ]\n",
      "Reset environment\n",
      "Episode reward: 2551.2056437134743\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [5.9997387 5.9834256 5.968706  4.5511804 5.7616167 5.9899416]\n",
      "Reset environment\n",
      "Episode reward: 2130.108962059021\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0034323 5.987143  5.972397  4.5552654 5.7649193 5.993643 ]\n",
      "Reset environment\n",
      "Episode reward: 2441.4144223332405\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0064387 5.990162  5.9753985 4.558581  5.767589  5.996648 ]\n",
      "Reset environment\n",
      "Episode reward: 4322.79035204649\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.012328  5.996073  5.9812765 4.5650206 5.772888  6.002545 ]\n",
      "Reset environment\n",
      "Episode reward: 208.39999413490295\n",
      "Total Steps: 6\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0122747 5.9960217 5.9812207 4.5649796 5.7728343 6.002492 ]\n",
      "Reset environment\n",
      "Episode reward: 149.76085576415062\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.012429  5.9958787 5.9816494 4.565595  5.772603  6.002635 ]\n",
      "Reset environment\n",
      "Episode reward: 5917.289643168449\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0207186 6.004199  5.9899035 4.574554  5.7800827 6.0109363]\n",
      "Reset environment\n",
      "Episode reward: 6430.680663228035\n",
      "Total Steps: 227\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0295367 6.0130982 5.99865   4.5842094 5.7880654 6.0197673]\n",
      "Reset environment\n",
      "Episode reward: 2106.333181500435\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.03322   6.016768  6.0023503 4.588267  5.791335  6.0234513]\n",
      "Reset environment\n",
      "Episode reward: 2994.681504905224\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0371714 6.0206647 6.0063505 4.592621  5.794881  6.0274076]\n",
      "Reset environment\n",
      "Episode reward: 3546.0612463653088\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.041747  6.0251026 6.011023  4.5976768 5.7987027 6.031973 ]\n",
      "Reset environment\n",
      "Episode reward: 5598.025520265102\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0495505 6.03293   6.0188217 4.606128  5.8057303 6.0397863]\n",
      "Reset environment\n",
      "Episode reward: 3390.060751490295\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.054009  6.037295  6.023366  4.6110215 5.8096385 6.044248 ]\n",
      "Reset environment\n",
      "Episode reward: -695.4204624891281\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0523977 6.035683  6.021777  4.6091714 5.808214  6.042641 ]\n",
      "Reset environment\n",
      "Episode reward: 4183.858764886856\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.058141  6.0414147 6.027516  4.615384  5.813413  6.0483813]\n",
      "Reset environment\n",
      "Episode reward: 2263.9515741094947\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.06137   6.0448527 6.0305142 4.6191435 5.8159595 6.051636 ]\n",
      "Reset environment\n",
      "Episode reward: 1824.1395520791411\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.064051  6.047746  6.0329385 4.622231  5.818004  6.054331 ]\n",
      "Reset environment\n",
      "Episode reward: 1671.7218282744288\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0664053 6.0503078 6.03507   4.624963  5.8198495 6.0567   ]\n",
      "Reset environment\n",
      "Episode reward: 4970.775266110897\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0730824 6.0569854 6.041755  4.632189  5.825822  6.063375 ]\n",
      "Reset environment\n",
      "Episode reward: 2994.7419486884028\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0565267 6.0401945 6.025592  4.6066113 5.8118424 6.0468946]\n",
      "Reset environment\n",
      "Episode reward: 2120.0282726734877\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.059184  6.0428033 6.0282803 4.6095157 5.8141637 6.0495524]\n",
      "Reset environment\n",
      "Episode reward: 1233.7547799944878\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.061501  6.0451336 6.030598  4.6121473 5.8161902 6.0518746]\n",
      "Reset environment\n",
      "Episode reward: 2726.134339570999\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0655966 6.0494285 6.034448  4.616826  5.819467  6.0559783]\n",
      "Reset environment\n",
      "Episode reward: -618.5033769607544\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.063746  6.047813  6.032421  4.615026  5.8178453 6.054141 ]\n",
      "Reset environment\n",
      "Episode reward: 4066.8224222660065\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.069047  6.0531063 6.0377407 4.620844  5.822559  6.0594463]\n",
      "Reset environment\n",
      "Episode reward: 3434.3392308950424\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.073534  6.057546  6.042274  4.6257896 5.8265586 6.0639305]\n",
      "Reset environment\n",
      "Episode reward: 1242.7946997880936\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.075862  6.059886  6.0445957 4.628425  5.8285913 6.0662622]\n",
      "Reset environment\n",
      "Episode reward: 2460.542929112911\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.079009  6.063062  6.04771   4.6319113 5.831362  6.069414 ]\n",
      "Reset environment\n",
      "Episode reward: 1361.944727063179\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0815706 6.0656147 6.0502896 4.6347504 5.833627  6.0719748]\n",
      "Reset environment\n",
      "Episode reward: 1335.6531975269318\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0830307 6.0670037 6.051815  4.6364074 5.834899  6.0734334]\n",
      "Reset environment\n",
      "Episode reward: 2792.2256927788258\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.087471  6.0715847 6.0561    4.6413245 5.838814  6.0778756]\n",
      "Reset environment\n",
      "Episode reward: 383.5118266791105\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.087606  6.0714893 6.056482  4.6416326 5.8388147 6.0780225]\n",
      "Reset environment\n",
      "Episode reward: 2778.4478176236153\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.091041  6.074939  6.0599003 4.64545   5.841893  6.081458 ]\n",
      "Reset environment\n",
      "Episode reward: 1265.6098234057426\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.092431  6.076328  6.0612926 4.6470575 5.843081  6.082852 ]\n",
      "Reset environment\n",
      "Episode reward: 3487.477889765054\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0966797 6.0805025 6.065599  4.6517644 5.8468437 6.0870953]\n",
      "Reset environment\n",
      "Episode reward: 1644.0200700759888\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0986834 6.082538  6.067564  4.654002  5.8486285 6.089094 ]\n",
      "Reset environment\n",
      "Episode reward: 1071.5936599373817\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.0998573 6.0839534 6.068501  4.6554065 5.8495975 6.090277 ]\n",
      "Reset environment\n",
      "Episode reward: 1480.059569656849\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.101595  6.085651  6.0702662 4.657318  5.8511405 6.092014 ]\n",
      "Reset environment\n",
      "Episode reward: 2712.9889968037605\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1049314 6.089009  6.0735793 4.661003  5.8540916 6.0953507]\n",
      "Reset environment\n",
      "Episode reward: 208.704056173563\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.104668  6.0888863 6.073195  4.6609783 5.853938  6.095098 ]\n",
      "Reset environment\n",
      "Episode reward: 3528.6709609031677\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1093354 6.0935082 6.077901  4.666092  5.858163  6.0997667]\n",
      "Reset environment\n",
      "Episode reward: 2670.909001611173\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1132264 6.0971613 6.08199   4.670587  5.861293  6.103653 ]\n",
      "Reset environment\n",
      "Episode reward: 2693.8920161128044\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.116559  6.100492  6.0853276 4.6742315 5.8642507 6.106982 ]\n",
      "Reset environment\n",
      "Episode reward: 3796.5389260947704\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1215787 6.10535   6.090483  4.6797795 5.868605  6.112004 ]\n",
      "Reset environment\n",
      "Episode reward: 2402.7633464336395\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.124639  6.108397  6.0935545 4.6831455 5.87132   6.1150656]\n",
      "Reset environment\n",
      "Episode reward: 2786.4379649162292\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.128009  6.111605  6.097063  4.6869044 5.874292  6.1184287]\n",
      "Reset environment\n",
      "Episode reward: 2032.6339061409235\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1309657 6.114357  6.100203  4.690301  5.876832  6.121384 ]\n",
      "Reset environment\n",
      "Episode reward: 1940.9869632720947\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.13383   6.117353  6.10293   4.6935186 5.879411  6.1242385]\n",
      "Reset environment\n",
      "Episode reward: 2659.706539809704\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1371145 6.1206517 6.106195  4.6971107 5.8823533 6.1275163]\n",
      "Reset environment\n",
      "Episode reward: 1335.362970650196\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1396093 6.1231475 6.1086826 4.6998982 5.884541  6.1300063]\n",
      "Reset environment\n",
      "Episode reward: 2906.732011884451\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.143357  6.1268454 6.1124673 4.70402   5.8878446 6.133753 ]\n",
      "Reset environment\n",
      "Episode reward: 2657.3833537995815\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1475296 6.131187  6.1164184 4.708768  5.89124   6.1379247]\n",
      "Reset environment\n",
      "Episode reward: 2594.3252732157707\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.150828  6.134512  6.1196923 4.712434  5.8941736 6.141226 ]\n",
      "Reset environment\n",
      "Episode reward: 4868.076991915703\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.15748   6.141214  6.1262646 4.719634  5.9001594 6.1478763]\n",
      "Reset environment\n",
      "Episode reward: 2856.7315632402897\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.160964  6.1448307 6.12958   4.7235684 5.9031253 6.1513615]\n",
      "Reset environment\n",
      "Episode reward: 4689.446598529816\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.167736  6.151747  6.136216  4.731015  5.9093623 6.158152 ]\n",
      "Reset environment\n",
      "Episode reward: 3036.2565834224224\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1716247 6.1555977 6.14014   4.735283  5.9128475 6.1620383]\n",
      "Reset environment\n",
      "Episode reward: 4220.4369396567345\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1773114 6.161335  6.14577   4.741472  5.9179244 6.167728 ]\n",
      "Reset environment\n",
      "Episode reward: 5793.861151456833\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1851907 6.169241  6.1536055 4.749988  5.9250245 6.1756124]\n",
      "Reset environment\n",
      "Episode reward: 4468.247781604528\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.191123  6.1752667 6.159432  4.7564673 5.930389  6.181543 ]\n",
      "Reset environment\n",
      "Episode reward: 3637.7942467331886\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1958823 6.18004   6.164189  4.7616878 5.9346623 6.1863055]\n",
      "Reset environment\n",
      "Episode reward: 2546.9544512033463\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.1991377 6.1832685 6.16747   4.765225  5.9375935 6.189563 ]\n",
      "Reset environment\n",
      "Episode reward: 4970.659689426422\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2058835 6.1900015 6.1742196 4.772579  5.943628  6.196312 ]\n",
      "Reset environment\n",
      "Episode reward: 5971.8998609781265\n",
      "Total Steps: 205\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.213848  6.1979747 6.182185  4.7811823 5.950864  6.204278 ]\n",
      "Reset environment\n",
      "Episode reward: 3241.377143740654\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.217857  6.201984  6.186193  4.785642  5.954392  6.2082906]\n",
      "Reset environment\n",
      "Episode reward: 1763.427550509572\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2203035 6.204613  6.1884494 4.7884464 5.956545  6.2107477]\n",
      "Reset environment\n",
      "Episode reward: 6249.656212866306\n",
      "Total Steps: 213\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2288685 6.2131763 6.1970024 4.797666  5.96427   6.21932  ]\n",
      "Reset environment\n",
      "Episode reward: 1619.2351421415806\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.231278  6.2155128 6.199473  4.8002644 5.9664392 6.2217283]\n",
      "Reset environment\n",
      "Episode reward: 2203.3195944726467\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2340136 6.2181945 6.2022495 4.803252  5.9688654 6.2244573]\n",
      "Reset environment\n",
      "Episode reward: 1604.4311067163944\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.236101  6.2200475 6.204554  4.8057566 5.970514  6.226545 ]\n",
      "Reset environment\n",
      "Episode reward: 4533.381723105907\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2420044 6.2259407 6.210464  4.812185  5.9757833 6.2324386]\n",
      "Reset environment\n",
      "Episode reward: 1985.3105124235153\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2444496 6.2283664 6.212923  4.8149014 5.9779525 6.2348814]\n",
      "Reset environment\n",
      "Episode reward: 3229.1153388619423\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.248659  6.232574  6.217131  4.8194957 5.981743  6.239085 ]\n",
      "Reset environment\n",
      "Episode reward: 1324.6910971999168\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.250096  6.2340055 6.2185655 4.821152  5.9829645 6.2405176]\n",
      "Reset environment\n",
      "Episode reward: 2073.3026839494705\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2536206 6.2375536 6.222067  4.8250475 5.9861093 6.2440414]\n",
      "Reset environment\n",
      "Episode reward: 2240.968956530094\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.257322  6.2412534 6.2257757 4.8291707 5.9893928 6.2477484]\n",
      "Reset environment\n",
      "Episode reward: 3270.458344876766\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.261528  6.24543   6.229999  4.8337765 5.9931602 6.251952 ]\n",
      "Reset environment\n",
      "Episode reward: 3471.3919466733932\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.265881  6.249794  6.2343473 4.8385577 5.997031  6.2563057]\n",
      "Reset environment\n",
      "Episode reward: 1599.7325247302651\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.267772  6.2514653 6.236444  4.8409567 5.998422  6.2582006]\n",
      "Reset environment\n",
      "Episode reward: 2214.1518670916557\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.271498 6.255205 6.240164 4.845024 6.001774 6.261928]\n",
      "Reset environment\n",
      "Episode reward: 2694.10290902853\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.274978  6.2587104 6.2436056 4.8488464 6.004881  6.2654037]\n",
      "Reset environment\n",
      "Episode reward: 5942.034504413605\n",
      "Total Steps: 207\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.282968  6.26671   6.251565  4.857502  6.0120716 6.273389 ]\n",
      "Reset environment\n",
      "Episode reward: 4399.168710291386\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2888145 6.2726207 6.257334  4.863895  6.017293  6.2792354]\n",
      "Reset environment\n",
      "Episode reward: 1319.812268435955\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.2912397 6.275019  6.2597823 4.8666162 6.0194144 6.2816586]\n",
      "Reset environment\n",
      "Episode reward: 3309.3038608580828\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.295375  6.2790346 6.264022  4.8711996 6.0230594 6.285794 ]\n",
      "Reset environment\n",
      "Episode reward: 1309.3832352161407\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.29778   6.2814584 6.266412  4.873897  6.025181  6.2881975]\n",
      "Reset environment\n",
      "Episode reward: 2122.722455263138\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3004074 6.2840366 6.26907   4.876764  6.027513  6.29082  ]\n",
      "Reset environment\n",
      "Episode reward: 2326.0518077015877\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3031073 6.286727  6.271777  4.8797665 6.029877  6.2935157]\n",
      "Reset environment\n",
      "Episode reward: 2640.509697318077\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3062944 6.2899265 6.2749405 4.883276  6.032723  6.2966948]\n",
      "Reset environment\n",
      "Episode reward: 5014.9403555989265\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3130183 6.2966466 6.2816706 4.890548  6.0388064 6.303421 ]\n",
      "Reset environment\n",
      "Episode reward: 2699.8749220967293\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.317023  6.3004575 6.285819  4.8949656 6.042227  6.3074126]\n",
      "Reset environment\n",
      "Episode reward: 1786.6938011795282\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.31946   6.302689  6.2884464 4.8977203 6.044303  6.3098574]\n",
      "Reset environment\n",
      "Episode reward: 1711.3770514726639\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3215375 6.304761  6.29053   4.9000216 6.046144  6.3119345]\n",
      "Reset environment\n",
      "Episode reward: 4448.313092380762\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.327447  6.3107266 6.296366  4.9065003 6.051367  6.317839 ]\n",
      "Reset environment\n",
      "Episode reward: 3152.17296987772\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3314962 6.3147597 6.3004265 4.9109063 6.055008  6.321882 ]\n",
      "Reset environment\n",
      "Episode reward: 2309.862453877926\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.334366  6.317682  6.303242  4.9140906 6.057568  6.3247495]\n",
      "Reset environment\n",
      "Episode reward: 2147.199513733387\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3369    6.320206  6.3057895 4.916939  6.0597963 6.32729  ]\n",
      "Reset environment\n",
      "Episode reward: 2130.471681714058\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.339525  6.3228154 6.3084254 4.9198136 6.0621576 6.3299146]\n",
      "Reset environment\n",
      "Episode reward: 4650.234467923641\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.345686  6.3289814 6.314582  4.926479  6.0677514 6.336082 ]\n",
      "Reset environment\n",
      "Episode reward: 2845.2660457491875\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3491297 6.3324413 6.31802   4.930284  6.070837  6.3395233]\n",
      "Reset environment\n",
      "Episode reward: 5321.315884590149\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.357228  6.3405557 6.3261375 4.9389796 6.0781474 6.3476396]\n",
      "Reset environment\n",
      "Episode reward: 2095.5978611707687\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3607755 6.344097  6.3296976 4.942856  6.0813265 6.3511925]\n",
      "Reset environment\n",
      "Episode reward: 5156.5759028196335\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.367653  6.350972  6.3365884 4.9503202 6.0875263 6.358076 ]\n",
      "Reset environment\n",
      "Episode reward: -688.0193483829498\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3660607 6.3493814 6.3350005 4.948702  6.086035  6.356488 ]\n",
      "Reset environment\n",
      "Episode reward: 2403.3320626169443\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3694015 6.3529024 6.3381686 4.952476  6.089043  6.359837 ]\n",
      "Reset environment\n",
      "Episode reward: 5250.839414298534\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3764296 6.3599453 6.3451924 4.9601336 6.095354  6.366875 ]\n",
      "Reset environment\n",
      "Episode reward: 4707.385865807533\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.382659  6.3661737 6.3514123 4.9669    6.100969  6.3731084]\n",
      "Reset environment\n",
      "Episode reward: 2464.4258986115456\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.385678  6.3691516 6.3544645 4.9702272 6.1036687 6.376127 ]\n",
      "Reset environment\n",
      "Episode reward: 4537.873090147972\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3916764 6.375154  6.3604503 4.976682  6.109106  6.3821244]\n",
      "Reset environment\n",
      "Episode reward: 1206.106321811676\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3928394 6.376038  6.3618956 4.978342  6.109981  6.383295 ]\n",
      "Reset environment\n",
      "Episode reward: 4072.773574233055\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.3981667 6.381365  6.3672304 4.9841104 6.1148024 6.388624 ]\n",
      "Reset environment\n",
      "Episode reward: 1903.6454299241304\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.400966  6.384329  6.3698187 4.9872527 6.117091  6.391421 ]\n",
      "Reset environment\n",
      "Episode reward: 4344.005651533604\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.406703  6.3900595 6.3755474 4.993496  6.122231  6.397162 ]\n",
      "Reset environment\n",
      "Episode reward: 4493.536608159542\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.412668  6.3960176 6.3815217 4.9999647 6.1275907 6.403134 ]\n",
      "Reset environment\n",
      "Episode reward: 1517.8362935185432\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4143887 6.397713  6.3832626 5.001864  6.1291246 6.404855 ]\n",
      "Reset environment\n",
      "Episode reward: 1350.794880092144\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4168415 6.4001794 6.385707  5.0045886 6.1312876 6.407307 ]\n",
      "Reset environment\n",
      "Episode reward: 2064.397380232811\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4193673 6.402737  6.3882017 5.0073953 6.133555  6.4098315]\n",
      "Reset environment\n",
      "Episode reward: 2261.6606597676873\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4221134 6.40544   6.390979  5.01039   6.1359386 6.4125795]\n",
      "Reset environment\n",
      "Episode reward: 4138.839152522385\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.427404  6.4107995 6.396182  5.016164  6.14072   6.417864 ]\n",
      "Reset environment\n",
      "Episode reward: 2985.161933362484\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.431181 6.41459  6.399942 5.020324 6.144083 6.421635]\n",
      "Reset environment\n",
      "Episode reward: 319.74762649834156\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.430694  6.413858  6.3997197 5.020235  6.1434712 6.4211545]\n",
      "Reset environment\n",
      "Episode reward: 2072.9706379175186\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.434141  6.4173064 6.4031696 5.02404   6.146541  6.4246025]\n",
      "Reset environment\n",
      "Episode reward: 2371.8083644509315\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4368267 6.419945  6.4058948 5.0270247 6.1489024 6.4272866]\n",
      "Reset environment\n",
      "Episode reward: 1376.361176252365\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4393063 6.422417  6.4083815 5.0297728 6.1510835 6.429769 ]\n",
      "Reset environment\n",
      "Episode reward: 185.8637889623642\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4389563 6.4223084 6.4078193 5.0295863 6.1506486 6.4294333]\n",
      "Reset environment\n",
      "Episode reward: 1929.315508544445\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4412227 6.424516  6.4101315 5.032084  6.152603  6.4316964]\n",
      "Reset environment\n",
      "Episode reward: 3592.6405999064445\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4458423 6.4291697 6.4147177 5.037162  6.1567106 6.4363203]\n",
      "Reset environment\n",
      "Episode reward: 5583.920227766037\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.454258  6.437586  6.4231133 5.0462284 6.164293  6.444736 ]\n",
      "Reset environment\n",
      "Episode reward: -736.2863374948502\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.452733  6.4358635 6.421803  5.044737  6.162929  6.44322  ]\n",
      "Reset environment\n",
      "Episode reward: 4468.808492228389\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4584904 6.441704  6.427458  5.0510116 6.1681347 6.448979 ]\n",
      "Reset environment\n",
      "Episode reward: 1987.6609362363815\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4613595 6.4446816 6.430213  5.0542    6.1707225 6.451845 ]\n",
      "Reset environment\n",
      "Episode reward: 5504.416743755341\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.468501  6.4518223 6.4373574 5.061971  6.1771016 6.458986 ]\n",
      "Reset environment\n",
      "Episode reward: 4762.691109061241\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.474522 6.457847 6.443384 5.068529 6.182496 6.465012]\n",
      "Reset environment\n",
      "Episode reward: 4612.439647138119\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4803667 6.4637003 6.4492226 5.074888  6.1877103 6.4708543]\n",
      "Reset environment\n",
      "Episode reward: 4494.874105751514\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4862    6.469545  6.4550395 5.0811863 6.192983  6.4766903]\n",
      "Reset environment\n",
      "Episode reward: 4883.125648915768\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.492613  6.47602   6.4613614 5.088169  6.198724  6.4831047]\n",
      "Reset environment\n",
      "Episode reward: 1583.3032799065113\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4945793 6.47779   6.463508  5.090484  6.2004066 6.48507  ]\n",
      "Reset environment\n",
      "Episode reward: 3834.7152854800224\n",
      "Total Steps: 994\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.489489  6.472885  6.4583206 5.0817394 6.196159  6.480018 ]\n",
      "Reset environment\n",
      "Episode reward: 3897.2283148169518\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4945135 6.4778996 6.4633436 5.0872126 6.20066   6.4850364]\n",
      "Reset environment\n",
      "Episode reward: 1959.5747581124306\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.497193  6.480332  6.466243  5.090339  6.2029057 6.487726 ]\n",
      "Reset environment\n",
      "Episode reward: 1348.442943394184\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.4996047 6.4827476 6.4686456 5.09303   6.2050304 6.490135 ]\n",
      "Reset environment\n",
      "Episode reward: 3640.3260332345963\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5043807 6.4875774 6.473353  5.098214  6.2093196 6.4949036]\n",
      "Reset environment\n",
      "Episode reward: 2132.547468394041\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.506891  6.4900317 6.4759107 5.100968  6.2115397 6.4974146]\n",
      "Reset environment\n",
      "Episode reward: 4624.179921030998\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5129433 6.496062  6.4819756 5.1074815 6.216988  6.5034776]\n",
      "Reset environment\n",
      "Episode reward: 6028.142448961735\n",
      "Total Steps: 207\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5209723 6.5041375 6.4899397 5.1161675 6.224218  6.511504 ]\n",
      "Reset environment\n",
      "Episode reward: 1717.8949673175812\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5231996 6.5061355 6.492365  5.1187954 6.226037  6.5137258]\n",
      "Reset environment\n",
      "Episode reward: 4444.94536614418\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5289555 6.5118866 6.498127  5.125029  6.2312407 6.5194902]\n",
      "Reset environment\n",
      "Episode reward: 2361.542759716511\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5316973 6.514652  6.500838  5.1280704 6.233691  6.5222287]\n",
      "Reset environment\n",
      "Episode reward: 3172.5240055322647\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.535652  6.5186334 6.504753  5.132462  6.237189  6.5261803]\n",
      "Reset environment\n",
      "Episode reward: 5105.665876746178\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.54233   6.5252886 6.5114446 5.1396976 6.24322   6.5328646]\n",
      "Reset environment\n",
      "Episode reward: -689.9717264175415\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5407243 6.523688  6.5098443 5.1380796 6.241688  6.531265 ]\n",
      "Reset environment\n",
      "Episode reward: 2102.892732858658\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5441713 6.527133  6.5132957 5.1419005 6.244748  6.534707 ]\n",
      "Reset environment\n",
      "Episode reward: 4857.6475039720535\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.550472  6.533522  6.519498  5.1487775 6.2504244 6.54101  ]\n",
      "Reset environment\n",
      "Episode reward: 2715.8887308835983\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.553745  6.536776  6.5227947 5.1523957 6.253346  6.544288 ]\n",
      "Reset environment\n",
      "Episode reward: 4930.424617469311\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.560228  6.5432587 6.5292645 5.1593976 6.259193  6.550765 ]\n",
      "Reset environment\n",
      "Episode reward: 1314.787659227848\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5625534 6.5455756 6.5315995 5.1620355 6.261221  6.5530934]\n",
      "Reset environment\n",
      "Episode reward: 2905.6926557421684\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.566011  6.5490475 6.535048  5.165844  6.264304  6.556552 ]\n",
      "Reset environment\n",
      "Episode reward: 4715.002491235733\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.572178  6.555211  6.5412087 5.1725206 6.269833  6.5627217]\n",
      "Reset environment\n",
      "Episode reward: 910.8454539179802\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.57308   6.5562997 6.541919  5.1735168 6.27055   6.563627 ]\n",
      "Reset environment\n",
      "Episode reward: 3022.74500798434\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5764375 6.5595603 6.545373  5.1772447 6.273494  6.5669866]\n",
      "Reset environment\n",
      "Episode reward: 2424.77145627141\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5798845 6.563147  6.5486774 5.181075  6.276584  6.5704374]\n",
      "Reset environment\n",
      "Episode reward: 3606.2245614528656\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.58438   6.5676856 6.5531282 5.1860676 6.2805486 6.5749373]\n",
      "Reset environment\n",
      "Episode reward: 2018.5637567043304\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.586774  6.570075  6.5555243 5.1887374 6.2826667 6.5773315]\n",
      "Reset environment\n",
      "Episode reward: 5358.377562403679\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.59381   6.5770893 6.562581  5.1963224 6.288955  6.5843806]\n",
      "Reset environment\n",
      "Episode reward: 4083.0555012226105\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.5990415 6.5822935 6.567817  5.202043  6.2936234 6.589611 ]\n",
      "Reset environment\n",
      "Episode reward: 3309.12916213274\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6031966 6.5864534 6.57196   5.2066035 6.2973313 6.5937624]\n",
      "Reset environment\n",
      "Episode reward: 5015.373471498489\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6097226 6.5930014 6.5784373 5.2136784 6.303187  6.600291 ]\n",
      "Reset environment\n",
      "Episode reward: 4386.357159078121\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6153765 6.59866   6.5840745 5.2198052 6.30827   6.6059446]\n",
      "Reset environment\n",
      "Episode reward: 1375.5582602620125\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.617814  6.6010876 6.58653   5.222498  6.3104267 6.608387 ]\n",
      "Reset environment\n",
      "Episode reward: 1291.838718175888\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6201034 6.6033754 6.5888143 5.2250724 6.312431  6.610673 ]\n",
      "Reset environment\n",
      "Episode reward: -346.72363662719727\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6191015 6.602384  6.5878224 5.2238884 6.3114486 6.609682 ]\n",
      "Reset environment\n",
      "Episode reward: 1999.93573269248\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.621772  6.6051927 6.590347  5.226921  6.31384   6.61235  ]\n",
      "Reset environment\n",
      "Episode reward: 3670.724523730576\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6263733 6.6097174 6.5950007 5.231945  6.317936  6.6169443]\n",
      "Reset environment\n",
      "Episode reward: 4824.144996702671\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6325927 6.615919  6.6012325 5.238667  6.3235703 6.623167 ]\n",
      "Reset environment\n",
      "Episode reward: 3458.187365412712\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6368747 6.6201863 6.6055365 5.2433476 6.3274183 6.627454 ]\n",
      "Reset environment\n",
      "Episode reward: 1957.0272561088204\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.639417  6.6229186 6.6078467 5.2462535 6.329385  6.6299996]\n",
      "Reset environment\n",
      "Episode reward: 3277.086226761341\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.625207  6.6086965 6.593747  5.217735  6.317182  6.615782 ]\n",
      "Reset environment\n",
      "Episode reward: 2183.419952571392\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6287456 6.612243  6.597283  5.2216277 6.3203506 6.61932  ]\n",
      "Reset environment\n",
      "Episode reward: 2497.269896864891\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.631708  6.615193  6.6002555 5.2249193 6.3229594 6.6222763]\n",
      "Reset environment\n",
      "Episode reward: 2436.8187704980373\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6352353 6.618846  6.603626  5.2288117 6.326036  6.625802 ]\n",
      "Reset environment\n",
      "Episode reward: 3765.9304728135467\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.640775  6.6244907 6.6090403 5.2348795 6.3310637 6.6313396]\n",
      "Reset environment\n",
      "Episode reward: 4407.000721633434\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.646419  6.6301336 6.614694  5.2409782 6.336134  6.636987 ]\n",
      "Reset environment\n",
      "Episode reward: 4314.206552147865\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.651681  6.635408  6.6199603 5.246708  6.340843  6.6422467]\n",
      "Reset environment\n",
      "Episode reward: 1834.5617985129356\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6541796 6.638048  6.6223164 5.249551  6.3430805 6.6447453]\n",
      "Reset environment\n",
      "Episode reward: 3564.6131497621536\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.658582  6.64243   6.626745  5.254361  6.3470387 6.6491504]\n",
      "Reset environment\n",
      "Episode reward: 1353.712308704853\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6609573 6.644786  6.6291413 5.2570066 6.34912   6.651527 ]\n",
      "Reset environment\n",
      "Episode reward: 4450.669948369265\n",
      "Total Steps: 215\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.667025  6.65095   6.63511   5.2637653 6.3546705 6.657602 ]\n",
      "Reset environment\n",
      "Episode reward: 1743.381413338706\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6694326 6.6532154 6.6376443 5.2664332 6.356744  6.6600094]\n",
      "Reset environment\n",
      "Episode reward: 6656.7219088077545\n",
      "Total Steps: 228\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.678101  6.6618905 6.64629   5.275737  6.3645725 6.668679 ]\n",
      "Reset environment\n",
      "Episode reward: 2025.247977167368\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6803274 6.664058  6.648562  5.2782316 6.36648   6.670895 ]\n",
      "Reset environment\n",
      "Episode reward: 280.2016870677471\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6802917 6.6637745 6.648788  5.2783446 6.3661966 6.6708684]\n",
      "Reset environment\n",
      "Episode reward: 5145.705003559589\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6867027 6.670182  6.6552014 5.2853184 6.371907  6.677274 ]\n",
      "Reset environment\n",
      "Episode reward: 4581.778852880001\n",
      "Total Steps: 209\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6930356 6.676609  6.6614304 5.2922673 6.3777385 6.683621 ]\n",
      "Reset environment\n",
      "Episode reward: 2029.4049723148346\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.695909  6.6794543 6.6643367 5.2955217 6.3803368 6.6865053]\n",
      "Reset environment\n",
      "Episode reward: -315.6290182672674\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.6945305 6.6782417 6.662808  5.294342  6.37916   6.6851377]\n",
      "Reset environment\n",
      "Episode reward: 672.9832298755646\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.695288  6.678862  6.663698  5.2950416 6.3798337 6.685901 ]\n",
      "Reset environment\n",
      "Episode reward: 5079.721685945988\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.701543  6.6851125 6.669966  5.301865  6.385454  6.69216  ]\n",
      "Reset environment\n",
      "Episode reward: 3851.595438361168\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.706118  6.6897016 6.6745315 5.3069143 6.389485  6.696742 ]\n",
      "Reset environment\n",
      "Episode reward: 3024.1606738567352\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.709745  6.693439  6.6780157 5.310937  6.392532  6.700375 ]\n",
      "Reset environment\n",
      "Episode reward: 4144.363486200571\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7149663 6.6987333 6.6831517 5.31666   6.3971868 6.7056003]\n",
      "Reset environment\n",
      "Episode reward: 3961.2711373493075\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.720737  6.7046165 6.688808  5.323     6.402416  6.711385 ]\n",
      "Reset environment\n",
      "Episode reward: 2946.8250888586044\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7243066 6.708165  6.6923957 5.3269043 6.405632  6.714956 ]\n",
      "Reset environment\n",
      "Episode reward: 3181.8231070358306\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7280364 6.7120037 6.6959934 5.3310413 6.4089527 6.7186832]\n",
      "Reset environment\n",
      "Episode reward: 2988.086287856102\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.731621  6.7156587 6.699497  5.335009  6.412139  6.7222633]\n",
      "Reset environment\n",
      "Episode reward: -0.08333933353424072\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7309804 6.71526   6.6986303 5.3344827 6.4114237 6.7216387]\n",
      "Reset environment\n",
      "Episode reward: 857.2587406635284\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7321095 6.716493  6.6996484 5.3356786 6.412464  6.722765 ]\n",
      "Reset environment\n",
      "Episode reward: 1390.666429400444\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7345037 6.718879  6.7020516 5.338338  6.414567  6.7251606]\n",
      "Reset environment\n",
      "Episode reward: 2485.585095450282\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.737439  6.7217584 6.7050333 5.341552  6.4171596 6.7280912]\n",
      "Reset environment\n",
      "Episode reward: 2110.2941395044327\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7399664 6.7243085 6.7075315 5.3443403 6.4194245 6.7306156]\n",
      "Reset environment\n",
      "Episode reward: 4542.219971299171\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.745729 6.730046 6.713323 5.350567 6.424641 6.736387]\n",
      "Reset environment\n",
      "Episode reward: 1333.816135942936\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.748052  6.7323637 6.7156534 5.3531623 6.426667  6.7387075]\n",
      "Reset environment\n",
      "Episode reward: -787.0958414375782\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.745835  6.730372  6.7132506 5.3509398 6.424735  6.736503 ]\n",
      "Reset environment\n",
      "Episode reward: 4916.883282259107\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7520347 6.7365074 6.719521  5.357738  6.4303493 6.742716 ]\n",
      "Reset environment\n",
      "Episode reward: 6434.310929238796\n",
      "Total Steps: 222\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.760127  6.7445745 6.727631  5.366525  6.4376283 6.7508006]\n",
      "Reset environment\n",
      "Episode reward: 2166.3066153526306\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7635927 6.748047  6.731092  5.370324  6.4407244 6.754262 ]\n",
      "Reset environment\n",
      "Episode reward: 5241.789565086365\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.770288  6.7547545 6.7377625 5.3775716 6.4467583 6.760957 ]\n",
      "Reset environment\n",
      "Episode reward: 3683.090407907963\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.774819  6.7593465 6.742216  5.382557  6.450774  6.76549  ]\n",
      "Reset environment\n",
      "Episode reward: 1427.2212876826525\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.776498  6.7611947 6.743705  5.384412  6.4521327 6.7671766]\n",
      "Reset environment\n",
      "Episode reward: 2804.909833610058\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7798896 6.7645493 6.7471128 5.38809   6.45518   6.7705584]\n",
      "Reset environment\n",
      "Episode reward: 1349.5876852869987\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7822285 6.766881  6.7494526 5.3906837 6.4572325 6.7728963]\n",
      "Reset environment\n",
      "Episode reward: 260.67604100704193\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7820826 6.7665434 6.7495    5.3907046 6.4570827 6.7727523]\n",
      "Reset environment\n",
      "Episode reward: 1788.7087948322296\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7846713 6.7692165 6.751994  5.3935423 6.459424  6.775338 ]\n",
      "Reset environment\n",
      "Episode reward: -686.0274615287781\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.783033  6.7675757 6.750362  5.391761  6.457912  6.7737007]\n",
      "Reset environment\n",
      "Episode reward: 734.5687466561794\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.7830358 6.767301  6.7506514 5.3922353 6.457709  6.773703 ]\n",
      "Reset environment\n",
      "Episode reward: 3199.1729660332203\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.786724  6.7708735 6.7544413 5.3963337 6.460836  6.7773924]\n",
      "Reset environment\n",
      "Episode reward: 2534.5160145759583\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.789543  6.7736363 6.757298  5.3994384 6.4631796 6.7802005]\n",
      "Reset environment\n",
      "Episode reward: 4035.101450920105\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.794374  6.778482  6.7621183 5.4047265 6.467505  6.785029 ]\n",
      "Reset environment\n",
      "Episode reward: 2533.3099778294563\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.797371  6.781504  6.7650924 5.4080205 6.4701653 6.7880316]\n",
      "Reset environment\n",
      "Episode reward: 1.3477085828781128\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.797077  6.781197  6.7648244 5.407543  6.469954  6.78774  ]\n",
      "Reset environment\n",
      "Episode reward: 4735.468535482883\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.802787  6.786922  6.770532  5.4137774 6.4750624 6.7934504]\n",
      "Reset environment\n",
      "Episode reward: 1167.380848735571\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8039813 6.7883105 6.771523  5.4151473 6.4760814 6.7946424]\n",
      "Reset environment\n",
      "Episode reward: 2629.0192528367043\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8069725 6.791298  6.774517  5.418424  6.47872   6.797632 ]\n",
      "Reset environment\n",
      "Episode reward: 4374.638207688928\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8124094 6.796807  6.7798667 5.4243703 6.4835544 6.8030677]\n",
      "Reset environment\n",
      "Episode reward: 2453.496222436428\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8152566 6.7996197 6.782738  5.4274945 6.486102  6.805909 ]\n",
      "Reset environment\n",
      "Episode reward: 2413.2122895121574\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8185906 6.8030825 6.785908  5.431147  6.4889593 6.8092427]\n",
      "Reset environment\n",
      "Episode reward: 1768.9447501301765\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.820818  6.8054743 6.7879486 5.433596  6.4908648 6.811466 ]\n",
      "Reset environment\n",
      "Episode reward: 1275.7852802276611\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.822136  6.80681   6.7892427 5.435095  6.4920354 6.8127832]\n",
      "Reset environment\n",
      "Episode reward: 3011.904913406819\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8255305 6.81033   6.7924776 5.438924  6.495004  6.8161697]\n",
      "Reset environment\n",
      "Episode reward: 1370.518097937107\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8278756 6.8126826 6.794819  5.441528  6.4970818 6.818518 ]\n",
      "Reset environment\n",
      "Episode reward: 1598.4808278679848\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8296247 6.8144193 6.796577  5.443502  6.4985986 6.8202634]\n",
      "Reset environment\n",
      "Episode reward: 1990.3515866994858\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.832775  6.817574  6.7997203 5.4469905 6.5013857 6.8234158]\n",
      "Reset environment\n",
      "Episode reward: 4013.437686622143\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.83771   6.822496  6.8046756 5.452353  6.505841  6.8283525]\n",
      "Reset environment\n",
      "Episode reward: 2333.0986545085907\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.840374  6.825126  6.8073664 5.4553113 6.508206  6.831014 ]\n",
      "Reset environment\n",
      "Episode reward: 2778.1660515163094\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8436027 6.8282614 6.810679  5.458854  6.511021  6.8342414]\n",
      "Reset environment\n",
      "Episode reward: 3271.75221863389\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8473973 6.8321733 6.8143325 5.4630837 6.5143337 6.83804  ]\n",
      "Reset environment\n",
      "Episode reward: 2053.3200947344303\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8496923 6.834427  6.8166575 5.4656067 6.516282  6.8403316]\n",
      "Reset environment\n",
      "Episode reward: 3907.053045630455\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.854378  6.8390536 6.8214006 5.4707713 6.5204844 6.8450265]\n",
      "Reset environment\n",
      "Episode reward: 2883.693912267685\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.857815  6.842455  6.8248672 5.4745216 6.523559  6.8484664]\n",
      "Reset environment\n",
      "Episode reward: 3059.5959156751633\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.861418  6.8460193 6.828506  5.478485  6.526781  6.852072 ]\n",
      "Reset environment\n",
      "Episode reward: 1728.276335120201\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.864218  6.848822  6.8313017 5.4815745 6.5292645 6.8548727]\n",
      "Reset environment\n",
      "Episode reward: 2621.4827709682286\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8672166 6.8519254 6.834168  5.48492   6.531802  6.857869 ]\n",
      "Reset environment\n",
      "Episode reward: 3624.1883414862677\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.871611  6.856208  6.838658  5.4897094 6.535742  6.8622646]\n",
      "Reset environment\n",
      "Episode reward: 1387.3147557377815\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8739815 6.858581  6.841029  5.4923368 6.537825  6.8646393]\n",
      "Reset environment\n",
      "Episode reward: -311.10525596141815\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8729796 6.857558  6.84005   5.4914427 6.5368285 6.863636 ]\n",
      "Reset environment\n",
      "Episode reward: 5307.142362773418\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.879467  6.864054  6.846553  5.4984846 6.5426517 6.8701296]\n",
      "Reset environment\n",
      "Episode reward: 855.274069070816\n",
      "Total Steps: 28\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8802223 6.864816  6.847301  5.4993753 6.543292  6.8708825]\n",
      "Reset environment\n",
      "Episode reward: 2099.1564487814903\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8835073 6.868101  6.850573  5.5029836 6.5462227 6.8741674]\n",
      "Reset environment\n",
      "Episode reward: 3165.3942400813103\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.887267  6.8718247 6.8543673 5.5071034 6.549597  6.8779283]\n",
      "Reset environment\n",
      "Episode reward: 1279.5020171403885\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.889439  6.873985  6.856545  5.5095544 6.551476  6.880095 ]\n",
      "Reset environment\n",
      "Episode reward: 3603.1217972040176\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8938026 6.878327  6.8609285 5.514317  6.5554113 6.884466 ]\n",
      "Reset environment\n",
      "Episode reward: 1350.7571387290955\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.8952208 6.879749  6.8623395 5.515917  6.5566635 6.8858805]\n",
      "Reset environment\n",
      "Episode reward: 2326.286857485771\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.897897  6.882402  6.8650336 5.5188317 6.5590615 6.888553 ]\n",
      "Reset environment\n",
      "Episode reward: 3502.081973642111\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9016895 6.886002  6.8689904 5.5231423 6.5621295 6.8923435]\n",
      "Reset environment\n",
      "Episode reward: 2466.8977358341217\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9046326 6.8889637 6.871914  5.526373  6.5647664 6.895283 ]\n",
      "Reset environment\n",
      "Episode reward: 2742.981522269547\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9083605 6.8928056 6.8755217 5.5305295 6.5681233 6.899017 ]\n",
      "Reset environment\n",
      "Episode reward: 5257.693306863308\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9149723 6.899415  6.882135  5.5376916 6.574123  6.9056277]\n",
      "Reset environment\n",
      "Episode reward: 4812.048579990864\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9218774 6.906326  6.889038  5.545228  6.58031   6.912534 ]\n",
      "Reset environment\n",
      "Episode reward: 5399.8427983522415\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9286876 6.9131246 6.895823  5.5525646 6.5864224 6.9193397]\n",
      "Reset environment\n",
      "Episode reward: 1338.7998650074005\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9309406 6.9153705 6.898085  5.5550876 6.5883856 6.9215965]\n",
      "Reset environment\n",
      "Episode reward: 255.6436550617218\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9308643 6.91551   6.897812  5.5550537 6.5882254 6.921537 ]\n",
      "Reset environment\n",
      "Episode reward: -655.8299499750137\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9288964 6.913801  6.8956394 5.5530014 6.5864477 6.9195905]\n",
      "Reset environment\n",
      "Episode reward: 3926.382323920727\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9337006 6.918601  6.9004636 5.558212  6.590764  6.924403 ]\n",
      "Reset environment\n",
      "Episode reward: 2117.662585437298\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.936148  6.921059  6.9028955 5.56089   6.592964  6.926847 ]\n",
      "Reset environment\n",
      "Episode reward: -686.1962209939957\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9345336 6.9194455 6.901279  5.559247  6.591414  6.925233 ]\n",
      "Reset environment\n",
      "Episode reward: 3348.427226662636\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.938544  6.9234505 6.905302  5.563627  6.5950117 6.929253 ]\n",
      "Reset environment\n",
      "Episode reward: 1386.9526151418686\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9409027 6.9258156 6.907649  5.5662374 6.597096  6.9316125]\n",
      "Reset environment\n",
      "Episode reward: 2191.0701256394386\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.944341  6.929241  6.9110847 5.570028  6.6001377 6.935043 ]\n",
      "Reset environment\n",
      "Episode reward: 1373.3112280368805\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.945773  6.9306574 6.912531  5.5716224 6.6014132 6.9364743]\n",
      "Reset environment\n",
      "Episode reward: 4741.063392877579\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.951677  6.936543  6.9184327 5.578     6.606744  6.9423738]\n",
      "Reset environment\n",
      "Episode reward: 2693.611669667065\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9547725 6.9397483 6.9213862 5.5814524 6.609308  6.945471 ]\n",
      "Reset environment\n",
      "Episode reward: 2173.9257944226265\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9581537 6.9431314 6.924769  5.5851665 6.6123347 6.9488544]\n",
      "Reset environment\n",
      "Episode reward: 4421.75753813982\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9635434 6.948571  6.9300947 5.591075  6.6171317 6.9542494]\n",
      "Reset environment\n",
      "Episode reward: 2096.8992661237717\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9667997 6.951824  6.9333663 5.594677  6.620028  6.9575157]\n",
      "Reset environment\n",
      "Episode reward: -171.34903305768967\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.966057  6.951192  6.932528  5.593772  6.61942   6.9567747]\n",
      "Reset environment\n",
      "Episode reward: 2475.375910460949\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9695573 6.954696  6.9360266 5.5976086 6.6225467 6.9602695]\n",
      "Reset environment\n",
      "Episode reward: 5420.996678650379\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.976379  6.961512  6.9428706 5.60494   6.628715  6.9671   ]\n",
      "Reset environment\n",
      "Episode reward: 2033.9030111432076\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9786925 6.9638214 6.945198  5.6074967 6.63077   6.9694147]\n",
      "Reset environment\n",
      "Episode reward: 1382.4302030205727\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9801173 6.9652486 6.9466176 5.609114  6.6320105 6.9708376]\n",
      "Reset environment\n",
      "Episode reward: 3892.091200992465\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9847393 6.9698052 6.951294  5.6141467 6.6361923 6.975464 ]\n",
      "Reset environment\n",
      "Episode reward: 2783.686514765024\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.988679  6.9738607 6.9550915 5.618519  6.6395745 6.9793973]\n",
      "Reset environment\n",
      "Episode reward: -703.8942134380341\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9869885 6.9721684 6.953425  5.6166935 6.6380496 6.9777174]\n",
      "Reset environment\n",
      "Episode reward: 1448.2365708947182\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9888854 6.9741797 6.9551997 5.61882   6.639757  6.9796147]\n",
      "Reset environment\n",
      "Episode reward: -813.1302894353867\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9869466 6.9720683 6.9534383 5.6165776 6.6381707 6.977682 ]\n",
      "Reset environment\n",
      "Episode reward: 3524.749890625477\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.991205  6.9763136 6.9577055 5.621206  6.6420264 6.981936 ]\n",
      "Reset environment\n",
      "Episode reward: 780.8707706779242\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9916067 6.976458  6.958384  5.6220694 6.6422324 6.982357 ]\n",
      "Reset environment\n",
      "Episode reward: 1973.4850542843342\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.994016  6.978665  6.96097   5.624858  6.644139  6.9847784]\n",
      "Reset environment\n",
      "Episode reward: -691.2131253480911\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9923797 6.9770308 6.959337  5.6231756 6.6425915 6.9831486]\n",
      "Reset environment\n",
      "Episode reward: 683.185201883316\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9925704 6.9773893 6.959374  5.623465  6.64269   6.983352 ]\n",
      "Reset environment\n",
      "Episode reward: 3602.194076836109\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [6.9967446 6.9815655 6.9635444 5.6280093 6.6464386 6.987526 ]\n",
      "Reset environment\n",
      "Episode reward: 2547.0604777932167\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.000589  6.9854145 6.967395  5.632218  6.649885  6.9913754]\n",
      "Reset environment\n",
      "Episode reward: 1675.0746536254883\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0023985 6.9872074 6.9692197 5.6342154 6.651498  6.9931836]\n",
      "Reset environment\n",
      "Episode reward: 1999.5348872542381\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0055337 6.990345  6.9723544 5.6376333 6.6543074 6.996321 ]\n",
      "Reset environment\n",
      "Episode reward: 1507.2150502204895\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0070815 6.9917154 6.974084  5.6394186 6.6556253 6.9978757]\n",
      "Reset environment\n",
      "Episode reward: 1003.2688794136047\n",
      "Total Steps: 33\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0080104 6.9926357 6.9750204 5.6404867 6.6564374 6.9988046]\n",
      "Reset environment\n",
      "Episode reward: 5085.515735805035\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.014293  6.9989614 6.9812336 5.6472936 6.662049  7.0050874]\n",
      "Reset environment\n",
      "Episode reward: 3362.4527962207794\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.018122  7.00267   6.985182  5.65154   6.6653147 7.008926 ]\n",
      "Reset environment\n",
      "Episode reward: 1531.3639178872108\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0199375 7.0046234 6.9868555 5.653568  6.666974  7.0107384]\n",
      "Reset environment\n",
      "Episode reward: 2553.3810916543007\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0229053 7.0076046 6.9898033 5.656848  6.6696067 7.013705 ]\n",
      "Reset environment\n",
      "Episode reward: 4083.751636713743\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.027756  7.0124207 6.9947047 5.662164  6.674     7.018565 ]\n",
      "Reset environment\n",
      "Episode reward: 4930.964666426182\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0338535 7.018539  7.000775  5.668757  6.6794896 7.024668 ]\n",
      "Reset environment\n",
      "Episode reward: 3728.377234697342\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0383506 7.023008  7.005296  5.673636  6.683556  7.029166 ]\n",
      "Reset environment\n",
      "Episode reward: 137.61926007270813\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.037959  7.022806  7.0047307 5.6732306 6.6831765 7.028788 ]\n",
      "Reset environment\n",
      "Episode reward: 1799.2237783968449\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.039916  7.0247407 7.0067134 5.6753583 6.6849194 7.030743 ]\n",
      "Reset environment\n",
      "Episode reward: 1555.1075594425201\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.041572  7.0263867 7.008375  5.677237  6.686369  7.032396 ]\n",
      "Reset environment\n",
      "Episode reward: 3798.792334854603\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0461597 7.0309963 7.0129385 5.6822405 6.6904583 7.036983 ]\n",
      "Reset environment\n",
      "Episode reward: 2640.603451013565\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0492005 7.03404   7.015974  5.685554  6.6931834 7.040019 ]\n",
      "Reset environment\n",
      "Episode reward: 5800.950935900211\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0563745 7.041224  7.023108  5.6932983 6.699637  7.0471883]\n",
      "Reset environment\n",
      "Episode reward: 946.2161636352539\n",
      "Total Steps: 31\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.057235  7.042102  7.0239553 5.694298  6.7003827 7.0480523]\n",
      "Reset environment\n",
      "Episode reward: 1574.6337609291077\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.059423  7.044288  7.026139  5.6967397 6.7023587 7.0502367]\n",
      "Reset environment\n",
      "Episode reward: 1982.416628330946\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0619965 7.046989  7.028577  5.6996083 6.704694  7.0528145]\n",
      "Reset environment\n",
      "Episode reward: 5919.609414875507\n",
      "Total Steps: 206\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.069121  7.054094  7.0357213 5.7073197 6.7111535 7.05994  ]\n",
      "Reset environment\n",
      "Episode reward: 3826.2570404708385\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.073704  7.0587764 7.0401506 5.712322  6.7151303 7.064517 ]\n",
      "Reset environment\n",
      "Episode reward: 4869.0559003949165\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0796556 7.0647125 7.046116  5.718775  6.720457  7.0704727]\n",
      "Reset environment\n",
      "Episode reward: 2143.2331933379173\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0829554 7.0680137 7.0494246 5.7223983 6.7233987 7.0737786]\n",
      "Reset environment\n",
      "Episode reward: 2086.960179388523\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0852795 7.0703278 7.051751  5.7249684 6.725441  7.0760975]\n",
      "Reset environment\n",
      "Episode reward: 1362.531830072403\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.087546  7.0725927 7.0540185 5.7274857 6.7274175 7.078365 ]\n",
      "Reset environment\n",
      "Episode reward: 3390.7836687266827\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.091398  7.0763383 7.0579667 5.731718  6.7307296 7.082219 ]\n",
      "Reset environment\n",
      "Episode reward: 3273.901113152504\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.095115  7.079954  7.061767  5.7358403 6.733987  7.0859437]\n",
      "Reset environment\n",
      "Episode reward: 3803.0890555381775\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.0996366 7.084505  7.066264  5.740783  6.738056  7.090467 ]\n",
      "Reset environment\n",
      "Episode reward: 4527.491674125195\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1051235 7.090006  7.0717287 5.7467313 6.742976  7.09596  ]\n",
      "Reset environment\n",
      "Episode reward: 2652.0710309147835\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.107991  7.0928864 7.0745826 5.749908  6.7455006 7.0988255]\n",
      "Reset environment\n",
      "Episode reward: 4128.713396966457\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1127033 7.0976133 7.079273  5.7550654 6.749708  7.1035347]\n",
      "Reset environment\n",
      "Episode reward: 2760.2807442210615\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1157284 7.1007504 7.0821524 5.7584763 6.752195  7.1065636]\n",
      "Reset environment\n",
      "Episode reward: 1428.782105088234\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.117356  7.102531  7.0836177 5.760244  6.75354   7.1082006]\n",
      "Reset environment\n",
      "Episode reward: 5427.066791474819\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1238465 7.109028  7.0900936 5.767253  6.7593503 7.1146917]\n",
      "Reset environment\n",
      "Episode reward: 2535.4317177534103\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1267304 7.1119227 7.0929604 5.7704225 6.761912  7.1175737]\n",
      "Reset environment\n",
      "Episode reward: 3057.438849836588\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1302333 7.115385  7.0964966 5.7742653 6.7650313 7.121082 ]\n",
      "Reset environment\n",
      "Episode reward: 5167.819758892059\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.136363  7.1215224 7.102636  5.780915  6.7705274 7.1272244]\n",
      "Reset environment\n",
      "Episode reward: 2581.49891936779\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.139163  7.1243076 7.105448  5.7840023 6.772998  7.1300263]\n",
      "Reset environment\n",
      "Episode reward: 2043.5815584659576\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1414514 7.126579  7.1077447 5.7865243 6.7750053 7.1323137]\n",
      "Reset environment\n",
      "Episode reward: 1383.7707483768463\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1428804 7.127987  7.1091924 5.7880845 6.7762856 7.1337433]\n",
      "Reset environment\n",
      "Episode reward: 1477.4668956398964\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1444116 7.1295156 7.110725  5.789799  6.777624  7.1352744]\n",
      "Reset environment\n",
      "Episode reward: 4313.448403656483\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1495647 7.134657  7.115892  5.7953978 6.7822704 7.1404343]\n",
      "Reset environment\n",
      "Episode reward: 2213.8972620368004\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.151962  7.1370177 7.1183224 5.7980413 6.784392  7.142833 ]\n",
      "Reset environment\n",
      "Episode reward: 1626.1481819748878\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1536884 7.138726  7.1200643 5.799945  6.7859406 7.1445594]\n",
      "Reset environment\n",
      "Episode reward: 1343.5705825686455\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1559105 7.1409516 7.1222854 5.8024144 6.787896  7.146784 ]\n",
      "Reset environment\n",
      "Episode reward: 6422.111594736576\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1636558 7.1487    7.1300273 5.8108244 6.7948112 7.1545377]\n",
      "Reset environment\n",
      "Episode reward: 1375.4155111312866\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.165038  7.1500506 7.1314445 5.8123584 6.7960477 7.1559215]\n",
      "Reset environment\n",
      "Episode reward: 1336.1234144568443\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.167245  7.152257  7.133646  5.8148317 6.797981  7.1581283]\n",
      "Reset environment\n",
      "Episode reward: 1399.1757507324219\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1695676 7.1545606 7.135985  5.8174043 6.8000154 7.160448 ]\n",
      "Reset environment\n",
      "Episode reward: 4403.940341591835\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.174849  7.1598363 7.1412683 5.8231277 6.804784  7.165736 ]\n",
      "Reset environment\n",
      "Episode reward: 2889.311831369996\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.177741  7.1625724 7.1442876 5.826496  6.8070664 7.1686373]\n",
      "Reset environment\n",
      "Episode reward: 2666.088299304247\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1806803 7.1656227 7.147078  5.829808  6.809497  7.171581 ]\n",
      "Reset environment\n",
      "Episode reward: 6546.626292824745\n",
      "Total Steps: 225\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.188547  7.1735253 7.1549344 5.838343  6.8165746 7.179455 ]\n",
      "Reset environment\n",
      "Episode reward: 2028.5360273122787\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.190694 7.175668 7.157082 5.840762 6.818441 7.181601]\n",
      "Reset environment\n",
      "Episode reward: 3509.8783400058746\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1948156 7.1797724 7.1612267 5.8452444 6.8221664 7.1857285]\n",
      "Reset environment\n",
      "Episode reward: 2153.4235276579857\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.1980834 7.1830516 7.1644936 5.848824  6.825089  7.1890016]\n",
      "Reset environment\n",
      "Episode reward: 14.917879081331193\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.196993  7.1821737 7.1632314 5.8480053 6.8241434 7.187923 ]\n",
      "Reset environment\n",
      "Episode reward: 4791.881370663643\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.202677  7.1878552 7.1689134 5.854224  6.8292456 7.193623 ]\n",
      "Reset environment\n",
      "Episode reward: 795.4858083128929\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2030587 7.188012  7.1695037 5.8548737 6.829537  7.194013 ]\n",
      "Reset environment\n",
      "Episode reward: 5599.504333436489\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2098885 7.194844  7.1763296 5.862276  6.8356752 7.2008543]\n",
      "Reset environment\n",
      "Episode reward: 5708.220341026783\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.216848  7.201777  7.1833053 5.8697886 6.8419323 7.2078223]\n",
      "Reset environment\n",
      "Episode reward: 5715.456520974636\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.223839  7.208757  7.190314  5.8773403 6.8482575 7.2148204]\n",
      "Reset environment\n",
      "Episode reward: 602.0544817447662\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.224278  7.2090716 7.1908774 5.87784   6.8487015 7.215269 ]\n",
      "Reset environment\n",
      "Episode reward: 3201.948017142713\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.227893  7.212615  7.1945724 5.8818107 6.85193   7.2188897]\n",
      "Reset environment\n",
      "Episode reward: 4702.006044209003\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.233553  7.218255  7.2002506 5.887935  6.857041  7.2245517]\n",
      "Reset environment\n",
      "Episode reward: 1833.5502988696098\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2354903 7.2201653 7.2022114 5.890067  6.858764  7.2264876]\n",
      "Reset environment\n",
      "Episode reward: 1407.681471824646\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2369323 7.2216167 7.2036395 5.891691  6.8600373 7.22793  ]\n",
      "Reset environment\n",
      "Episode reward: 3628.656559690833\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2416997 7.226502  7.2082644 5.896909  6.8642907 7.2327   ]\n",
      "Reset environment\n",
      "Episode reward: 2599.506235539913\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2446632 7.2294884 7.2112007 5.900168  6.8669415 7.2356586]\n",
      "Reset environment\n",
      "Episode reward: 2268.276491045952\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.248086  7.232892  7.2146454 5.9039383 6.8699875 7.239078 ]\n",
      "Reset environment\n",
      "Episode reward: 4270.235946118832\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.25409   7.2389693 7.2205734 5.9104805 6.8753314 7.2450747]\n",
      "Reset environment\n",
      "Episode reward: -411.35387021303177\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.252362  7.2370462 7.2190466 5.9087825 6.873823  7.243356 ]\n",
      "Reset environment\n",
      "Episode reward: 3894.4112080931664\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.25678   7.2415595 7.2233515 5.9136424 6.877768  7.2477727]\n",
      "Reset environment\n",
      "Episode reward: 1410.7070996761322\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2581944 7.242943  7.224795  5.9152193 6.8790193 7.2491875]\n",
      "Reset environment\n",
      "Episode reward: 4701.237255215645\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.263782  7.248557  7.230376  5.9212956 6.884054  7.2547874]\n",
      "Reset environment\n",
      "Episode reward: 2927.2215108908713\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2666287 7.2515073 7.233089  5.924561  6.8862257 7.2576413]\n",
      "Reset environment\n",
      "Episode reward: 3136.2519677579403\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2700014 7.2550073 7.236314  5.928367  6.8891015 7.261013 ]\n",
      "Reset environment\n",
      "Episode reward: 2208.9980536699295\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.272417  7.2574325 7.2387147 5.9310656 6.891243  7.26343  ]\n",
      "Reset environment\n",
      "Episode reward: 1408.14157640934\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2747083 7.2597265 7.241006  5.933594  6.893273  7.2657223]\n",
      "Reset environment\n",
      "Episode reward: 3206.6254108175635\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2786217 7.2636485 7.24493   5.9379444 6.8968906 7.269656 ]\n",
      "Reset environment\n",
      "Episode reward: 2483.423302054405\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2814    7.266432  7.247695  5.9409986 6.89935   7.272435 ]\n",
      "Reset environment\n",
      "Episode reward: 3161.634637236595\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2849255 7.2698903 7.251287  5.9448867 6.9025006 7.275967 ]\n",
      "Reset environment\n",
      "Episode reward: 2010.7355078458786\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.287069  7.272022  7.2534375 5.9472795 6.9043736 7.2781115]\n",
      "Reset environment\n",
      "Episode reward: 3623.8251044750214\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.291199  7.2761474 7.25756   5.9518094 6.908056  7.2822385]\n",
      "Reset environment\n",
      "Episode reward: 2039.9137204289436\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2933197 7.2782288 7.2597175 5.954152  6.909925  7.2843547]\n",
      "Reset environment\n",
      "Episode reward: 2996.7947726249695\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.2966228 7.281643  7.2628818 5.9578395 6.9127274 7.2876534]\n",
      "Reset environment\n",
      "Episode reward: 2825.071611136198\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.300398  7.2855344 7.266521  5.9619904 6.9161005 7.2914314]\n",
      "Reset environment\n",
      "Episode reward: 2960.952553778887\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.303712  7.288947  7.2696986 5.9656754 6.9188538 7.2947426]\n",
      "Reset environment\n",
      "Episode reward: 1720.1691489219666\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3055515 7.290796  7.2715325 5.9677124 6.920483  7.2965827]\n",
      "Reset environment\n",
      "Episode reward: 4906.722806751728\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.311481 7.296716 7.277469 5.974091 6.925836 7.302517]\n",
      "Reset environment\n",
      "Episode reward: 2339.2165393531322\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3139377 7.299283  7.279809  5.9768453 6.9279046 7.30498  ]\n",
      "Reset environment\n",
      "Episode reward: 4101.01569133997\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.318775  7.3041134 7.2846437 5.982092  6.932246  7.3098145]\n",
      "Reset environment\n",
      "Episode reward: 6426.580123364925\n",
      "Total Steps: 224\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.326324  7.31167   7.2921925 5.9902787 6.9390116 7.317371 ]\n",
      "Reset environment\n",
      "Episode reward: 4246.416871249676\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.331332  7.316664  7.297209  5.9956827 6.943515  7.322384 ]\n",
      "Reset environment\n",
      "Episode reward: 2991.3412278294563\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3347063 7.3200226 7.3006086 5.9993806 6.94656   7.3257637]\n",
      "Reset environment\n",
      "Episode reward: 3125.895852088928\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3383117 7.3236656 7.304163  6.003317  6.9497643 7.3293705]\n",
      "Reset environment\n",
      "Episode reward: -129.92521929740906\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.33713   7.3223023 7.3031754 6.0022492 6.9487324 7.328204 ]\n",
      "Reset environment\n",
      "Episode reward: 3918.6147484108806\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.342491  7.3277774 7.3084264 6.008099  6.9535885 7.3335686]\n",
      "Reset environment\n",
      "Episode reward: 3357.9796090722084\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.346336  7.3316355 7.312257  6.0122924 6.9570355 7.337415 ]\n",
      "Reset environment\n",
      "Episode reward: 1702.4926295876503\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3489943 7.3342934 7.3149204 6.0152235 6.9593787 7.340073 ]\n",
      "Reset environment\n",
      "Episode reward: 589.365396976471\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3492956 7.3347735 7.315047  6.0155425 6.959601  7.340374 ]\n",
      "Reset environment\n",
      "Episode reward: 2775.8083031773567\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3523917 7.337902  7.318099  6.018956  6.962329  7.343468 ]\n",
      "Reset environment\n",
      "Episode reward: 1712.5786681771278\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.354197  7.339727  7.3198853 6.0209527 6.9639363 7.3452682]\n",
      "Reset environment\n",
      "Episode reward: 3538.1557053774595\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.359012  7.3446484 7.3245745 6.0262437 6.9682055 7.3500776]\n",
      "Reset environment\n",
      "Episode reward: 2993.6788741201162\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.362201  7.347737  7.327851  6.0297947 6.9709454 7.353269 ]\n",
      "Reset environment\n",
      "Episode reward: 36.26516807079315\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3617    7.3473516 7.327244  6.029239  6.97056   7.35277  ]\n",
      "Reset environment\n",
      "Episode reward: 1714.7629544138908\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3634324 7.3490644 7.3290005 6.031181  6.9720907 7.3545017]\n",
      "Reset environment\n",
      "Episode reward: -182.749745413661\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.362033  7.347871  7.327435  6.0297685 6.9708233 7.353122 ]\n",
      "Reset environment\n",
      "Episode reward: 1434.8222237229347\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3643494 7.350192  7.329746  6.032328  6.972868  7.3554373]\n",
      "Reset environment\n",
      "Episode reward: 777.0908690690994\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3648973 7.3507075 7.330327  6.0329976 6.973347  7.3559847]\n",
      "Reset environment\n",
      "Episode reward: 3358.6894346773624\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.368773  7.354681  7.334083  6.037278  6.9765587 7.3598633]\n",
      "Reset environment\n",
      "Episode reward: 750.2266459465027\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.36966   7.3555746 7.3349695 6.03819   6.9774456 7.360755 ]\n",
      "Reset environment\n",
      "Episode reward: 3295.3575116991997\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3732576 7.3591776 7.3385653 6.0421324 6.9806437 7.3643517]\n",
      "Reset environment\n",
      "Episode reward: 2247.014952003956\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.376577  7.3625    7.341884  6.0457544 6.9836125 7.3676753]\n",
      "Reset environment\n",
      "Episode reward: -248.47402238845825\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3754663 7.3614626 7.3407154 6.0447364 6.9827347 7.3665776]\n",
      "Reset environment\n",
      "Episode reward: 2550.0127826035023\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.378189  7.3641524 7.3434734 6.0477448 6.985145  7.369301 ]\n",
      "Reset environment\n",
      "Episode reward: 4230.5954531133175\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3831124 7.369119  7.3483534 6.0531187 6.989504  7.3742275]\n",
      "Reset environment\n",
      "Episode reward: -685.4802906513214\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.381455  7.367467  7.3467183 6.051433  6.9879513 7.3725886]\n",
      "Reset environment\n",
      "Episode reward: 1349.4995921254158\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.383629  7.3696303 7.348904  6.0538573 6.9898577 7.374761 ]\n",
      "Reset environment\n",
      "Episode reward: 264.94787192344666\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3836346 7.369713  7.348837  6.053789  6.989911  7.374776 ]\n",
      "Reset environment\n",
      "Episode reward: 5227.554750263691\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3896337 7.375694  7.3548446 6.0603104 6.9952564 7.380776 ]\n",
      "Reset environment\n",
      "Episode reward: 4761.083939224482\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3952303 7.381283  7.360463  6.066374  7.000313  7.3863783]\n",
      "Reset environment\n",
      "Episode reward: -670.5512831807137\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3936024 7.379641  7.3588614 6.0647745 6.9987674 7.384759 ]\n",
      "Reset environment\n",
      "Episode reward: -409.24362909793854\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3924127 7.378476  7.357645  6.0633383 6.997792  7.3835783]\n",
      "Reset environment\n",
      "Episode reward: 3978.1686729192734\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3970256 7.383081  7.362264  6.0683546 7.001954  7.3881955]\n",
      "Reset environment\n",
      "Episode reward: 1.3558825254440308\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.3966455 7.382676  7.3619194 6.067697  7.001657  7.3878155]\n",
      "Reset environment\n",
      "Episode reward: 4061.324093937874\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4012017 7.387223  7.3664846 6.072647  7.005708  7.3923707]\n",
      "Reset environment\n",
      "Episode reward: 4186.794449329376\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4060793 7.392103  7.371372  6.077939  7.010109  7.3972507]\n",
      "Reset environment\n",
      "Episode reward: 1360.5889416337013\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4082494 7.3942714 7.3735538 6.0803647 7.012007  7.399425 ]\n",
      "Reset environment\n",
      "Episode reward: 2852.960948884487\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.411412  7.397446  7.3767147 6.083859  7.0148273 7.402591 ]\n",
      "Reset environment\n",
      "Episode reward: 1316.9333815574646\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4135213 7.3995566 7.3788266 6.08623   7.0166726 7.404701 ]\n",
      "Reset environment\n",
      "Episode reward: 4324.027449786663\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4185724 7.4046235 7.383864  6.091702  7.021231  7.4097614]\n",
      "Reset environment\n",
      "Episode reward: 1363.3086650371552\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.420768  7.4068203 7.386057  6.0941305 7.0231643 7.411958 ]\n",
      "Reset environment\n",
      "Episode reward: 2219.162432074547\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4231844 7.409243  7.3884583 6.0968094 7.0253277 7.4143744]\n",
      "Reset environment\n",
      "Episode reward: 4252.97307831049\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.428111  7.414192  7.3933673 6.1021824 7.0297527 7.4193125]\n",
      "Reset environment\n",
      "Episode reward: 5252.713004827499\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4343576 7.420455  7.399585  6.108925  7.0353556 7.4255633]\n",
      "Reset environment\n",
      "Episode reward: 1356.8628348112106\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4356837 7.4217687 7.400924  6.1104016 7.0365357 7.426885 ]\n",
      "Reset environment\n",
      "Episode reward: 1658.047734245658\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4371853 7.423089  7.4026074 6.1122212 7.0375547 7.428408 ]\n",
      "Reset environment\n",
      "Episode reward: 1329.0195585116744\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4385414 7.4242015 7.4042068 6.1139946 7.0386367 7.429777 ]\n",
      "Reset environment\n",
      "Episode reward: 1670.464319229126\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.440234  7.425829  7.4059467 6.1158447 7.040128  7.4314675]\n",
      "Reset environment\n",
      "Episode reward: -700.7232631444931\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4385552 7.424141  7.4042764 6.1140695 7.0385704 7.429789 ]\n",
      "Reset environment\n",
      "Episode reward: -688.9186341762543\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4368234 7.4224014 7.4025617 6.1121087 7.0370097 7.4280562]\n",
      "Reset environment\n",
      "Episode reward: 4821.744199872017\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4424696 7.428056  7.408183  6.11822   7.0420723 7.4337077]\n",
      "Reset environment\n",
      "Episode reward: 607.8610014915466\n",
      "Total Steps: 20\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4428554 7.4284453 7.408567  6.1187067 7.0423875 7.4340935]\n",
      "Reset environment\n",
      "Episode reward: 525.5544199347496\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.442544  7.4279127 7.408502  6.1185694 7.041938  7.4338026]\n",
      "Reset environment\n",
      "Episode reward: 3807.6385374069214\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4468975 7.4322977 7.412814  6.123328  7.045798  7.4381514]\n",
      "Reset environment\n",
      "Episode reward: 2248.760986328125\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.450193  7.4355903 7.4161057 6.1269336 7.048746  7.4414463]\n",
      "Reset environment\n",
      "Episode reward: -609.9123148918152\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.448493  7.4337244 7.4145894 6.1251774 7.0472674 7.439757 ]\n",
      "Reset environment\n",
      "Episode reward: 1675.528446316719\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.450179  7.4353924 7.416289  6.1270328 7.0487604 7.4414425]\n",
      "Reset environment\n",
      "Episode reward: 5057.889595806599\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4559684 7.4411764 7.422091  6.133292  7.053936  7.447235 ]\n",
      "Reset environment\n",
      "Episode reward: 4399.6726016402245\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.461108  7.446294  7.4272447 6.1388516 7.0585785 7.4523807]\n",
      "Reset environment\n",
      "Episode reward: 1.1780446767807007\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.460029  7.4453897 7.426022  6.137827  7.057643  7.451315 ]\n",
      "Reset environment\n",
      "Episode reward: 2693.9028974324465\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.462962  7.4482765 7.4290056 6.1410236 7.060252  7.4542537]\n",
      "Reset environment\n",
      "Episode reward: 5660.51567953825\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4696817 7.4549527 7.4357624 6.148255  7.066322  7.460976 ]\n",
      "Reset environment\n",
      "Episode reward: 1699.3792905211449\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.471433  7.4567256 7.437493  6.1502094 7.067883  7.4627256]\n",
      "Reset environment\n",
      "Episode reward: 1419.7182564735413\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4728236 7.4581046 7.4388924 6.1517696 7.0691257 7.464113 ]\n",
      "Reset environment\n",
      "Episode reward: 5905.4686216712\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.479811  7.465093  7.4458976 6.1593246 7.075429  7.4711123]\n",
      "Reset environment\n",
      "Episode reward: 3802.1332926154137\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.484117  7.4694133 7.4501777 6.1640224 7.0792794 7.475414 ]\n",
      "Reset environment\n",
      "Episode reward: 2187.3851497769356\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.487309  7.472608  7.4533715 6.167539  7.0821333 7.478605 ]\n",
      "Reset environment\n",
      "Episode reward: -362.9953509569168\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4858418 7.4712563 7.451805  6.166016  7.08091   7.4771543]\n",
      "Reset environment\n",
      "Episode reward: 4690.52297526598\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.491406  7.476816  7.457335  6.1719904 7.085908  7.4827194]\n",
      "Reset environment\n",
      "Episode reward: 546.1562840938568\n",
      "Total Steps: 17\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.491723  7.4771338 7.4576473 6.172407  7.0861607 7.483036 ]\n",
      "Reset environment\n",
      "Episode reward: 1823.545966744423\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.493613  7.4790206 7.459524  6.1745067 7.087833  7.4849186]\n",
      "Reset environment\n",
      "Episode reward: 2530.2408883571625\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.496355  7.4817715 7.462258  6.177525  7.0902743 7.4876633]\n",
      "Reset environment\n",
      "Episode reward: 5153.450702905655\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4809556 7.466367  7.4468756 6.1484857 7.077139  7.4722524]\n",
      "Reset environment\n",
      "Episode reward: 5467.502777814865\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.487409  7.472814  7.4533463 6.155419  7.082987  7.478716 ]\n",
      "Reset environment\n",
      "Episode reward: 2071.6180378198624\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.489497  7.475007  7.4553223 6.1577797 7.084779  7.480806 ]\n",
      "Reset environment\n",
      "Episode reward: 2191.898966729641\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.4917917 7.477337  7.457574  6.1603312 7.0868063 7.483095 ]\n",
      "Reset environment\n",
      "Episode reward: 2368.6782374978065\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.494322  7.4798417 7.4601316 6.163102  7.089081  7.485624 ]\n",
      "Reset environment\n",
      "Episode reward: 4348.991934031248\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.499225  7.484738  7.465048  6.1684723 7.0934877 7.4905286]\n",
      "Reset environment\n",
      "Episode reward: 1414.423828817904\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.50065   7.4863176 7.4663124 6.1700597 7.0947614 7.4919643]\n",
      "Reset environment\n",
      "Episode reward: 4793.383411586285\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5062256 7.4919024 7.4718685 6.176082  7.0998034 7.4975395]\n",
      "Reset environment\n",
      "Episode reward: 1394.9724264144897\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5084233 7.4941063 7.474052  6.1785245 7.1017375 7.499733 ]\n",
      "Reset environment\n",
      "Episode reward: 5591.434240281582\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5148115 7.5005093 7.4804387 6.1854153 7.1075053 7.506126 ]\n",
      "Reset environment\n",
      "Episode reward: 5124.68764090538\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.520576  7.5062733 7.4862146 6.191649  7.1126847 7.5118976]\n",
      "Reset environment\n",
      "Episode reward: 2840.156081020832\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5241385 7.509956  7.4896626 6.195592  7.115877  7.5154676]\n",
      "Reset environment\n",
      "Episode reward: -345.6139286458492\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5229473 7.5087676 7.4884744 6.194381  7.114718  7.5142775]\n",
      "Reset environment\n",
      "Episode reward: 2180.140367627144\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5261326 7.5119567 7.4916615 6.1978493 7.1175694 7.517462 ]\n",
      "Reset environment\n",
      "Episode reward: 4520.43951934576\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.531297  7.5170984 7.496852  6.2034764 7.122213  7.5226336]\n",
      "Reset environment\n",
      "Episode reward: 2568.044665634632\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.534137  7.5199413 7.4996934 6.206567  7.124776  7.525477 ]\n",
      "Reset environment\n",
      "Episode reward: 4913.136184036732\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.539881  7.5256977 7.5054183 6.212742  7.129967  7.5312285]\n",
      "Reset environment\n",
      "Episode reward: 397.7694444656372\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.539942  7.525879  7.5053673 6.2128043 7.1300635 7.5312924]\n",
      "Reset environment\n",
      "Episode reward: 4559.38757699728\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.545195  7.5312023 7.510527  6.2185073 7.1347895 7.5365424]\n",
      "Reset environment\n",
      "Episode reward: 3464.221609532833\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5497375 7.5358486 7.5149446 6.223503  7.1388946 7.541084 ]\n",
      "Reset environment\n",
      "Episode reward: 2689.9660147149116\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5525775 7.5388026 7.517654  6.2266703 7.1413503 7.543924 ]\n",
      "Reset environment\n",
      "Episode reward: 1920.0944895148277\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5545435 7.540743  7.51964   6.2288504 7.143081  7.5458894]\n",
      "Reset environment\n",
      "Episode reward: -685.1917383670807\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.552827  7.5390286 7.5179386 6.227063  7.1415124 7.544182 ]\n",
      "Reset environment\n",
      "Episode reward: 3863.77645778656\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5574517 7.5437164 7.5225    6.2321415 7.145768  7.5488143]\n",
      "Reset environment\n",
      "Episode reward: 684.3190986216068\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.557569  7.544041  7.522436  6.2323785 7.1456976 7.5489507]\n",
      "Reset environment\n",
      "Episode reward: 2467.2977098822594\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5599594 7.5465446 7.524719  6.235076  7.1477485 7.551342 ]\n",
      "Reset environment\n",
      "Episode reward: 1480.48222191073\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5611634 7.547545  7.526126  6.23664   7.1487045 7.5525546]\n",
      "Reset environment\n",
      "Episode reward: 1232.235445022583\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.562299  7.5486484 7.5272865 6.2379117 7.1497154 7.553688 ]\n",
      "Reset environment\n",
      "Episode reward: -689.588681101799\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5605083 7.5468473 7.5255184 6.236086  7.1481247 7.5519085]\n",
      "Reset environment\n",
      "Episode reward: 5479.75017362833\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5669656 7.5532923 7.5319805 6.242997  7.1539416 7.5583644]\n",
      "Reset environment\n",
      "Episode reward: 2137.108527779579\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.570071  7.556405  7.5350766 6.246398  7.156719  7.5614686]\n",
      "Reset environment\n",
      "Episode reward: 1931.888500571251\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.572053  7.5583997 7.5370355 6.2485857 7.158469  7.5634465]\n",
      "Reset environment\n",
      "Episode reward: 1569.953249424696\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5737796 7.5599117 7.5389543 6.250616  7.1599026 7.5651793]\n",
      "Reset environment\n",
      "Episode reward: 1989.976135313511\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5758367 7.5619874 7.540987  6.252891  7.1617107 7.567237 ]\n",
      "Reset environment\n",
      "Episode reward: 1236.5108506679535\n",
      "Total Steps: 39\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5770116 7.563152  7.5421715 6.254197  7.162757  7.5684104]\n",
      "Reset environment\n",
      "Episode reward: 6643.13488972187\n",
      "Total Steps: 230\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5845475 7.5706916 7.5497255 6.2623677 7.1695666 7.5759425]\n",
      "Reset environment\n",
      "Episode reward: 2152.299232913181\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.587066  7.5733266 7.5521264 6.2651863 7.1718187 7.5784683]\n",
      "Reset environment\n",
      "Episode reward: -590.6018975973129\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.585614  7.5718265 7.550717  6.2638783 7.170398  7.577021 ]\n",
      "Reset environment\n",
      "Episode reward: 1415.316746354103\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.587822  7.5740333 7.552926  6.266311  7.1723485 7.5792274]\n",
      "Reset environment\n",
      "Episode reward: 1751.1317192316055\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.589584  7.5757785 7.55469   6.26827   7.173874  7.580988 ]\n",
      "Reset environment\n",
      "Episode reward: 2133.7866191864014\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.59176   7.5779753 7.5568533 6.270707  7.17578   7.5831685]\n",
      "Reset environment\n",
      "Episode reward: 1978.89345318079\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5937386 7.5799766 7.5588074 6.272952  7.177512  7.585148 ]\n",
      "Reset environment\n",
      "Episode reward: 1359.7135412096977\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.5958576 7.5821114 7.5609202 6.2753105 7.1793733 7.587269 ]\n",
      "Reset environment\n",
      "Episode reward: 4296.580367922783\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.600782  7.587039  7.5658503 6.2806377 7.1838217 7.592196 ]\n",
      "Reset environment\n",
      "Episode reward: 1455.5260264873505\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.601856  7.5882225 7.566828  6.281951  7.1847053 7.593277 ]\n",
      "Reset environment\n",
      "Episode reward: 2552.0098900794983\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.604998  7.5914836 7.569841  6.285444  7.1875577 7.5964217]\n",
      "Reset environment\n",
      "Episode reward: 1322.5916341543198\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.607059  7.5935435 7.571905  6.2877436 7.189366  7.5984817]\n",
      "Reset environment\n",
      "Episode reward: 5382.58315050602\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6133003 7.5997796 7.5781536 6.294467  7.194979  7.604724 ]\n",
      "Reset environment\n",
      "Episode reward: 4744.857500731945\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.618726  7.6052246 7.583544  6.3003426 7.199843  7.6101546]\n",
      "Reset environment\n",
      "Episode reward: 2616.4732085466385\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6214533 7.6079245 7.586308  6.303361  7.2022567 7.612882 ]\n",
      "Reset environment\n",
      "Episode reward: 1302.2770977020264\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6234655 7.6099424 7.5883117 6.305626  7.2040215 7.6148934]\n",
      "Reset environment\n",
      "Episode reward: 3849.1003201305866\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6277986 7.6143055 7.5925965 6.3103385 7.2078524 7.619224 ]\n",
      "Reset environment\n",
      "Episode reward: 1216.356111049652\n",
      "Total Steps: 40\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6289287 7.6154494 7.5937037 6.3116164 7.208838  7.620353 ]\n",
      "Reset environment\n",
      "Episode reward: 1817.9833726882935\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6306787 7.6172924 7.5953603 6.3136    7.210315  7.6221085]\n",
      "Reset environment\n",
      "Episode reward: 6148.115233838558\n",
      "Total Steps: 211\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.637831  7.6244445 7.602503  6.321305  7.2167826 7.6292734]\n",
      "Reset environment\n",
      "Episode reward: 1377.283061504364\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6399717 7.6265917 7.6046424 6.3236704 7.2186747 7.631421 ]\n",
      "Reset environment\n",
      "Episode reward: -1224.6337478458881\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6368065 7.623692  7.60126   6.320599  7.2157464 7.628279 ]\n",
      "Reset environment\n",
      "Episode reward: 2552.0993335843086\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6395416 7.626415  7.604008  6.3235774 7.218212  7.6310115]\n",
      "Reset environment\n",
      "Episode reward: 2297.1492047309875\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6419196 7.6288304 7.6063385 6.3262196 7.2202935 7.6333866]\n",
      "Reset environment\n",
      "Episode reward: -683.8191920518875\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6401653 7.6270747 7.604594  6.3245406 7.218653  7.6316366]\n",
      "Reset environment\n",
      "Episode reward: 1317.3728596568108\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.642215  7.629136  7.6066337 6.326829  7.2204432 7.633686 ]\n",
      "Reset environment\n",
      "Episode reward: 4267.5626455545425\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6468787 7.6337976 7.611302  6.3318934 7.224637  7.6383576]\n",
      "Reset environment\n",
      "Episode reward: 3170.9563322998583\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.650281  7.6373034 7.6145945 6.3356686 7.22752   7.6417665]\n",
      "Reset environment\n",
      "Episode reward: 3364.4837653934956\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.653885  7.6408334 7.618273  6.3396072 7.23064   7.645375 ]\n",
      "Reset environment\n",
      "Episode reward: 2006.2247849106789\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.655973  7.64291   7.62037   6.3419285 7.2324724 7.647464 ]\n",
      "Reset environment\n",
      "Episode reward: 3444.977384120226\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6597557 7.6467276 7.6241174 6.346083  7.235821  7.6512494]\n",
      "Reset environment\n",
      "Episode reward: 1787.081741988659\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6623735 7.649348  7.6267486 6.348981  7.2381516 7.6538715]\n",
      "Reset environment\n",
      "Episode reward: 1384.0900509357452\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6645246 7.6514935 7.6289067 6.351352  7.240046  7.656023 ]\n",
      "Reset environment\n",
      "Episode reward: 4210.810245990753\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6692677 7.6562448 7.633655  6.3565006 7.244318  7.6607695]\n",
      "Reset environment\n",
      "Episode reward: 1863.050080448389\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.671188  7.658123  7.635609  6.3585787 7.246029  7.662687 ]\n",
      "Reset environment\n",
      "Episode reward: 3388.1795821785927\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6748033 7.661674  7.639301  6.362534  7.2492285 7.66632  ]\n",
      "Reset environment\n",
      "Episode reward: 1501.7509328126907\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6765    7.663492  7.6408696 6.3644533 7.2507706 7.668018 ]\n",
      "Reset environment\n",
      "Episode reward: 2099.4207760095596\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.679492  7.66648   7.643862  6.3677363 7.253448  7.671008 ]\n",
      "Reset environment\n",
      "Episode reward: 4413.198837459087\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6844754 7.6714587 7.648858  6.3731084 7.2579513 7.6759872]\n",
      "Reset environment\n",
      "Episode reward: -680.9000076055527\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.68276  7.669739 7.647159 6.37126  7.256392 7.674278]\n",
      "Reset environment\n",
      "Episode reward: 2357.0426738262177\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.685259  7.672246  7.6496506 6.373997  7.258625  7.6767764]\n",
      "Reset environment\n",
      "Episode reward: 1869.2319185733795\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.687334  7.6741443 7.6518917 6.3763604 7.260451  7.678856 ]\n",
      "Reset environment\n",
      "Episode reward: 2832.0609775185585\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6902905 7.6770644 7.654883  6.3796024 7.2630787 7.681814 ]\n",
      "Reset environment\n",
      "Episode reward: 4038.4531541764736\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.6947846 7.6815333 7.659407  6.3844976 7.2671227 7.6863146]\n",
      "Reset environment\n",
      "Episode reward: 2249.4342018961906\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.697079  7.683794  7.6617517 6.387008  7.2691483 7.688619 ]\n",
      "Reset environment\n",
      "Episode reward: 3526.1161672770977\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7006783 7.6872478 7.665496  6.3910303 7.2721844 7.6922293]\n",
      "Reset environment\n",
      "Episode reward: 2120.63673889637\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.703692  7.6902566 7.6685214 6.3943563 7.274868  7.695243 ]\n",
      "Reset environment\n",
      "Episode reward: 2702.6643286943436\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7066245 7.693184  7.671455  6.397549  7.277491  7.6981726]\n",
      "Reset environment\n",
      "Episode reward: 4050.8974376916885\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.711177  7.6977363 7.6760154 6.4024987 7.2816243 7.702721 ]\n",
      "Reset environment\n",
      "Episode reward: -1142.1732195019722\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.708669  7.695021  7.6737204 6.3998933 7.2794237 7.7002225]\n",
      "Reset environment\n",
      "Episode reward: 4362.678846359253\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.713615  7.6999454 7.678684  6.405215  7.283914  7.7051735]\n",
      "Reset environment\n",
      "Episode reward: 3702.95201292634\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.717548  7.703749  7.682739  6.4095454 7.287341  7.7091002]\n",
      "Reset environment\n",
      "Episode reward: 2792.366513520479\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.720523  7.7067723 7.6856594 6.412828  7.2899694 7.712076 ]\n",
      "Reset environment\n",
      "Episode reward: 2525.334604680538\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7240367 7.7102976 7.6891675 6.4166627 7.2931285 7.7155933]\n",
      "Reset environment\n",
      "Episode reward: 1604.9354335665703\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.725701  7.7118535 7.690944  6.4183874 7.2946134 7.717272 ]\n",
      "Reset environment\n",
      "Episode reward: 1576.282941699028\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7271953 7.7131715 7.6926203 6.420159  7.295978  7.7187696]\n",
      "Reset environment\n",
      "Episode reward: 3775.3579155802727\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.731383  7.7173367 7.696835  6.4247026 7.2997565 7.7229595]\n",
      "Reset environment\n",
      "Episode reward: 5593.030333518982\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.737733  7.723665  7.703206  6.431577  7.305455  7.7293134]\n",
      "Reset environment\n",
      "Episode reward: 1377.0873107910156\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7398324 7.7257576 7.705322  6.433907  7.3073063 7.731414 ]\n",
      "Reset environment\n",
      "Episode reward: 1393.4521032571793\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7419643 7.7278895 7.7074504 6.4362674 7.3091855 7.7335505]\n",
      "Reset environment\n",
      "Episode reward: 2009.9273346662521\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.74482   7.730746  7.710301  6.439432  7.3117194 7.73641  ]\n",
      "Reset environment\n",
      "Episode reward: 1331.9509224891663\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7468414 7.7327795 7.712324  6.4416995 7.3135023 7.7384357]\n",
      "Reset environment\n",
      "Episode reward: 4532.352758347988\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.751756  7.7377005 7.7172427 6.447044  7.317891  7.7433486]\n",
      "Reset environment\n",
      "Episode reward: 5816.0621364712715\n",
      "Total Steps: 200\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7583876 7.7443385 7.723857  6.45417   7.3239007 7.749988 ]\n",
      "Reset environment\n",
      "Episode reward: 2127.2616893053055\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7614083 7.747362  7.7268734 6.457464  7.3266096 7.7530103]\n",
      "Reset environment\n",
      "Episode reward: 2119.925742685795\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7643943 7.7503495 7.729849  6.460745  7.3292737 7.7559958]\n",
      "Reset environment\n",
      "Episode reward: 1599.4180135726929\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.766775  7.7527375 7.732219  6.463358  7.3313813 7.7583776]\n",
      "Reset environment\n",
      "Episode reward: 1138.7271785736084\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7677917 7.753768  7.73322   6.4645147 7.3322716 7.7593923]\n",
      "Reset environment\n",
      "Episode reward: 2364.322308808565\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.770393  7.7562165 7.7359653 6.467343  7.3344088 7.762008 ]\n",
      "Reset environment\n",
      "Episode reward: 2253.1744971871376\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7735405 7.7593527 7.7391195 6.470798  7.3372283 7.7651563]\n",
      "Reset environment\n",
      "Episode reward: 2360.541746556759\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7759633 7.761788  7.7415323 6.473484  7.339365  7.767576 ]\n",
      "Reset environment\n",
      "Episode reward: -681.723029255867\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7742815 7.760102  7.739857  6.4718785 7.3377414 7.765901 ]\n",
      "Reset environment\n",
      "Episode reward: 1245.006856918335\n",
      "Total Steps: 40\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.77543   7.761237  7.7410164 6.4731674 7.3387647 7.767049 ]\n",
      "Reset environment\n",
      "Episode reward: 2480.753956615925\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.778885  7.7646904 7.7444687 6.4769278 7.3418565 7.770502 ]\n",
      "Reset environment\n",
      "Episode reward: 2732.219142436981\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7816916 7.767441  7.747341  6.480032  7.3443594 7.7733116]\n",
      "Reset environment\n",
      "Episode reward: 1348.7828826904297\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7837563 7.7695055 7.7494097 6.4823246 7.346184  7.7753787]\n",
      "Reset environment\n",
      "Episode reward: 926.9140632152557\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7843723 7.770268  7.7498817 6.4829617 7.3467126 7.776002 ]\n",
      "Reset environment\n",
      "Episode reward: 4610.3240532279015\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.789567  7.7754493 7.755097  6.48854   7.3514323 7.781202 ]\n",
      "Reset environment\n",
      "Episode reward: 1406.6049047708511\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.7917137 7.777595  7.757238  6.490905  7.3533306 7.783347 ]\n",
      "Reset environment\n",
      "Episode reward: 2784.2737896367908\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.794489  7.7804585 7.759915  6.4940157 7.3558054 7.7861247]\n",
      "Reset environment\n",
      "Episode reward: 1953.1964821219444\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.797276  7.783238  7.762709  6.497076  7.358275  7.7889047]\n",
      "Reset environment\n",
      "Episode reward: 1270.6242079734802\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.798547  7.784627  7.7638435 6.4984493 7.3594036 7.7901764]\n",
      "Reset environment\n",
      "Episode reward: 2341.0639880895615\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.800983  7.7870417 7.766299  6.5010977 7.3615913 7.792613 ]\n",
      "Reset environment\n",
      "Episode reward: 2053.760624587536\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.803899  7.789958  7.7692227 6.504302  7.364183  7.7955265]\n",
      "Reset environment\n",
      "Episode reward: 2638.5768897533417\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.806619  7.7926826 7.77194   6.5073185 7.366588  7.79825  ]\n",
      "Reset environment\n",
      "Episode reward: 3678.6991024017334\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.81066   7.7967296 7.7759724 6.511695  7.3702345 7.802293 ]\n",
      "Reset environment\n",
      "Episode reward: 4923.622249543667\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8161664 7.8022203 7.7815003 6.5176625 7.375208  7.807798 ]\n",
      "Reset environment\n",
      "Episode reward: 1966.6097860597074\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8183494 7.80427   7.783818  6.5200586 7.3771586 7.8099856]\n",
      "Reset environment\n",
      "Episode reward: 4193.30645275116\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.823003  7.8089395 7.7884574 6.525103  7.381361  7.81465  ]\n",
      "Reset environment\n",
      "Episode reward: 1962.913163304329\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8248944 7.810856  7.790327  6.527245  7.383017  7.816543 ]\n",
      "Reset environment\n",
      "Episode reward: 2004.7926976978779\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8269157 7.812851  7.792375  6.529426  7.3848324 7.818562 ]\n",
      "Reset environment\n",
      "Episode reward: 2262.10559630394\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.830061  7.815998  7.7955265 6.5328703 7.3876357 7.8217053]\n",
      "Reset environment\n",
      "Episode reward: 4832.130702286959\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8355136 7.821516  7.8009033 6.5387554 7.392532  7.8271575]\n",
      "Reset environment\n",
      "Episode reward: -779.209642469883\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8334084 7.819189  7.799031  6.5365887 7.3904185 7.8250666]\n",
      "Reset environment\n",
      "Episode reward: 4074.41998308897\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.837658  7.823434  7.8032923 6.5412726 7.394175  7.8293195]\n",
      "Reset environment\n",
      "Episode reward: 4100.625446796417\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8421946 7.827964  7.8078265 6.5461636 7.3982396 7.8338537]\n",
      "Reset environment\n",
      "Episode reward: 2330.9555289149284\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8446107 7.830382  7.8102307 6.5488205 7.4003916 7.836268 ]\n",
      "Reset environment\n",
      "Episode reward: 3654.5007032603025\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.848452  7.8343267 7.8139663 6.55305   7.403859  7.840107 ]\n",
      "Reset environment\n",
      "Episode reward: 1698.317011386156\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8500824 7.8359294 7.8156304 6.554854  7.405313  7.8417397]\n",
      "Reset environment\n",
      "Episode reward: 3788.685538828373\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.854172  7.8399997 7.81976   6.5593143 7.408984  7.845837 ]\n",
      "Reset environment\n",
      "Episode reward: 1013.7726328372955\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8550014 7.84082   7.8205886 6.5602884 7.4097037 7.8466673]\n",
      "Reset environment\n",
      "Episode reward: 4398.911924958229\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8597198 7.8455386 7.8253136 6.5653987 7.4139075 7.851394 ]\n",
      "Reset environment\n",
      "Episode reward: 3972.4512189030647\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8640876 7.849909  7.829673  6.5701575 7.4178367 7.855761 ]\n",
      "Reset environment\n",
      "Episode reward: 4429.366210222244\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.868838 7.854652 7.834425 6.575308 7.422093 7.860508]\n",
      "Reset environment\n",
      "Episode reward: 4775.802225291729\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8741603 7.859985  7.8397303 6.5810637 7.426889  7.865827 ]\n",
      "Reset environment\n",
      "Episode reward: 1849.893928349018\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.876766  7.86259   7.8423405 6.5839653 7.4291825 7.868432 ]\n",
      "Reset environment\n",
      "Episode reward: 5192.021264851093\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8825903 7.8684297 7.848141  6.5902357 7.4344687 7.87426  ]\n",
      "Reset environment\n",
      "Episode reward: 1817.1040505170822\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.884379  7.87023   7.8499064 6.59223   7.436057  7.8760505]\n",
      "Reset environment\n",
      "Episode reward: 2194.5032887756824\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8866053 7.872413  7.8521676 6.5946355 7.438034  7.878276 ]\n",
      "Reset environment\n",
      "Episode reward: 2207.223049044609\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.889689  7.875498  7.8552437 6.597983  7.440804  7.8813567]\n",
      "Reset environment\n",
      "Episode reward: 4082.2306073904037\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.894188  7.8799815 7.859749  6.6028657 7.4449077 7.885858 ]\n",
      "Reset environment\n",
      "Episode reward: 5268.615820169449\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8999104 7.8856874 7.8654885 6.609055  7.450009  7.891577 ]\n",
      "Reset environment\n",
      "Episode reward: 1880.0155926048756\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9013667 7.8869963 7.8670926 6.610881  7.451155  7.8930426]\n",
      "Reset environment\n",
      "Episode reward: -686.4711220264435\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.8996196 7.8852415 7.865352  6.609186  7.449509  7.8913007]\n",
      "Reset environment\n",
      "Episode reward: 1307.3211336135864\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.901577  7.887203  7.867314  6.611382  7.4512277 7.893261 ]\n",
      "Reset environment\n",
      "Episode reward: 3488.290197610855\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9051747 7.8908086 7.87091   6.6152987 7.454458  7.8968616]\n",
      "Reset environment\n",
      "Episode reward: 1355.8303437232971\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9072127 7.8928385 7.8729587 6.6175876 7.4562297 7.8989053]\n",
      "Reset environment\n",
      "Episode reward: -291.14116168022156\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9051747 7.8906236 7.8711033 6.6157174 7.454333  7.896886 ]\n",
      "Reset environment\n",
      "Episode reward: -1724.3599768579006\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9011965 7.88692   7.866914  6.611717  7.450683  7.892937 ]\n",
      "Reset environment\n",
      "Episode reward: 26.39765927195549\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9004016 7.885993  7.8662667 6.6109276 7.4500136 7.8921585]\n",
      "Reset environment\n",
      "Episode reward: 2027.11272585392\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.903234  7.888834  7.869094  6.6140323 7.452546  7.8949947]\n",
      "Reset environment\n",
      "Episode reward: 4489.311491012573\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9082007 7.893814  7.8740325 6.619405  7.4570293 7.8999586]\n",
      "Reset environment\n",
      "Episode reward: 2033.5295982956886\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.910261  7.895859  7.876097  6.6216793 7.4588428 7.9020176]\n",
      "Reset environment\n",
      "Episode reward: 1795.8176394104958\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9120097 7.8975596 7.8778925 6.623598  7.4603834 7.9037666]\n",
      "Reset environment\n",
      "Episode reward: 4727.0691484212875\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9172153 7.9027433 7.883118  6.629235  7.4651012 7.908976 ]\n",
      "Reset environment\n",
      "Episode reward: 1740.9757100343704\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.919709  7.9052444 7.885614  6.6319923 7.467309  7.911477 ]\n",
      "Reset environment\n",
      "Episode reward: 3757.4294775128365\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.923709  7.9092574 7.889583  6.636365  7.4708886 7.915477 ]\n",
      "Reset environment\n",
      "Episode reward: 4521.798993170261\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.928686  7.914263  7.8945117 6.641776  7.47533   7.920462 ]\n",
      "Reset environment\n",
      "Episode reward: 2869.4505515545607\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.932181  7.9177113 7.8980627 6.6455345 7.4785104 7.9239736]\n",
      "Reset environment\n",
      "Episode reward: 517.3808534145355\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9322023 7.917854  7.897963  6.6455097 7.4785776 7.9240017]\n",
      "Reset environment\n",
      "Episode reward: 1024.1337784864008\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.932738  7.9181943 7.898721  6.646107  7.4790945 7.924561 ]\n",
      "Reset environment\n",
      "Episode reward: 4365.565325319767\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9375134 7.9229555 7.9035144 6.651277  7.4833927 7.9293385]\n",
      "Reset environment\n",
      "Episode reward: 3011.8321991562843\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.940699  7.926149  7.9066973 6.6547456 7.486254  7.9325304]\n",
      "Reset environment\n",
      "Episode reward: 219.22330248355865\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.940173  7.9254947 7.9063015 6.654184  7.4858317 7.9320164]\n",
      "Reset environment\n",
      "Episode reward: 3179.6030019521713\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9435678 7.9289174 7.909659  6.6578865 7.48884   7.9354157]\n",
      "Reset environment\n",
      "Episode reward: 4090.8561424463987\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.948003  7.9334235 7.914016  6.6627154 7.4928308 7.939856 ]\n",
      "Reset environment\n",
      "Episode reward: 3514.5102276802063\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.951729 7.937124 7.91777  6.666783 7.496183 7.943587]\n",
      "Reset environment\n",
      "Episode reward: 1795.0013959407806\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.954281  7.9396763 7.920332  6.669581  7.4984555 7.9461436]\n",
      "Reset environment\n",
      "Episode reward: 3185.6311428546906\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.957557  7.9429455 7.923605  6.6731358 7.5013547 7.9494157]\n",
      "Reset environment\n",
      "Episode reward: 5060.269030094147\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9629583 7.94835   7.9290032 6.678967  7.5062013 7.9548163]\n",
      "Reset environment\n",
      "Episode reward: 3507.0417895317078\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.966564  7.951874  7.932694  6.682922  7.509325  7.9584255]\n",
      "Reset environment\n",
      "Episode reward: 5123.985227882862\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.97224   7.9575267 7.938392  6.689054  7.5144324 7.96411  ]\n",
      "Reset environment\n",
      "Episode reward: 1693.31256377697\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9738564 7.959157  7.939992  6.690857  7.515847  7.9657288]\n",
      "Reset environment\n",
      "Episode reward: 5087.123666346073\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.979541  7.9648438 7.9456606 6.6969633 7.5209913 7.9714127]\n",
      "Reset environment\n",
      "Episode reward: 1373.726204752922\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9815793 7.9668875 7.9477024 6.6992273 7.5227838 7.973452 ]\n",
      "Reset environment\n",
      "Episode reward: 2809.3173865675926\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.984549  7.969847  7.9506855 6.7024565 7.5254455 7.976423 ]\n",
      "Reset environment\n",
      "Episode reward: 5914.6035615205765\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.991195  7.976496  7.9573092 6.7095776 7.531402  7.9830685]\n",
      "Reset environment\n",
      "Episode reward: 6476.2852420806885\n",
      "Total Steps: 222\n",
      "Agent status: Game started\n",
      "Mean Q-values: [7.9984493 7.983752  7.964555  6.7173843 7.537966  7.990329 ]\n",
      "Reset environment\n",
      "Episode reward: 2416.214927792549\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.000803  7.986107  7.966899  6.7199693 7.5400867 7.992678 ]\n",
      "Reset environment\n",
      "Episode reward: 410.1671693325043\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.000987  7.9862757 7.967092  6.720164  7.5403175 7.9928584]\n",
      "Reset environment\n",
      "Episode reward: 1632.7866795659065\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.003323  7.9886136 7.9694257 6.7227416 7.542377  7.995195 ]\n",
      "Reset environment\n",
      "Episode reward: 4286.701786696911\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.007815  7.9931006 7.973931  6.7276216 7.5463777 7.9996924]\n",
      "Reset environment\n",
      "Episode reward: 2431.8271154761314\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.010217  7.9955034 7.976325  6.7302465 7.5485187 8.00209  ]\n",
      "Reset environment\n",
      "Episode reward: 6079.870992124081\n",
      "Total Steps: 228\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.017855  8.003133  7.983947  6.7383547 7.555441  8.009731 ]\n",
      "Reset environment\n",
      "Episode reward: 2153.022566616535\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.020005  8.005278  7.9861026 6.7407236 7.557325  8.011877 ]\n",
      "Reset environment\n",
      "Episode reward: 1672.958566904068\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.021547  8.00677   7.987697  6.7424293 7.558679  8.01342  ]\n",
      "Reset environment\n",
      "Episode reward: 1535.6046026051044\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.022955  8.008012  7.989251  6.7440276 7.559947  8.014834 ]\n",
      "Reset environment\n",
      "Episode reward: 4497.110888004303\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.027913  8.012967  7.994201  6.7493553 7.5644226 8.01979  ]\n",
      "Reset environment\n",
      "Episode reward: 2111.3492805957794\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.030796  8.015852  7.997076  6.75251   7.5670037 8.022672 ]\n",
      "Reset environment\n",
      "Episode reward: 1822.3471527695656\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.0333605 8.018413  7.99964   6.7553415 7.56928   8.025232 ]\n",
      "Reset environment\n",
      "Episode reward: 1363.8540099263191\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.0353775 8.020418  8.001672  6.7575755 7.571051  8.027251 ]\n",
      "Reset environment\n",
      "Episode reward: 2676.5315641760826\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.038041  8.023162  8.00425   6.7605357 7.5733786 8.029924 ]\n",
      "Reset environment\n",
      "Episode reward: 1002.4709286987782\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.038246  8.023224  8.004616  6.7610106 7.5734444 8.030148 ]\n",
      "Reset environment\n",
      "Episode reward: 2927.5112121999264\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.041297  8.026317  8.007608  6.7643642 7.576139  8.033193 ]\n",
      "Reset environment\n",
      "Episode reward: 1372.0525168776512\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.04333   8.028348  8.0096445 6.7666287 7.577918  8.03522  ]\n",
      "Reset environment\n",
      "Episode reward: 2266.409209251404\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.045556  8.030651  8.011794  6.7691355 7.5798526 8.037451 ]\n",
      "Reset environment\n",
      "Episode reward: 1864.9134120345116\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.047352  8.032449  8.013586  6.7711477 7.581431  8.039245 ]\n",
      "Reset environment\n",
      "Episode reward: 2580.9385046958923\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.049841  8.03493   8.016076  6.7738905 7.5836306 8.041731 ]\n",
      "Reset environment\n",
      "Episode reward: 3222.379016816616\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.053178  8.038233  8.019455  6.7775416 7.5866284 8.04507  ]\n",
      "Reset environment\n",
      "Episode reward: 4712.179054737091\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.058273  8.043319  8.024557  6.7830486 7.5912194 8.050159 ]\n",
      "Reset environment\n",
      "Episode reward: 4774.635419368744\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.063478  8.048501  8.029781  6.7886605 7.5959272 8.055365 ]\n",
      "Reset environment\n",
      "Episode reward: 1395.0701864659786\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.06457   8.049441  8.031038  6.789938  7.5968876 8.056468 ]\n",
      "Reset environment\n",
      "Episode reward: 187.79935204982758\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.063593 8.048657 8.02989  6.788963 7.596021 8.05551 ]\n",
      "Reset environment\n",
      "Episode reward: 2201.8957511782646\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.065796  8.050841  8.032113  6.7913713 7.5979886 8.057716 ]\n",
      "Reset environment\n",
      "Episode reward: 3606.6244562864304\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.06946   8.054501  8.035784  6.795363  7.6012425 8.061381 ]\n",
      "Reset environment\n",
      "Episode reward: -122.99085375666618\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.067887  8.05277   8.034396  6.793901  7.5998187 8.059829 ]\n",
      "Reset environment\n",
      "Episode reward: 2593.096777141094\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.070437  8.055287  8.036987  6.796708  7.6020837 8.062379 ]\n",
      "Reset environment\n",
      "Episode reward: 3800.63678920269\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.0745535 8.059393  8.041094  6.801148  7.6057568 8.066489 ]\n",
      "Reset environment\n",
      "Episode reward: 2040.351904809475\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.077358 8.062195 8.043899 6.804201 7.608264 8.069295]\n",
      "Reset environment\n",
      "Episode reward: 1200.8171119317412\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.077967 8.062971 8.044344 6.804968 7.608647 8.069923]\n",
      "Reset environment\n",
      "Episode reward: 741.193648815155\n",
      "Total Steps: 23\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.078494 8.063498 8.044875 6.805593 7.609096 8.070452]\n",
      "Reset environment\n",
      "Episode reward: 1871.9855403453112\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.080541  8.065645  8.046802  6.807863  7.6109586 8.0725   ]\n",
      "Reset environment\n",
      "Episode reward: 2843.1036992818117\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.083775  8.068978  8.049937  6.8114686 7.6138697 8.075746 ]\n",
      "Reset environment\n",
      "Episode reward: 2946.0401099920273\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.086854  8.072051  8.05302   6.8148074 7.616633  8.078825 ]\n",
      "Reset environment\n",
      "Episode reward: 1520.7384145855904\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.088275  8.0734825 8.054417  6.8164024 7.6178894 8.080243 ]\n",
      "Reset environment\n",
      "Episode reward: 3422.03911742568\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.091571  8.076665  8.057821  6.8201027 7.620701  8.083547 ]\n",
      "Reset environment\n",
      "Episode reward: 5272.448275446892\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.098155 8.08322  8.064424 6.827189 7.626645 8.090136]\n",
      "Reset environment\n",
      "Episode reward: 1885.4276078939438\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.100045  8.0850935 8.066318  6.8292522 7.6283283 8.092022 ]\n",
      "Reset environment\n",
      "Episode reward: 3443.5024768710136\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.103673  8.088747  8.06993   6.833186  7.6315947 8.095657 ]\n",
      "Reset environment\n",
      "Episode reward: 1317.3410552740097\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.105611  8.0907    8.071855  6.8353543 7.633285  8.097599 ]\n",
      "Reset environment\n",
      "Episode reward: 1252.8597877025604\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.106712  8.091777  8.072978  6.8365693 7.6342697 8.098701 ]\n",
      "Reset environment\n",
      "Episode reward: 3217.30450149253\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.109622  8.094833  8.075761  6.8399034 7.6367493 8.101631 ]\n",
      "Reset environment\n",
      "Episode reward: 5356.992322742939\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.115293  8.100511  8.081426  6.8460402 7.641862  8.107297 ]\n",
      "Reset environment\n",
      "Episode reward: 5122.05414301157\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.120887  8.106099  8.087026  6.8520517 7.6469116 8.1129   ]\n",
      "Reset environment\n",
      "Episode reward: 3018.0939306914806\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.123976  8.109235  8.090052  6.8554626 7.649653  8.115987 ]\n",
      "Reset environment\n",
      "Episode reward: 2248.944305598736\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.12701   8.112283  8.093076  6.8587775 7.652374  8.119022 ]\n",
      "Reset environment\n",
      "Episode reward: 1855.7214183807373\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.128829  8.1140785 8.094912  6.8607855 7.6539946 8.120837 ]\n",
      "Reset environment\n",
      "Episode reward: 4247.612525224686\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.13337   8.118604  8.0994625 6.8656845 7.6581    8.125379 ]\n",
      "Reset environment\n",
      "Episode reward: 2175.658483579755\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.135463  8.120649  8.101595  6.867973  7.6599436 8.127475 ]\n",
      "Reset environment\n",
      "Episode reward: 5612.988341510296\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.141656  8.126845  8.107775  6.8745975 7.6655273 8.133672 ]\n",
      "Reset environment\n",
      "Episode reward: 3902.748316526413\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.14566  8.130958 8.111671 6.878996 7.669168 8.137685]\n",
      "Reset environment\n",
      "Episode reward: 4950.636869430542\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.151027  8.136335  8.117013  6.8847585 7.6740127 8.143058 ]\n",
      "Reset environment\n",
      "Episode reward: -583.229373883456\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.149545 8.134815 8.115572 6.883388 7.672612 8.141577]\n",
      "Reset environment\n",
      "Episode reward: 3015.9413462281227\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.152955  8.138261  8.118949  6.887041  7.6758137 8.144996 ]\n",
      "Reset environment\n",
      "Episode reward: -614.9056398868561\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.151434  8.136799  8.117369  6.8853564 7.674484  8.1434765]\n",
      "Reset environment\n",
      "Episode reward: 4333.018810927868\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.156059  8.14142   8.121995  6.8903594 7.678646  8.148107 ]\n",
      "Reset environment\n",
      "Episode reward: 4022.204707041383\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.160245  8.145545  8.126253  6.894916  7.6824245 8.152301 ]\n",
      "Reset environment\n",
      "Episode reward: 4981.173991322517\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.165647  8.150971  8.131644  6.9007187 7.6873136 8.157707 ]\n",
      "Reset environment\n",
      "Episode reward: 1853.579119861126\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.16749   8.15281   8.133492  6.902746  7.6889486 8.15955  ]\n",
      "Reset environment\n",
      "Episode reward: 1085.724347114563\n",
      "Total Steps: 40\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.16837  8.153676 8.13439  6.903746 7.689721 8.16043 ]\n",
      "Reset environment\n",
      "Episode reward: 3640.4556299746037\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.172217  8.157527  8.138208  6.907948  7.6931524 8.16427  ]\n",
      "Reset environment\n",
      "Episode reward: 2456.769321322441\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.175441  8.160754  8.141442  6.9114885 7.696026  8.167496 ]\n",
      "Reset environment\n",
      "Episode reward: 2283.538834154606\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.178506  8.16383   8.144499  6.9148417 7.6987724 8.170566 ]\n",
      "Reset environment\n",
      "Episode reward: 4276.533544182777\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.183078  8.168392  8.149076  6.9197836 7.7028937 8.175129 ]\n",
      "Reset environment\n",
      "Episode reward: 3263.100666195154\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.186635 8.171892 8.152688 6.923612 7.706186 8.178695]\n",
      "Reset environment\n",
      "Episode reward: 4710.257600784302\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.191515  8.176777  8.157573  6.9288993 7.710545  8.183575 ]\n",
      "Reset environment\n",
      "Episode reward: 1954.636530995369\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.193338  8.178586  8.159405  6.9309416 7.7121506 8.185397 ]\n",
      "Reset environment\n",
      "Episode reward: 4363.628209531307\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.197981  8.183246  8.164033  6.935953  7.7163334 8.190042 ]\n",
      "Reset environment\n",
      "Episode reward: 3428.859652400017\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.201509  8.186797  8.1675415 6.939801  7.719484  8.193574 ]\n",
      "Reset environment\n",
      "Episode reward: 1888.5774828791618\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.204096  8.189391  8.170135  6.9426327 7.7217975 8.196166 ]\n",
      "Reset environment\n",
      "Episode reward: 3128.2617961764336\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.206981  8.192358  8.172943  6.945871  7.7243543 8.199052 ]\n",
      "Reset environment\n",
      "Episode reward: 2073.1094601154327\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.208912  8.19423   8.1749325 6.9480033 7.726046  8.200986 ]\n",
      "Reset environment\n",
      "Episode reward: 3920.0581614375114\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.213014  8.198307  8.179062  6.952454  7.7297444 8.205093 ]\n",
      "Reset environment\n",
      "Episode reward: 2523.295367288403\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.215173  8.200399  8.181293  6.9549303 7.731402  8.207262 ]\n",
      "Reset environment\n",
      "Episode reward: 1405.919416308403\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.216434  8.2016325 8.182582  6.956325  7.732524  8.208524 ]\n",
      "Reset environment\n",
      "Episode reward: 5408.660206615925\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.222285  8.207463  8.18847   6.9626236 7.7378464 8.214387 ]\n",
      "Reset environment\n",
      "Episode reward: 2295.1969371438026\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.225355  8.210542  8.1915245 6.965971  7.740593  8.217456 ]\n",
      "Reset environment\n",
      "Episode reward: 2366.913040101528\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.227533  8.212836  8.193604  6.9684534 7.7424545 8.219635 ]\n",
      "Reset environment\n",
      "Episode reward: 3269.3453086316586\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.230553  8.215747  8.196739  6.971847  7.7449503 8.22266  ]\n",
      "Reset environment\n",
      "Episode reward: 1041.3302018940449\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.230893 8.2159   8.197288 6.972417 7.74528  8.223018]\n",
      "Reset environment\n",
      "Episode reward: 562.1026730537415\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.230297  8.215129  8.196892  6.9720745 7.7446995 8.2224455]\n",
      "Reset environment\n",
      "Episode reward: 1490.9738350510597\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.232399  8.217223  8.198992  6.9744086 7.7465434 8.224541 ]\n",
      "Reset environment\n",
      "Episode reward: 4942.095247209072\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.23764   8.222451  8.204256  6.9800873 7.7512636 8.229792 ]\n",
      "Reset environment\n",
      "Episode reward: 2323.577434360981\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.239947  8.224771  8.206556  6.9826155 7.753319  8.2321005]\n",
      "Reset environment\n",
      "Episode reward: 128.20070433616638\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.239002  8.223684  8.205768  6.9817004 7.752515  8.23117  ]\n",
      "Reset environment\n",
      "Episode reward: 5853.328119814396\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.24514   8.229821  8.211916  6.9883337 7.758044  8.237306 ]\n",
      "Reset environment\n",
      "Episode reward: 2364.5730172991753\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.248253  8.232943  8.21502   6.9917383 7.760831  8.24042  ]\n",
      "Reset environment\n",
      "Episode reward: 2092.6602804362774\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.25024   8.234899  8.2170315 6.993896  7.7625937 8.242405 ]\n",
      "Reset environment\n",
      "Episode reward: 1369.200615823269\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.252215 8.236867 8.219019 6.996086 7.76433  8.24438 ]\n",
      "Reset environment\n",
      "Episode reward: 4487.705397486687\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.256895 8.241534 8.22371  7.001188 7.768527 8.24906 ]\n",
      "Reset environment\n",
      "Episode reward: 4176.870999902487\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.261313 8.245995 8.228091 7.005981 7.772463 8.253481]\n",
      "Reset environment\n",
      "Episode reward: 1593.797881603241\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.262778  8.24745   8.229561  7.0076036 7.773767  8.254947 ]\n",
      "Reset environment\n",
      "Episode reward: 1362.3056559562683\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.264015  8.248683  8.230798  7.0089846 7.774854  8.256181 ]\n",
      "Reset environment\n",
      "Episode reward: 5381.523976266384\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.2697935 8.254468  8.236555  7.015203  7.7800784 8.261958 ]\n",
      "Reset environment\n",
      "Episode reward: 4677.44613212347\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.274778  8.25944   8.241567  7.020568  7.784609  8.2669525]\n",
      "Reset environment\n",
      "Episode reward: 2220.197302609682\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.276871  8.261499  8.243696  7.022872  7.7864633 8.269047 ]\n",
      "Reset environment\n",
      "Episode reward: 1877.3066861778498\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.278547  8.263257  8.245304  7.0247927 7.787885  8.270731 ]\n",
      "Reset environment\n",
      "Episode reward: 3995.127311348915\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.282741  8.26746   8.249491  7.0293407 7.791672  8.274932 ]\n",
      "Reset environment\n",
      "Episode reward: 1159.7751967906952\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.283617  8.268464  8.250242  7.0302744 7.792501  8.27582  ]\n",
      "Reset environment\n",
      "Episode reward: 5290.617074966431\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.289295  8.274116  8.2559395 7.0364003 7.7976184 8.281498 ]\n",
      "Reset environment\n",
      "Episode reward: 1315.8030695915222\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.290418  8.275252  8.25704   7.0376964 7.7986007 8.282618 ]\n",
      "Reset environment\n",
      "Episode reward: 1702.7541505098343\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.291994 8.276809 8.258631 7.039461 7.799988 8.284194]\n",
      "Reset environment\n",
      "Episode reward: 1770.3516240119934\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.293628  8.278483  8.260214  7.0412936 7.801453  8.285826 ]\n",
      "Reset environment\n",
      "Episode reward: 2713.558840751648\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.2963295 8.281173  8.262932  7.0442514 7.803881  8.2885275]\n",
      "Reset environment\n",
      "Episode reward: 302.99430787563324\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.295957  8.280754  8.262618  7.0436244 7.8037357 8.288159 ]\n",
      "Reset environment\n",
      "Episode reward: 168.84506165981293\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.295141  8.279749  8.262     7.0428977 7.8029013 8.287363 ]\n",
      "Reset environment\n",
      "Episode reward: 3604.730010315776\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.298748  8.283464  8.265497  7.0468526 7.8061657 8.2909775]\n",
      "Reset environment\n",
      "Episode reward: 2609.09876267612\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.301213  8.285884  8.268008  7.0495715 7.808343  8.293435 ]\n",
      "Reset environment\n",
      "Episode reward: 3421.5163289010525\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.304923  8.289704  8.271612  7.0537143 7.811697  8.297157 ]\n",
      "Reset environment\n",
      "Episode reward: 4959.887101650238\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.310263  8.295064  8.276901  7.0594482 7.8164654 8.302494 ]\n",
      "Reset environment\n",
      "Episode reward: 2350.177131175995\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.312567  8.297396  8.27918   7.06197   7.8185086 8.304802 ]\n",
      "Reset environment\n",
      "Episode reward: 5793.103946149349\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.318739  8.303576  8.285325  7.0686603 7.8241444 8.310979 ]\n",
      "Reset environment\n",
      "Episode reward: 2106.688264787197\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.321532  8.3063755 8.28811   7.0717072 7.826639  8.313766 ]\n",
      "Reset environment\n",
      "Episode reward: 2187.307830989361\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.323686  8.308559  8.290226  7.0740905 7.8285656 8.315915 ]\n",
      "Reset environment\n",
      "Episode reward: 3871.208689928055\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.327613  8.312467  8.29418   7.0783763 7.8320966 8.319849 ]\n",
      "Reset environment\n",
      "Episode reward: 2058.7320897579193\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.329606  8.314436  8.296197  7.0805273 7.833884  8.321842 ]\n",
      "Reset environment\n",
      "Episode reward: 5301.050321936607\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.335114 8.319938 8.301721 7.086463 7.83883  8.327353]\n",
      "Reset environment\n",
      "Episode reward: 2040.9786593317986\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.337111 8.321929 8.303733 7.088658 7.840614 8.329353]\n",
      "Reset environment\n",
      "Episode reward: 1067.480808019638\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.337973  8.32279   8.3046    7.0896544 7.841366  8.330213 ]\n",
      "Reset environment\n",
      "Episode reward: 2041.678917258978\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.339892  8.324682  8.306553  7.0917473 7.843058  8.332132 ]\n",
      "Reset environment\n",
      "Episode reward: 717.7850326299667\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.339896 8.324852 8.30641  7.0918   7.843057 8.332149]\n",
      "Reset environment\n",
      "Episode reward: 3193.2433065772057\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.343539  8.328489  8.3100395 7.0957103 7.846335  8.335794 ]\n",
      "Reset environment\n",
      "Episode reward: 2081.9723225831985\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.3463335 8.331287  8.312832  7.098751  7.8488393 8.338588 ]\n",
      "Reset environment\n",
      "Episode reward: 3137.616910994053\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.349676  8.334739  8.316088  7.102544  7.8519044 8.34194  ]\n",
      "Reset environment\n",
      "Episode reward: 5533.2872332930565\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.356348  8.341391  8.322782  7.10975   7.8579373 8.348616 ]\n",
      "Reset environment\n",
      "Episode reward: 3345.5429720431566\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.35964   8.344767  8.325982  7.1133733 7.860906  8.351903 ]\n",
      "Reset environment\n",
      "Episode reward: 6259.190098702908\n",
      "Total Steps: 212\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.366362 8.351491 8.332706 7.120624 7.867014 8.358627]\n",
      "Reset environment\n",
      "Episode reward: 1860.7864225506783\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.368122 8.353265 8.334449 7.122561 7.868591 8.360386]\n",
      "Reset environment\n",
      "Episode reward: 5572.469631433487\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.374033  8.359182  8.340352  7.128947  7.873907  8.3663025]\n",
      "Reset environment\n",
      "Episode reward: 798.7919149398804\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.374339 8.359361 8.340796 7.129298 7.874193 8.366621]\n",
      "Reset environment\n",
      "Episode reward: 1376.4311260581017\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.376296 8.361323 8.342753 7.131471 7.875925 8.368581]\n",
      "Reset environment\n",
      "Episode reward: 4684.234898865223\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.381214 8.366222 8.347706 7.136767 7.880408 8.373507]\n",
      "Reset environment\n",
      "Episode reward: 3568.589072614908\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.384868  8.369941  8.351297  7.1407475 7.8836546 8.377162 ]\n",
      "Reset environment\n",
      "Episode reward: 1207.9533361792564\n",
      "Total Steps: 39\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.385901  8.37098   8.352327  7.1419144 7.8845625 8.378196 ]\n",
      "Reset environment\n",
      "Episode reward: 3432.445433974266\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.38938  8.374453 8.355815 7.145702 7.887676 8.38168 ]\n",
      "Reset environment\n",
      "Episode reward: 2962.9390379190445\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.3923025 8.377399  8.358713  7.1489143 7.89026   8.384605 ]\n",
      "Reset environment\n",
      "Episode reward: 3169.14942497015\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.395502  8.380595  8.361919  7.1523886 7.8931375 8.387809 ]\n",
      "Reset environment\n",
      "Episode reward: 1910.0295659899712\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.39806  8.383154 8.364486 7.155204 7.895415 8.390367]\n",
      "Reset environment\n",
      "Episode reward: 1958.26138818264\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.399894  8.384968  8.366334  7.1572213 7.8970428 8.3921995]\n",
      "Reset environment\n",
      "Episode reward: 2330.5924708247185\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.402068 8.387219 8.368426 7.159634 7.898957 8.394372]\n",
      "Reset environment\n",
      "Episode reward: 2876.190895050764\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.404984  8.390012  8.37147   7.162872  7.9013257 8.397306 ]\n",
      "Reset environment\n",
      "Episode reward: 2795.511847257614\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.407759  8.392807  8.374223  7.1659155 7.903809  8.40008  ]\n",
      "Reset environment\n",
      "Episode reward: 1950.8720321655273\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.409599  8.394632  8.37607   7.1679254 7.9054294 8.401921 ]\n",
      "Reset environment\n",
      "Episode reward: 2667.6569727659225\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.412243  8.397264  8.378718  7.1708007 7.9077973 8.404563 ]\n",
      "Reset environment\n",
      "Episode reward: 4648.635461032391\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.417087 8.402121 8.383538 7.176044 7.912148 8.409409]\n",
      "Reset environment\n",
      "Episode reward: 4701.167997777462\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.421987 8.407017 8.388454 7.181357 7.91658  8.414314]\n",
      "Reset environment\n",
      "Episode reward: 5426.645868360996\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.428532  8.413561  8.394982  7.188334  7.9224954 8.420859 ]\n",
      "Reset environment\n",
      "Episode reward: 6474.297578632832\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.435513  8.420542  8.401946  7.1957674 7.9288034 8.427846 ]\n",
      "Reset environment\n",
      "Episode reward: 2005.2813740372658\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.437415  8.42244   8.4038515 7.1978626 7.9304943 8.429747 ]\n",
      "Reset environment\n",
      "Episode reward: 4950.256063580513\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.442627  8.427662  8.409055  7.2034574 7.935218  8.434959 ]\n",
      "Reset environment\n",
      "Episode reward: 4380.716332376003\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.44717   8.43222   8.413575  7.2083607 7.9392977 8.439503 ]\n",
      "Reset environment\n",
      "Episode reward: 1500.3857454657555\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.448517  8.433543  8.414946  7.2098346 7.940501  8.440851 ]\n",
      "Reset environment\n",
      "Episode reward: 2054.965032696724\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.450467  8.435491  8.4169035 7.211977  7.9422317 8.442804 ]\n",
      "Reset environment\n",
      "Episode reward: 2371.9023556113243\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.452768  8.4378    8.419193  7.2144876 7.944287  8.445106 ]\n",
      "Reset environment\n",
      "Episode reward: 2101.878172338009\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.454786  8.439797  8.421231  7.2167034 7.9460864 8.447124 ]\n",
      "Reset environment\n",
      "Episode reward: 4854.183357477188\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.460673  8.445701  8.427103  7.2230263 7.9514194 8.453011 ]\n",
      "Reset environment\n",
      "Episode reward: 2001.1547399759293\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.46253   8.447562  8.428951  7.225095  7.953072  8.4548645]\n",
      "Reset environment\n",
      "Episode reward: 2438.877564162016\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.464725  8.449695  8.431196  7.227514  7.9549856 8.457057 ]\n",
      "Reset environment\n",
      "Episode reward: 3151.6568251252174\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.467847 8.452841 8.434296 7.230921 7.957748 8.460184]\n",
      "Reset environment\n",
      "Episode reward: 1812.1571148335934\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.469736  8.454837  8.436077  7.2330256 7.9594593 8.462077 ]\n",
      "Reset environment\n",
      "Episode reward: 192.79896783828735\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.469289  8.454484  8.435531  7.2324777 7.959072  8.461635 ]\n",
      "Reset environment\n",
      "Episode reward: 4429.474712193012\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.473872  8.459054  8.440134  7.2374067 7.9632096 8.46622  ]\n",
      "Reset environment\n",
      "Episode reward: 1412.3903983831406\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.475857  8.461049  8.442118  7.239592  7.9649644 8.468206 ]\n",
      "Reset environment\n",
      "Episode reward: 1498.318026959896\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.477141  8.462318  8.443417  7.2410493 7.966083  8.469492 ]\n",
      "Reset environment\n",
      "Episode reward: -684.4055467844009\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.475378  8.460556  8.44166   7.2391925 7.96446   8.467734 ]\n",
      "Reset environment\n",
      "Episode reward: 2193.7096921801567\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.477319  8.462436  8.443654  7.2413387 7.966117  8.469674 ]\n",
      "Reset environment\n",
      "Episode reward: 130.27049589157104\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.476483  8.461727  8.442704  7.2404118 7.9654603 8.468848 ]\n",
      "Reset environment\n",
      "Episode reward: 1221.246865451336\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.4781685 8.463426  8.444388  7.2423387 7.966922  8.47054  ]\n",
      "Reset environment\n",
      "Episode reward: 2085.6134914159775\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.4808855 8.466144  8.447107  7.245319  7.969349  8.473257 ]\n",
      "Reset environment\n",
      "Episode reward: 2190.8391935676336\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.482972  8.468049  8.449372  7.2477646 7.971052  8.475359 ]\n",
      "Reset environment\n",
      "Episode reward: 2045.6249263882637\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.485669  8.470752  8.452067  7.250732  7.9734607 8.47806  ]\n",
      "Reset environment\n",
      "Episode reward: 1884.6101118326187\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.488151  8.4732485 8.454545  7.2534537 7.9756794 8.480547 ]\n",
      "Reset environment\n",
      "Episode reward: 2217.985056400299\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.491049  8.476149  8.457449  7.2566013 7.97828   8.483449 ]\n",
      "Reset environment\n",
      "Episode reward: 5104.880380749702\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.496326  8.481391  8.462767  7.2623143 7.9830284 8.488721 ]\n",
      "Reset environment\n",
      "Episode reward: 1518.166172862053\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.497634  8.48269   8.464085  7.2637954 7.984176  8.49003  ]\n",
      "Reset environment\n",
      "Episode reward: 5471.983625769615\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.504176  8.489218  8.470636  7.2707944 7.9901056 8.496575 ]\n",
      "Reset environment\n",
      "Episode reward: 3047.6961575429887\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.507467  8.492623  8.473829  7.2744675 7.9931216 8.499874 ]\n",
      "Reset environment\n",
      "Episode reward: 1414.0896589066833\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.508725 8.49399  8.474993 7.275791 7.994249 8.501143]\n",
      "Reset environment\n",
      "Episode reward: 1475.6113793849945\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.510034  8.4953165 8.47627   7.277257  7.9954104 8.502448 ]\n",
      "Reset environment\n",
      "Episode reward: 1618.4297281233594\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.510954  8.496112  8.477318  7.278482  7.9960566 8.503377 ]\n",
      "Reset environment\n",
      "Episode reward: 4782.402903735638\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.515904 8.501048 8.482275 7.283811 8.000517 8.508329]\n",
      "Reset environment\n",
      "Episode reward: -138.15053284168243\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.515084  8.500298  8.481388  7.2827888 7.999791  8.507518 ]\n",
      "Reset environment\n",
      "Episode reward: 2263.6297900676727\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.517195  8.502418  8.483499  7.2851396 8.001653  8.509629 ]\n",
      "Reset environment\n",
      "Episode reward: 2801.437849968672\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.519815 8.504999 8.486164 7.288029 8.003981 8.512249]\n",
      "Reset environment\n",
      "Episode reward: 2068.559984445572\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.522497 8.507694 8.488841 7.290973 8.006375 8.514936]\n",
      "Reset environment\n",
      "Episode reward: 3275.3728310465813\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.525775  8.5109825 8.492103  7.294523  8.009311  8.518214 ]\n",
      "Reset environment\n",
      "Episode reward: 4621.643108665943\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.530553  8.515775  8.496877  7.2996764 8.013645  8.522992 ]\n",
      "Reset environment\n",
      "Episode reward: 3789.8616262078285\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.534388  8.519624  8.500688  7.3038244 8.017079  8.526829 ]\n",
      "Reset environment\n",
      "Episode reward: 5471.077511668205\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.540075  8.52531   8.506393  7.3099756 8.02222   8.532532 ]\n",
      "Reset environment\n",
      "Episode reward: 2690.590436935425\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.542694  8.52796   8.508975  7.3128414 8.024539  8.53515  ]\n",
      "Reset environment\n",
      "Episode reward: 2201.9084982275963\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.54554  8.530798 8.51183  7.315954 8.027091 8.537995]\n",
      "Reset environment\n",
      "Episode reward: 5535.33144646883\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.551283  8.536527  8.517585  7.3221645 8.032279  8.543735 ]\n",
      "Reset environment\n",
      "Episode reward: 4105.4161312282085\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.55546   8.540739  8.521726  7.3266892 8.035998  8.547914 ]\n",
      "Reset environment\n",
      "Episode reward: 2936.7543709278107\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.558334 8.543585 8.524628 7.329821 8.038592 8.55079 ]\n",
      "Reset environment\n",
      "Episode reward: 596.5431934595108\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.558516 8.543847 8.524736 7.329922 8.0388   8.55098 ]\n",
      "Reset environment\n",
      "Episode reward: 1533.0126812160015\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.559836  8.545147  8.526086  7.331372  8.039981  8.5522995]\n",
      "Reset environment\n",
      "Episode reward: 1944.853071987629\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.561602  8.54689   8.527882  7.3333    8.041553  8.5540695]\n",
      "Reset environment\n",
      "Episode reward: 3335.970501296222\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.564791  8.550035  8.531125  7.3368087 8.044408  8.557269 ]\n",
      "Reset environment\n",
      "Episode reward: 702.8339173793793\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.5649805 8.550331  8.531215  7.33689   8.044611  8.557467 ]\n",
      "Reset environment\n",
      "Episode reward: -281.9471483230591\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.563214  8.548737  8.529304  7.3350906 8.043059  8.55572  ]\n",
      "Reset environment\n",
      "Episode reward: -685.8330007791519\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.561464  8.546987  8.527555  7.3333173 8.041421  8.55398  ]\n",
      "Reset environment\n",
      "Episode reward: 1768.1534560322762\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.56304  8.548587 8.529107 7.335072 8.042816 8.555557]\n",
      "Reset environment\n",
      "Episode reward: 5028.977938950062\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.56897  8.554518 8.535034 7.341499 8.048188 8.561492]\n",
      "Reset environment\n",
      "Episode reward: 4859.002822339535\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.573942 8.559499 8.539977 7.346852 8.052673 8.566472]\n",
      "Reset environment\n",
      "Episode reward: 2001.9339982941747\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.575648  8.561072  8.541831  7.348779  8.0540085 8.568198 ]\n",
      "Reset environment\n",
      "Episode reward: 1909.4299938678741\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.57688   8.562371  8.542989  7.3502326 8.05496   8.569439 ]\n",
      "Reset environment\n",
      "Episode reward: 2095.3888724446297\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.579593 8.56509  8.545692 7.35321  8.05739  8.572153]\n",
      "Reset environment\n",
      "Episode reward: -122.3196192830801\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.578207  8.563574  8.544446  7.3519616 8.05608   8.570787 ]\n",
      "Reset environment\n",
      "Episode reward: 3476.6633744835854\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.581695  8.567064  8.547915  7.3557506 8.059185  8.574273 ]\n",
      "Reset environment\n",
      "Episode reward: 3889.8113836199045\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.5858    8.571212  8.551999  7.3602514 8.062975  8.578397 ]\n",
      "Reset environment\n",
      "Episode reward: 1486.0604875087738\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.5869055 8.57224   8.553181  7.361553  8.063928  8.579505 ]\n",
      "Reset environment\n",
      "Episode reward: 1705.290684401989\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.588487  8.573821  8.554761  7.3632956 8.065337  8.581082 ]\n",
      "Reset environment\n",
      "Episode reward: 4110.265993535519\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.592643 8.577986 8.558893 7.367799 8.069095 8.585236]\n",
      "Reset environment\n",
      "Episode reward: 2113.565943002701\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.595343  8.580701  8.561582  7.3707576 8.071514  8.587938 ]\n",
      "Reset environment\n",
      "Episode reward: 3140.0783421248198\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.59836   8.5837555 8.564556  7.374091  8.074202  8.590958 ]\n",
      "Reset environment\n",
      "Episode reward: 3172.9865462183952\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.601446  8.5868025 8.567678  7.37744   8.076986  8.594042 ]\n",
      "Reset environment\n",
      "Episode reward: 303.52467107772827\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.600865  8.58638   8.566957  7.3768473 8.076493  8.593475 ]\n",
      "Reset environment\n",
      "Episode reward: 3168.5342841744423\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.604039  8.58955   8.570146  7.3802676 8.079356  8.596655 ]\n",
      "Reset environment\n",
      "Episode reward: 4738.634884195402\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.608712 8.594278 8.574753 7.3854   8.083621 8.601327]\n",
      "Reset environment\n",
      "Episode reward: 3621.32890689373\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.612331  8.597946  8.578331  7.3893685 8.086833  8.60495  ]\n",
      "Reset environment\n",
      "Episode reward: 4392.020124971867\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.61679  8.602436 8.582759 7.394197 8.090832 8.609413]\n",
      "Reset environment\n",
      "Episode reward: 4879.16435199976\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.621798  8.607455  8.5877495 7.399565  8.095347  8.6144285]\n",
      "Reset environment\n",
      "Episode reward: 3969.378766655922\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.625762  8.611435  8.591697  7.4038773 8.098922  8.618396 ]\n",
      "Reset environment\n",
      "Episode reward: 3911.2887080311775\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.629653  8.615339  8.595576  7.4081054 8.102405  8.622296 ]\n",
      "Reset environment\n",
      "Episode reward: 2213.306046500802\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.631503  8.617278  8.597352  7.4102316 8.103949  8.62416  ]\n",
      "Reset environment\n",
      "Episode reward: 4941.016347169876\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.637303  8.623071  8.603155  7.4165115 8.109187  8.629961 ]\n",
      "Reset environment\n",
      "Episode reward: -817.4232221841812\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.634864 8.620865 8.600535 7.414078 8.106967 8.627552]\n",
      "Reset environment\n",
      "Episode reward: 3021.170534014702\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.63783   8.623847  8.603478  7.4173074 8.109624  8.630521 ]\n",
      "Reset environment\n",
      "Episode reward: 2239.7154905200005\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.639804 8.625822 8.605459 7.41947  8.111382 8.632498]\n",
      "Reset environment\n",
      "Episode reward: 1960.5212514996529\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.642331  8.6283655 8.607978  7.422235  8.113653  8.635029 ]\n",
      "Reset environment\n",
      "Episode reward: 2308.353761970997\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.64502   8.631076  8.610654  7.4251933 8.116064  8.637723 ]\n",
      "Reset environment\n",
      "Episode reward: 4222.514498233795\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.649267  8.6353245 8.614919  7.429775  8.119901  8.641987 ]\n",
      "Reset environment\n",
      "Episode reward: 4602.475367128849\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.653936 8.64     8.619577 7.434825 8.124125 8.646659]\n",
      "Reset environment\n",
      "Episode reward: 3881.0373646616936\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.657819  8.643866  8.623487  7.4390125 8.127658  8.650539 ]\n",
      "Reset environment\n",
      "Episode reward: 3204.0755855441093\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.660925  8.646952  8.626617  7.4423904 8.1304655 8.6536455]\n",
      "Reset environment\n",
      "Episode reward: 2996.56047103554\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.66375  8.649743 8.629487 7.44548  8.132995 8.656479]\n",
      "Reset environment\n",
      "Episode reward: 6408.853519022465\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.67023   8.65622   8.635966  7.4525023 8.138777  8.662962 ]\n",
      "Reset environment\n",
      "Episode reward: 4986.476267457008\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.675132  8.661117  8.640889  7.4577737 8.143213  8.667872 ]\n",
      "Reset environment\n",
      "Episode reward: 4392.210137009621\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.679568 8.665543 8.64535  7.462555 8.147247 8.672315]\n",
      "Reset environment\n",
      "Episode reward: 319.1112788915634\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.679173  8.665266  8.644842  7.4620395 8.146953  8.671927 ]\n",
      "Reset environment\n",
      "Episode reward: 2929.5870811343193\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.681963  8.668058  8.647633  7.4650974 8.149441  8.674717 ]\n",
      "Reset environment\n",
      "Episode reward: 5300.0991343557835\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.687369 8.673454 8.653051 7.470958 8.15437  8.680124]\n",
      "Reset environment\n",
      "Episode reward: 4237.648120239377\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.668749  8.654738  8.634488  7.4442935 8.137671  8.661483 ]\n",
      "Reset environment\n",
      "Episode reward: 1446.900203049183\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.670742 8.656727 8.636487 7.446483 8.139432 8.663477]\n",
      "Reset environment\n",
      "Episode reward: 3868.5267664194107\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.674569 8.660568 8.640305 7.450641 8.142868 8.667307]\n",
      "Reset environment\n",
      "Episode reward: 5450.282060146332\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.680205 8.666193 8.645955 7.456674 8.147958 8.672946]\n",
      "Reset environment\n",
      "Episode reward: 885.2570456266403\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.680241  8.666414  8.645829  7.4567523 8.147941  8.672997 ]\n",
      "Reset environment\n",
      "Episode reward: 1386.0082831978798\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.682144  8.66832   8.647736  7.4588428 8.149633  8.674903 ]\n",
      "Reset environment\n",
      "Episode reward: 4363.986752927303\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.686515 8.672704 8.652076 7.463551 8.153551 8.679265]\n",
      "Reset environment\n",
      "Episode reward: 4291.830187141895\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.690783 8.676949 8.656359 7.468174 8.157401 8.683536]\n",
      "Reset environment\n",
      "Episode reward: 4455.426466226578\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.695311  8.681474  8.660876  7.473045  8.1614685 8.688061 ]\n",
      "Reset environment\n",
      "Episode reward: -682.1223167181015\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.69348  8.679649 8.659049 7.471239 8.159804 8.686237]\n",
      "Reset environment\n",
      "Episode reward: 4250.5931742191315\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.69776   8.683926  8.663335  7.4758344 8.163675  8.6905155]\n",
      "Reset environment\n",
      "Episode reward: 1337.7911756038666\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.698726  8.684954  8.664242  7.4769607 8.164515  8.691482 ]\n",
      "Reset environment\n",
      "Episode reward: 3199.2062123417854\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.701833  8.688055  8.66735   7.4803452 8.167308  8.694594 ]\n",
      "Reset environment\n",
      "Episode reward: 896.1968576908112\n",
      "Total Steps: 28\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.702491  8.688717  8.668004  7.4811106 8.16788   8.6952505]\n",
      "Reset environment\n",
      "Episode reward: 2793.6418800354004\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.705193  8.69141   8.670715  7.4840417 8.170314  8.697952 ]\n",
      "Reset environment\n",
      "Episode reward: 3825.3240538835526\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.708974  8.695181  8.674517  7.488121  8.1737385 8.701737 ]\n",
      "Reset environment\n",
      "Episode reward: 2884.489326775074\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.711736 8.697954 8.677264 7.491124 8.176205 8.704499]\n",
      "Reset environment\n",
      "Episode reward: 4841.089788377285\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.716627 8.702847 8.682165 7.496414 8.180651 8.709392]\n",
      "Reset environment\n",
      "Episode reward: 2358.7001264691353\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.718795 8.704993 8.684354 7.49878  8.182595 8.711559]\n",
      "Reset environment\n",
      "Episode reward: 1185.995120048523\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.719695 8.705933 8.685217 7.499815 8.18339  8.712456]\n",
      "Reset environment\n",
      "Episode reward: 4117.426462173462\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.723857  8.71009   8.689371  7.5042887 8.187186  8.716618 ]\n",
      "Reset environment\n",
      "Episode reward: 2058.2935559749603\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.725697  8.711922  8.691218  7.5063415 8.188802  8.718457 ]\n",
      "Reset environment\n",
      "Episode reward: 3712.006386220455\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.729251 8.715481 8.694765 7.510212 8.191974 8.722004]\n",
      "Reset environment\n",
      "Episode reward: 5371.978261709213\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.734797  8.721016  8.700306  7.5161233 8.196998  8.727547 ]\n",
      "Reset environment\n",
      "Episode reward: 5439.7503272295\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.740373  8.7266    8.705858  7.5221004 8.202023  8.733117 ]\n",
      "Reset environment\n",
      "Episode reward: 1033.7451496124268\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.740985  8.72732   8.70637   7.5227156 8.202581  8.733735 ]\n",
      "Reset environment\n",
      "Episode reward: 1278.0312736034393\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.742188  8.728558  8.707534  7.524044  8.20368   8.7349415]\n",
      "Reset environment\n",
      "Episode reward: 1435.0097160935402\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.74337  8.729716 8.708741 7.525342 8.204737 8.736122]\n",
      "Reset environment\n",
      "Episode reward: 3486.836376145482\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.746449 8.732913 8.711712 7.528823 8.207342 8.739204]\n",
      "Reset environment\n",
      "Episode reward: 2704.3587579131126\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.749052 8.735609 8.714246 7.531889 8.209788 8.74182 ]\n",
      "Reset environment\n",
      "Episode reward: 2101.113268315792\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.751703  8.738262  8.716891  7.5347733 8.212163  8.744467 ]\n",
      "Reset environment\n",
      "Episode reward: 1734.9933478534222\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.753219  8.739746  8.718438  7.5364194 8.213519  8.74598  ]\n",
      "Reset environment\n",
      "Episode reward: 4093.2271860837936\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.757252  8.7437725 8.722505  7.5407715 8.21719   8.750023 ]\n",
      "Reset environment\n",
      "Episode reward: 1901.0404951758683\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.758506  8.745105  8.723689  7.5422897 8.218193  8.751289 ]\n",
      "Reset environment\n",
      "Episode reward: 4249.325967907906\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.762555  8.749157  8.727729  7.5466766 8.2218075 8.755334 ]\n",
      "Reset environment\n",
      "Episode reward: 1770.355806529522\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.764119  8.750731  8.729275  7.5484147 8.223201  8.756895 ]\n",
      "Reset environment\n",
      "Episode reward: 1851.1896043419838\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.765752  8.75233   8.730938  7.5502067 8.224656  8.758526 ]\n",
      "Reset environment\n",
      "Episode reward: 1891.1788041591644\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.768166  8.754745  8.733351  7.5528665 8.226805  8.76094  ]\n",
      "Reset environment\n",
      "Episode reward: 5387.823297083378\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.77366   8.760243  8.73883   7.5587444 8.2317505 8.766441 ]\n",
      "Reset environment\n",
      "Episode reward: 2416.6325083673\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.776023 8.762523 8.741278 7.561289 8.233865 8.76882 ]\n",
      "Reset environment\n",
      "Episode reward: 4460.472649097443\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.780441  8.766929  8.745722  7.5660715 8.237861  8.773246 ]\n",
      "Reset environment\n",
      "Episode reward: 1661.5650061368942\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.781886 8.768366 8.74717  7.567668 8.23913  8.774691]\n",
      "Reset environment\n",
      "Episode reward: -55.354793310165405\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.781106  8.767641  8.746346  7.5667143 8.238477  8.773918 ]\n",
      "Reset environment\n",
      "Episode reward: 3065.6895352602005\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.78381   8.770228  8.7491455 7.569736  8.240773  8.776622 ]\n",
      "Reset environment\n",
      "Episode reward: 1983.01573869586\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.785471  8.771847  8.750859  7.5715785 8.242244  8.778282 ]\n",
      "Reset environment\n",
      "Episode reward: 4774.953413963318\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.7902355 8.77662   8.755615  7.5767093 8.246548  8.78305  ]\n",
      "Reset environment\n",
      "Episode reward: 3920.2409199774265\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.794091  8.780504  8.759419  7.5808716 8.249977  8.786906 ]\n",
      "Reset environment\n",
      "Episode reward: 1331.3855806589127\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.795876  8.782287  8.761204  7.5828547 8.251536  8.788692 ]\n",
      "Reset environment\n",
      "Episode reward: 2898.3407204151154\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.798623  8.785067  8.763905  7.585871  8.2539625 8.79144  ]\n",
      "Reset environment\n",
      "Episode reward: -386.54726839321665\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.796843  8.78347   8.761964  7.5842285 8.252358  8.789676 ]\n",
      "Reset environment\n",
      "Episode reward: 2565.190236747265\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.799997  8.786614  8.765119  7.5876594 8.255203  8.792831 ]\n",
      "Reset environment\n",
      "Episode reward: 3851.1705892682076\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.803742  8.790341  8.768878  7.5917163 8.258572  8.7965765]\n",
      "Reset environment\n",
      "Episode reward: 3275.6696992218494\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.80683   8.793403  8.772001  7.5950875 8.261357  8.799665 ]\n",
      "Reset environment\n",
      "Episode reward: 2135.8143658041954\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.809516  8.7960825 8.774682  7.598001  8.263764  8.802354 ]\n",
      "Reset environment\n",
      "Episode reward: 2423.1329575181007\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.811764  8.798341  8.776922  7.6004496 8.265777  8.804606 ]\n",
      "Reset environment\n",
      "Episode reward: 2181.7927230596542\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.814482  8.801066  8.7796335 7.603428  8.268215  8.807327 ]\n",
      "Reset environment\n",
      "Episode reward: 1391.5818532705307\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.816346 8.802936 8.781504 7.605485 8.269864 8.809194]\n",
      "Reset environment\n",
      "Episode reward: 2977.0698249042034\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.81914  8.805814 8.784235 7.608774 8.272531 8.811997]\n",
      "Reset environment\n",
      "Episode reward: 1396.527204632759\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.821005 8.807675 8.786101 7.610832 8.27417  8.813862]\n",
      "Reset environment\n",
      "Episode reward: -682.1992726325989\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.819107  8.805752  8.784243  7.6091104 8.272428  8.811982 ]\n",
      "Reset environment\n",
      "Episode reward: 1685.0263693928719\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.821296 8.807941 8.786437 7.6115   8.27437  8.814171]\n",
      "Reset environment\n",
      "Episode reward: 5175.659125983715\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.827251  8.81388   8.792407  7.6178865 8.279742  8.820127 ]\n",
      "Reset environment\n",
      "Episode reward: 1728.9412461668253\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.828596  8.8152895 8.793687  7.6194263 8.280905  8.821475 ]\n",
      "Reset environment\n",
      "Episode reward: 1224.988044977188\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.8295555 8.816252  8.7946415 7.6205163 8.28173   8.822434 ]\n",
      "Reset environment\n",
      "Episode reward: -195.2869644165039\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.82795  8.814809 8.792911 7.618878 8.280316 8.820856]\n",
      "Reset environment\n",
      "Episode reward: 1977.3361498713493\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.829727  8.81658   8.794695  7.6208467 8.2819    8.822633 ]\n",
      "Reset environment\n",
      "Episode reward: 2863.2188979387283\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.832383 8.819218 8.797378 7.623749 8.284295 8.825293]\n",
      "Reset environment\n",
      "Episode reward: 1953.0754299461842\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.8340845 8.82079   8.799213  7.6256385 8.2857275 8.8270035]\n",
      "Reset environment\n",
      "Episode reward: 2440.9940915629268\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.83638   8.82316   8.801455  7.6283817 8.287943  8.829311 ]\n",
      "Reset environment\n",
      "Episode reward: 2353.0720202475786\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.838408  8.82526   8.8034115 7.630631  8.289759  8.831343 ]\n",
      "Reset environment\n",
      "Episode reward: 4499.228020787239\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.842787  8.829625  8.807812  7.6353884 8.293714  8.835718 ]\n",
      "Reset environment\n",
      "Episode reward: 2125.0234129428864\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.845435  8.832277  8.810472  7.6382794 8.296084  8.838371 ]\n",
      "Reset environment\n",
      "Episode reward: 3403.559444129467\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.848672  8.835493  8.813736  7.6418104 8.299007  8.841609 ]\n",
      "Reset environment\n",
      "Episode reward: 1893.7562423348427\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.850361  8.837184  8.81543   7.6436687 8.300505  8.8433   ]\n",
      "Reset environment\n",
      "Episode reward: 3235.5906747579575\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.853445 8.840279 8.818497 7.647018 8.303286 8.846386]\n",
      "Reset environment\n",
      "Episode reward: 4549.564924299717\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.857779 8.844619 8.822826 7.651696 8.307171 8.850719]\n",
      "Reset environment\n",
      "Episode reward: 2675.072094619274\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.860184  8.847022  8.825235  7.6543083 8.309305  8.853124 ]\n",
      "Reset environment\n",
      "Episode reward: 4232.237111449242\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.864346  8.851182  8.829366  7.6587873 8.313017  8.857277 ]\n",
      "Reset environment\n",
      "Episode reward: 2886.780322149396\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.866992 8.853794 8.832055 7.661687 8.315382 8.859926]\n",
      "Reset environment\n",
      "Episode reward: 989.8815796375275\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.867754 8.854636 8.832745 7.662476 8.31607  8.860697]\n",
      "Reset environment\n",
      "Episode reward: 2006.7006754279137\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.870266  8.857149  8.835255  7.6652255 8.31831   8.863211 ]\n",
      "Reset environment\n",
      "Episode reward: -137.19339907169342\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.868993 8.855998 8.833884 7.663987 8.317226 8.861952]\n",
      "Reset environment\n",
      "Episode reward: 1911.7663341760635\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.871398 8.858406 8.836288 7.666623 8.319374 8.86436 ]\n",
      "Reset environment\n",
      "Episode reward: 211.62147080898285\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.871347  8.858341  8.836234  7.666463  8.319371  8.8643055]\n",
      "Reset environment\n",
      "Episode reward: 2981.3585294932127\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.874161 8.86111  8.839095 7.669498 8.321902 8.867123]\n",
      "Reset environment\n",
      "Episode reward: 1664.0578846931458\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.876302  8.8632555 8.8412285 7.6718674 8.323797  8.869261 ]\n",
      "Reset environment\n",
      "Episode reward: 1916.2899653315544\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.878697  8.865655  8.843631  7.6745033 8.325926  8.871656 ]\n",
      "Reset environment\n",
      "Episode reward: 3650.574545800686\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.8821745 8.869095  8.847132  7.6783013 8.329058  8.87513  ]\n",
      "Reset environment\n",
      "Episode reward: 3340.177011653781\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.885233 8.872228 8.850125 7.681661 8.331828 8.878195]\n",
      "Reset environment\n",
      "Episode reward: 3417.442981094122\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.888393 8.875471 8.853202 7.685131 8.334701 8.881356]\n",
      "Reset environment\n",
      "Episode reward: 1772.7568650245667\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.889983  8.877057  8.854795  7.6868615 8.3361225 8.882943 ]\n",
      "Reset environment\n",
      "Episode reward: 1792.1810419559479\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.892266 8.879334 8.857087 7.689369 8.338161 8.885226]\n",
      "Reset environment\n",
      "Episode reward: 2841.9785902500153\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.894911 8.881964 8.859751 7.692248 8.340544 8.887875]\n",
      "Reset environment\n",
      "Episode reward: 3076.315024547279\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.89773  8.88474  8.862625 7.695339 8.343058 8.89069 ]\n",
      "Reset environment\n",
      "Episode reward: 3879.615529626608\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.90149  8.888471 8.866417 7.699422 8.346462 8.894458]\n",
      "Reset environment\n",
      "Episode reward: 1385.0011895895004\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.90331   8.890286  8.868239  7.7014327 8.348072  8.8962755]\n",
      "Reset environment\n",
      "Episode reward: 2074.1256699562073\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.905881 8.892851 8.870825 7.704245 8.350375 8.898849]\n",
      "Reset environment\n",
      "Episode reward: 2691.966088652611\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.908374 8.895355 8.87331  7.706952 8.352603 8.901343]\n",
      "Reset environment\n",
      "Episode reward: 1352.1672124266624\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.910163  8.897147  8.875101  7.7089434 8.354174  8.903136 ]\n",
      "Reset environment\n",
      "Episode reward: 1602.0854355096817\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.911505 8.89847  8.876452 7.710434 8.355364 8.904472]\n",
      "Reset environment\n",
      "Episode reward: 2822.615341424942\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.914128 8.901104 8.879074 7.713301 8.357731 8.907103]\n",
      "Reset environment\n",
      "Episode reward: 4769.9416608810425\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.918656 8.90564  8.88361  7.718194 8.361783 8.911634]\n",
      "Reset environment\n",
      "Episode reward: 3935.7282680869102\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.922489  8.909486  8.887419  7.7223244 8.365239  8.9154625]\n",
      "Reset environment\n",
      "Episode reward: 3670.749377131462\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.925976 8.912963 8.890919 7.726105 8.368391 8.918952]\n",
      "Reset environment\n",
      "Episode reward: -690.7907936573029\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.92419   8.91118   8.8891325 7.7241774 8.36677   8.917173 ]\n",
      "Reset environment\n",
      "Episode reward: 3188.9501473903656\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.927144 8.914159 8.892059 7.727419 8.369413 8.920128]\n",
      "Reset environment\n",
      "Episode reward: 2516.7244706749916\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.929404  8.916395  8.894337  7.7299037 8.371444  8.922384 ]\n",
      "Reset environment\n",
      "Episode reward: 3213.788336083293\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.932234  8.919289  8.897102  7.7330728 8.374034  8.925221 ]\n",
      "Reset environment\n",
      "Episode reward: 2521.032746732235\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.934512 8.921587 8.899361 7.735579 8.376039 8.927504]\n",
      "Reset environment\n",
      "Episode reward: 1410.1571830511093\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.936379  8.923458  8.901217  7.7376356 8.3776865 8.92937  ]\n",
      "Reset environment\n",
      "Episode reward: 3536.6858291625977\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.939779  8.926859  8.904607  7.7412963 8.38075   8.932774 ]\n",
      "Reset environment\n",
      "Episode reward: 2519.818920850754\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.942056 8.929113 8.90692  7.74377  8.382784 8.935058]\n",
      "Reset environment\n",
      "Episode reward: -351.0580245703459\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.94082  8.927878 8.905689 7.742522 8.381591 8.933827]\n",
      "Reset environment\n",
      "Episode reward: 1299.2241251766682\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.941869 8.929028 8.906637 7.743652 8.382558 8.93488 ]\n",
      "Reset environment\n",
      "Episode reward: 2698.2860313653946\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.944664  8.931816  8.909427  7.7466683 8.385086  8.937677 ]\n",
      "Reset environment\n",
      "Episode reward: 2186.4831652343273\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.946497  8.933745  8.911178  7.7487206 8.386693  8.939516 ]\n",
      "Reset environment\n",
      "Episode reward: 1774.132607460022\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.948754  8.9360075 8.913427  7.75119   8.388693  8.941774 ]\n",
      "Reset environment\n",
      "Episode reward: 1065.7285158634186\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.949522  8.936799  8.91416   7.7520657 8.38937   8.942536 ]\n",
      "Reset environment\n",
      "Episode reward: 2906.984299995005\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.952119 8.939365 8.916798 7.75492  8.39169  8.945139]\n",
      "Reset environment\n",
      "Episode reward: 4069.829272389412\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.956041  8.943272  8.920734  7.7591615 8.395231  8.949059 ]\n",
      "Reset environment\n",
      "Episode reward: 2002.6792771220207\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.958505  8.945739  8.923187  7.7618666 8.39743   8.951526 ]\n",
      "Reset environment\n",
      "Episode reward: 3896.62636038661\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.962258  8.9495125 8.926911  7.765936  8.400779  8.955284 ]\n",
      "Reset environment\n",
      "Episode reward: 3277.304714381695\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.965208 8.952464 8.929865 7.769158 8.403404 8.958241]\n",
      "Reset environment\n",
      "Episode reward: 2657.0000546872616\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.967673  8.954904  8.932355  7.7718153 8.405629  8.960702 ]\n",
      "Reset environment\n",
      "Episode reward: 1678.8806080818176\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.969128  8.956341  8.933822  7.773417  8.406931  8.9621525]\n",
      "Reset environment\n",
      "Episode reward: 4334.498960137367\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.973337 8.960542 8.938044 7.777956 8.410717 8.966365]\n",
      "Reset environment\n",
      "Episode reward: 4735.118422150612\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.977958  8.96514   8.942685  7.7829404 8.414899  8.970989 ]\n",
      "Reset environment\n",
      "Episode reward: 5554.476989984512\n",
      "Total Steps: 213\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.984161  8.971317  8.948925  7.7896404 8.420506  8.9772   ]\n",
      "Reset environment\n",
      "Episode reward: 1677.5512464046478\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.985532  8.972728  8.950254  7.7911735 8.4217205 8.97857  ]\n",
      "Reset environment\n",
      "Episode reward: 1807.5469416975975\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.98697   8.974134  8.951732  7.7927628 8.422988  8.980011 ]\n",
      "Reset environment\n",
      "Episode reward: 2135.3785778582096\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.988863 8.976004 8.953652 7.794814 8.424694 8.981902]\n",
      "Reset environment\n",
      "Episode reward: 2201.391573011875\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.991557  8.978692  8.956353  7.7977443 8.427113  8.984594 ]\n",
      "Reset environment\n",
      "Episode reward: 4467.934241473675\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.995915 8.983052 8.960702 7.802447 8.431054 8.98895 ]\n",
      "Reset environment\n",
      "Episode reward: 2354.8983845710754\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.997953 8.985177 8.962667 7.804738 8.432812 8.990998]\n",
      "Reset environment\n",
      "Episode reward: 1902.9528411626816\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [8.99959   8.986826  8.964289  7.806549  8.4342575 8.992634 ]\n",
      "Reset environment\n",
      "Episode reward: 3546.5112845003605\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.003035 8.99032  8.967672 7.810296 8.437333 8.996078]\n",
      "Reset environment\n",
      "Episode reward: 2565.102387070656\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.005421  8.992696  8.9700575 7.8128886 8.439468  8.998458 ]\n",
      "Reset environment\n",
      "Episode reward: 4203.49526566267\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.009295 8.996559 8.973948 7.817115 8.442919 9.002336]\n",
      "Reset environment\n",
      "Episode reward: 2106.4725021719933\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.011097  8.998338  8.975767  7.8191156 8.444522  9.004131 ]\n",
      "Reset environment\n",
      "Episode reward: 3359.900371968746\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.014248  9.001475  8.978938  7.8225355 8.447376  9.007287 ]\n",
      "Reset environment\n",
      "Episode reward: 3379.98424641788\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.017388  9.004671  8.982007  7.8259764 8.450192  9.01043  ]\n",
      "Reset environment\n",
      "Episode reward: 4507.6127172112465\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.021733 9.008995 8.986373 7.830684 8.454102 9.014778]\n",
      "Reset environment\n",
      "Episode reward: -421.67012782394886\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.0201435 9.007505  8.984704  7.829139  8.452716  9.0132   ]\n",
      "Reset environment\n",
      "Episode reward: 1747.3824598789215\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.021625 9.008995 8.986172 7.830797 8.454044 9.01468 ]\n",
      "Reset environment\n",
      "Episode reward: 5210.040128052235\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.026749 9.014127 8.991286 7.836292 8.458702 9.019807]\n",
      "Reset environment\n",
      "Episode reward: 4243.022702336311\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.030836 9.018223 8.995358 7.840692 8.462364 9.023895]\n",
      "Reset environment\n",
      "Episode reward: 4938.243958055973\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.035682 9.023076 9.000192 7.845875 8.466755 9.028741]\n",
      "Reset environment\n",
      "Episode reward: 2329.3831017315388\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.037688 9.02516  9.002131 7.848119 8.468538 9.030759]\n",
      "Reset environment\n",
      "Episode reward: 2179.152449309826\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.040333 9.0278   9.004786 7.851009 8.470902 9.033405]\n",
      "Reset environment\n",
      "Episode reward: 1769.589760541916\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.042555 9.030025 9.007008 7.853445 8.472878 9.035626]\n",
      "Reset environment\n",
      "Episode reward: 1907.123994231224\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.044602 9.032086 9.009038 7.855705 8.474749 9.037682]\n",
      "Reset environment\n",
      "Episode reward: 1330.3315362930298\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.046326  9.033807  9.010763  7.8576283 8.476252  9.039404 ]\n",
      "Reset environment\n",
      "Episode reward: 2131.5507240891457\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.048888  9.036378  9.013328  7.8604407 8.478552  9.04197  ]\n",
      "Reset environment\n",
      "Episode reward: 2445.486518263817\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.051076 9.038592 9.015486 7.862841 8.480503 9.044163]\n",
      "Reset environment\n",
      "Episode reward: 3334.5170527100563\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.054199  9.041733  9.018584  7.8662295 8.483282  9.04728  ]\n",
      "Reset environment\n",
      "Episode reward: 2341.986524283886\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.056252  9.043835  9.02058   7.8684916 8.485096  9.04933  ]\n",
      "Reset environment\n",
      "Episode reward: 2740.994501501322\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.058773  9.046387  9.023066  7.8712487 8.487332  9.051851 ]\n",
      "Reset environment\n",
      "Episode reward: -159.10523808002472\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.0577965 9.04546   9.022052  7.8700857 8.486491  9.050882 ]\n",
      "Reset environment\n",
      "Episode reward: 1803.716985821724\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.060017  9.047691  9.024272  7.8725305 8.488461  9.0531025]\n",
      "Reset environment\n",
      "Episode reward: 2501.2901733517647\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.062269  9.049943  9.026525  7.8750014 8.4904585 9.055355 ]\n",
      "Reset environment\n",
      "Episode reward: 4018.6986923217773\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.066057  9.053707  9.030337  7.8791056 8.493881  9.05914  ]\n",
      "Reset environment\n",
      "Episode reward: 2921.554757833481\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.068683  9.056393  9.032905  7.8819776 8.496248  9.061772 ]\n",
      "Reset environment\n",
      "Episode reward: 2065.89826002717\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.070374  9.058163  9.034521  7.8838634 8.497753  9.063469 ]\n",
      "Reset environment\n",
      "Episode reward: 1971.4098610281944\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.072066  9.059857  9.036211  7.8857355 8.499239  9.065162 ]\n",
      "Reset environment\n",
      "Episode reward: 2588.8339975476265\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.074393 9.062163 9.038556 7.888265 8.501328 9.06749 ]\n",
      "Reset environment\n",
      "Episode reward: 315.5848500728607\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.073556  9.061247  9.037785  7.8874583 8.500422  9.066651 ]\n",
      "Reset environment\n",
      "Episode reward: 5234.00456237793\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.078694 9.066379 9.042931 7.892971 8.505055 9.071796]\n",
      "Reset environment\n",
      "Episode reward: 2939.4870840907097\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.081417 9.069093 9.045663 7.895935 8.507482 9.07452 ]\n",
      "Reset environment\n",
      "Episode reward: 2010.5063798427582\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.082952  9.070598  9.04724   7.8976636 8.50881   9.076057 ]\n",
      "Reset environment\n",
      "Episode reward: 2475.345486551523\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.084809  9.072511  9.049049  7.8998137 8.510544  9.077923 ]\n",
      "Reset environment\n",
      "Episode reward: 2526.2032541781664\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.086797  9.074442  9.0511    7.9020767 8.512153  9.079925 ]\n",
      "Reset environment\n",
      "Episode reward: 1251.8187329769135\n",
      "Total Steps: 41\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.087765  9.07543   9.052042  7.9031687 8.513018  9.08089  ]\n",
      "Reset environment\n",
      "Episode reward: 4719.520289897919\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.092354  9.080051  9.056588  7.9081345 8.51712   9.08548  ]\n",
      "Reset environment\n",
      "Episode reward: 1006.2826298475266\n",
      "Total Steps: 33\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.093075 9.080781 9.057299 7.908962 8.517759 9.0862  ]\n",
      "Reset environment\n",
      "Episode reward: 2262.772366017103\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.094861  9.0825205 9.05913   7.910962  8.519233  9.087988 ]\n",
      "Reset environment\n",
      "Episode reward: 3051.5925628095865\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.097939  9.085715  9.06211   7.9143677 8.522102  9.091079 ]\n",
      "Reset environment\n",
      "Episode reward: 3997.2488172650337\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.101747  9.089512  9.065939  7.9184575 8.525561  9.094892 ]\n",
      "Reset environment\n",
      "Episode reward: 1789.0949147939682\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.103258  9.0910425 9.067436  7.9201336 8.5269    9.096404 ]\n",
      "Reset environment\n",
      "Episode reward: 2015.8659270703793\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.104951 9.092714 9.069159 7.921985 8.52841  9.098101]\n",
      "Reset environment\n",
      "Episode reward: 3441.2300513088703\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.108183  9.095987  9.072348  7.925495  8.531283  9.1013365]\n",
      "Reset environment\n",
      "Episode reward: 2396.400604814291\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.110218  9.0981    9.074316  7.9277525 8.533104  9.103375 ]\n",
      "Reset environment\n",
      "Episode reward: 2402.297360122204\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.112369  9.100257  9.076453  7.9301033 8.5350275 9.105529 ]\n",
      "Reset environment\n",
      "Episode reward: 2256.3889466524124\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.114373 9.10226  9.078459 7.932305 8.536827 9.107537]\n",
      "Reset environment\n",
      "Episode reward: 2139.8931297659874\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.116951 9.104834 9.081039 7.935112 8.539141 9.110115]\n",
      "Reset environment\n",
      "Episode reward: 5199.809418082237\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.122031 9.109912 9.086133 7.94055  8.543725 9.115201]\n",
      "Reset environment\n",
      "Episode reward: 2903.254019677639\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.124652  9.112521  9.088769  7.9434204 8.546087  9.117823 ]\n",
      "Reset environment\n",
      "Episode reward: 3810.3318175673485\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.128226  9.116081  9.092367  7.9472923 8.549328  9.1214   ]\n",
      "Reset environment\n",
      "Episode reward: -694.6510047912598\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.126359  9.114214  9.090509  7.9452806 8.547689  9.119542 ]\n",
      "Reset environment\n",
      "Episode reward: -352.31141421198845\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.125098  9.112956  9.089257  7.9440317 8.546462  9.118292 ]\n",
      "Reset environment\n",
      "Episode reward: 2199.4837884306908\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.127739  9.115593  9.091896  7.9469004 8.548832  9.120932 ]\n",
      "Reset environment\n",
      "Episode reward: 5615.224097311497\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.133235  9.121104  9.097367  7.9527807 8.553795  9.126426 ]\n",
      "Reset environment\n",
      "Episode reward: 4722.81227773428\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.137744  9.1256275 9.101859  7.9576316 8.557832  9.130941 ]\n",
      "Reset environment\n",
      "Episode reward: 3293.2621186971664\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.140722 9.128562 9.104897 7.960873 8.560484 9.133926]\n",
      "Reset environment\n",
      "Episode reward: 4572.443663299084\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.144882 9.132719 9.109068 7.965394 8.564191 9.138088]\n",
      "Reset environment\n",
      "Episode reward: 1934.9608709216118\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.146551  9.134398  9.110733  7.9672275 8.565687  9.139759 ]\n",
      "Reset environment\n",
      "Episode reward: 5984.537346065044\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.152454  9.140292  9.116654  7.9735255 8.571024  9.145662 ]\n",
      "Reset environment\n",
      "Episode reward: 2206.7558137774467\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.155109  9.142942  9.1193075 7.9764085 8.573402  9.148319 ]\n",
      "Reset environment\n",
      "Episode reward: 1412.248758494854\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.156919 9.144753 9.121121 7.978409 8.575008 9.150127]\n",
      "Reset environment\n",
      "Episode reward: 3431.8871006965637\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.16014   9.148008  9.124293  7.9819245 8.577854  9.153349 ]\n",
      "Reset environment\n",
      "Episode reward: 2877.850483953953\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.162808  9.150669  9.126972  7.9848175 8.580239  9.156017 ]\n",
      "Reset environment\n",
      "Episode reward: 417.13202595710754\n",
      "Total Steps: 15\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.162892 9.150754 9.127055 7.984982 8.580282 9.156099]\n",
      "Reset environment\n",
      "Episode reward: -682.3933826684952\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.161103  9.148974  9.125261  7.9831257 8.578621  9.154318 ]\n",
      "Reset environment\n",
      "Episode reward: 3976.939086139202\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.164707 9.152581 9.128867 7.987032 8.581831 9.15793 ]\n",
      "Reset environment\n",
      "Episode reward: 901.5457499027252\n",
      "Total Steps: 28\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.165325 9.153196 9.129491 7.987747 8.58237  9.158548]\n",
      "Reset environment\n",
      "Episode reward: 1038.0600545406342\n",
      "Total Steps: 32\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.166098  9.153969  9.130264  7.9886208 8.58305   9.159322 ]\n",
      "Reset environment\n",
      "Episode reward: 2148.0799118876457\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.168689 9.156552 9.132859 7.991438 8.58536  9.161907]\n",
      "Reset environment\n",
      "Episode reward: 3127.54892587231\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.171833  9.159786  9.13592   7.9948807 8.588302  9.165058 ]\n",
      "Reset environment\n",
      "Episode reward: 3479.786450713873\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.17503   9.163031  9.139072  7.9983706 8.591165  9.168262 ]\n",
      "Reset environment\n",
      "Episode reward: 4657.943653970957\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.179456  9.167453  9.143518  8.003162  8.5952015 9.17269  ]\n",
      "Reset environment\n",
      "Episode reward: 2821.891690760851\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.181888  9.1699505 9.145886  8.005834  8.597414  9.175129 ]\n",
      "Reset environment\n",
      "Episode reward: 1328.9802191257477\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.183586 9.171653 9.147585 8.00772  8.59891  9.176829]\n",
      "Reset environment\n",
      "Episode reward: 2458.7689499258995\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.185735  9.17389   9.149673  8.010346  8.600879  9.1789875]\n",
      "Reset environment\n",
      "Episode reward: 2057.181811928749\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.187519 9.175685 9.151445 8.012313 8.602476 9.180772]\n",
      "Reset environment\n",
      "Episode reward: 3410.232031226158\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.190683 9.178872 9.154568 8.01574  8.605287 9.183937]\n",
      "Reset environment\n",
      "Episode reward: 3402.0461017489433\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.193691  9.1818905 9.157579  8.019016  8.607959  9.186949 ]\n",
      "Reset environment\n",
      "Episode reward: 3825.0398563742638\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.197095 9.185295 9.16099  8.022716 8.610991 9.190359]\n",
      "Reset environment\n",
      "Episode reward: 1954.8618406057358\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.199461 9.187657 9.16337  8.025298 8.613101 9.19273 ]\n",
      "Reset environment\n",
      "Episode reward: 2278.5269255638123\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.2014675 9.189653  9.165382  8.0274725 8.614888  9.194736 ]\n",
      "Reset environment\n",
      "Episode reward: 2087.004900842905\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.203182 9.19134  9.167123 8.029363 8.616405 9.196449]\n",
      "Reset environment\n",
      "Episode reward: 5466.131655335426\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.208468  9.196609  9.172425  8.035041  8.6211605 9.201738 ]\n",
      "Reset environment\n",
      "Episode reward: -659.4058454334736\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.2064905 9.194409  9.17067   8.033057  8.619176  9.199772 ]\n",
      "Reset environment\n",
      "Episode reward: 4503.7283807992935\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.210734 9.198638 9.174927 8.037631 8.622992 9.204016]\n",
      "Reset environment\n",
      "Episode reward: 2617.6899505853653\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.213052 9.200973 9.177231 8.040154 8.625056 9.206342]\n",
      "Reset environment\n",
      "Episode reward: 4247.041966795921\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.217089 9.205018 9.181238 8.044484 8.628702 9.21037 ]\n",
      "Reset environment\n",
      "Episode reward: 1759.9446420669556\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.219229 9.207158 9.183373 8.046842 8.630601 9.212512]\n",
      "Reset environment\n",
      "Episode reward: 5124.800925731659\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.224121 9.212067 9.188247 8.052112 8.635027 9.217416]\n",
      "Reset environment\n",
      "Episode reward: 1635.2234023213387\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.225443 9.213395 9.189552 8.053592 8.636202 9.218735]\n",
      "Reset environment\n",
      "Episode reward: 2902.5194670557976\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.227995 9.215938 9.192123 8.0564   8.63851  9.221289]\n",
      "Reset environment\n",
      "Episode reward: 2877.2497784947045\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.23048  9.21838  9.194663 8.059124 8.640727 9.223776]\n",
      "Reset environment\n",
      "Episode reward: 2732.7480947822332\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.232997 9.220988 9.19711  8.062089 8.643168 9.226309]\n",
      "Reset environment\n",
      "Episode reward: 1777.9318337328732\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.23444   9.222401  9.198581  8.0636635 8.644443  9.227747 ]\n",
      "Reset environment\n",
      "Episode reward: 1673.3981106095016\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.235542 9.223368 9.199831 8.064935 8.645422 9.228866]\n",
      "Reset environment\n",
      "Episode reward: 2846.8362219929695\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.238092 9.225944 9.202352 8.067718 8.647685 9.231417]\n",
      "Reset environment\n",
      "Episode reward: 4025.4137131273746\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.241853 9.229724 9.206077 8.071784 8.651033 9.235172]\n",
      "Reset environment\n",
      "Episode reward: 2122.7497978806496\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.243671 9.231525 9.207909 8.073773 8.652664 9.236992]\n",
      "Reset environment\n",
      "Episode reward: -268.9679734110832\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.242196 9.230193 9.206314 8.07222  8.651393 9.235537]\n",
      "Reset environment\n",
      "Episode reward: 2045.20737606287\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.244624 9.232631 9.208742 8.074865 8.653578 9.237969]\n",
      "Reset environment\n",
      "Episode reward: 3078.3581592440605\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.247416 9.235464 9.211509 8.077924 8.656041 9.240765]\n",
      "Reset environment\n",
      "Episode reward: 289.9544669389725\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.2466135 9.234556  9.210845  8.077046  8.655395  9.239981 ]\n",
      "Reset environment\n",
      "Episode reward: 2128.598798573017\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.249139 9.237082 9.213376 8.079785 8.657668 9.242509]\n",
      "Reset environment\n",
      "Episode reward: 5930.624105036259\n",
      "Total Steps: 204\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.254797 9.24271  9.21907  8.08591  8.662756 9.248168]\n",
      "Reset environment\n",
      "Episode reward: 3165.3857731819153\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.257696 9.245596 9.221989 8.089047 8.665386 9.251066]\n",
      "Reset environment\n",
      "Episode reward: 2251.2594107985497\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.260319 9.248209 9.224622 8.091911 8.667746 9.253681]\n",
      "Reset environment\n",
      "Episode reward: 5610.470329403877\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.26574  9.253634 9.230033 8.097702 8.672673 9.259104]\n",
      "Reset environment\n",
      "Episode reward: 3898.650947332382\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.269337 9.257245 9.233615 8.101604 8.675883 9.262706]\n",
      "Reset environment\n",
      "Episode reward: 2179.2781891822815\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.27116   9.2590475 9.235461  8.103596  8.677504  9.264533 ]\n",
      "Reset environment\n",
      "Episode reward: 2298.137817978859\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.2738495 9.261736  9.238158  8.106521  8.679916  9.267227 ]\n",
      "Reset environment\n",
      "Episode reward: -618.0865507125854\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.272319 9.26021  9.236626 8.104607 8.678582 9.265703]\n",
      "Reset environment\n",
      "Episode reward: 1911.5931134223938\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.27372  9.261713 9.237935 8.106201 8.67982  9.267108]\n",
      "Reset environment\n",
      "Episode reward: 1330.5339255332947\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.274739 9.262705 9.23898  8.107333 8.680727 9.268129]\n",
      "Reset environment\n",
      "Episode reward: 1008.0110256671906\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.275454 9.26345  9.239654 8.108049 8.681428 9.26885 ]\n",
      "Reset environment\n",
      "Episode reward: 1475.506409585476\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.276619 9.264703 9.240734 8.109242 8.68252  9.270025]\n",
      "Reset environment\n",
      "Episode reward: 2881.549936625175\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.252867 9.241396 9.216592 8.081264 8.663127 9.246239]\n",
      "Reset environment\n",
      "Episode reward: 1499.7393044829369\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.25377  9.242396 9.217421 8.082158 8.663893 9.247161]\n",
      "Reset environment\n",
      "Episode reward: 4404.948544949293\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.257848 9.246457 9.22151  8.086578 8.667585 9.251236]\n",
      "Reset environment\n",
      "Episode reward: 2896.5522035732865\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.2607155 9.249438  9.224276  8.089744  8.67026   9.254116 ]\n",
      "Reset environment\n",
      "Episode reward: 3404.7018314301968\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.263727 9.252526 9.227202 8.093047 8.672988 9.257136]\n",
      "Reset environment\n",
      "Episode reward: 4088.0960387587547\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.267519 9.256301 9.231008 8.097121 8.676418 9.260928]\n",
      "Reset environment\n",
      "Episode reward: 531.0984383821487\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.267395 9.256257 9.230811 8.096901 8.676367 9.260813]\n",
      "Reset environment\n",
      "Episode reward: 3008.9190868139267\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.27006  9.25891  9.233473 8.099808 8.67877  9.263479]\n",
      "Reset environment\n",
      "Episode reward: 2201.362958729267\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.271949 9.260785 9.235382 8.101869 8.680467 9.265367]\n",
      "Reset environment\n",
      "Episode reward: 57.598533153533936\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.271342 9.260149 9.234804 8.100964 8.680052 9.264764]\n",
      "Reset environment\n",
      "Episode reward: 2833.6715744137764\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.273818 9.262609 9.237296 8.103655 8.682275 9.267241]\n",
      "Reset environment\n",
      "Episode reward: 146.4663017988205\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.272686 9.261634 9.236045 8.102517 8.681322 9.266129]\n",
      "Reset environment\n",
      "Episode reward: 2299.90373390913\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.274681  9.263611  9.23805   8.1046915 8.683113  9.26812  ]\n",
      "Reset environment\n",
      "Episode reward: 3399.4595407247543\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.277692 9.266589 9.241098 8.107966 8.685828 9.271134]\n",
      "Reset environment\n",
      "Episode reward: 4826.419480681419\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.282955  9.2718525 9.246343  8.113594  8.690589  9.276396 ]\n",
      "Reset environment\n",
      "Episode reward: 2387.745245695114\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.284995 9.273876 9.2484   8.115816 8.692429 9.278439]\n",
      "Reset environment\n",
      "Episode reward: 1384.4235556721687\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.286743  9.27563   9.250149  8.117757  8.693976  9.2801895]\n",
      "Reset environment\n",
      "Episode reward: 4493.65936422348\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.290939 9.279816 9.254364 8.122279 8.697787 9.284393]\n",
      "Reset environment\n",
      "Episode reward: 4292.839491426945\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.294912 9.283785 9.258355 8.126568 8.701373 9.288367]\n",
      "Reset environment\n",
      "Episode reward: -685.8137685060501\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.29312  9.282    9.256562 8.124707 8.699688 9.286582]\n",
      "Reset environment\n",
      "Episode reward: 1543.6748393177986\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.294715 9.283603 9.258146 8.126484 8.701148 9.28818 ]\n",
      "Reset environment\n",
      "Episode reward: 2686.899427562952\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.297055 9.285965 9.260465 8.129041 8.703211 9.290522]\n",
      "Reset environment\n",
      "Episode reward: 2055.6822101958096\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.298944 9.287926 9.262266 8.131086 8.704915 9.292409]\n",
      "Reset environment\n",
      "Episode reward: 4119.955353587866\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.302679 9.291644 9.266028 8.135152 8.708295 9.296148]\n",
      "Reset environment\n",
      "Episode reward: 1113.5401387661695\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.303253  9.292323  9.2665    8.1357565 8.708838  9.296729 ]\n",
      "Reset environment\n",
      "Episode reward: 3115.141716837883\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.306423  9.295588  9.269577  8.139293  8.711795  9.2999115]\n",
      "Reset environment\n",
      "Episode reward: 1876.9015328288078\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.308655 9.297814 9.271812 8.141744 8.713776 9.302142]\n",
      "Reset environment\n",
      "Episode reward: 1967.5784668326378\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.310309 9.299464 9.273468 8.143561 8.715243 9.303796]\n",
      "Reset environment\n",
      "Episode reward: 2163.683720767498\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.312842 9.301988 9.276008 8.146326 8.717499 9.306328]\n",
      "Reset environment\n",
      "Episode reward: 2220.656168103218\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.314754 9.303901 9.277924 8.148429 8.719199 9.308239]\n",
      "Reset environment\n",
      "Episode reward: 3131.366410046816\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.317538 9.3067   9.280684 8.151453 8.721671 9.311027]\n",
      "Reset environment\n",
      "Episode reward: 5089.9413222670555\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.322177 9.311335 9.285329 8.156445 8.725825 9.315665]\n",
      "Reset environment\n",
      "Episode reward: 1079.9362580776215\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.322941 9.3121   9.286091 8.157324 8.726489 9.316427]\n",
      "Reset environment\n",
      "Episode reward: 2483.9726163446903\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.324948 9.314071 9.288147 8.159539 8.728234 9.318437]\n",
      "Reset environment\n",
      "Episode reward: 1904.325973391533\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.327211  9.316332  9.290415  8.162017  8.730249  9.3207035]\n",
      "Reset environment\n",
      "Episode reward: 2199.8962705135345\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.329783  9.318896  9.2929945 8.164811  8.732563  9.323274 ]\n",
      "Reset environment\n",
      "Episode reward: 3271.744196474552\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.332545 9.321663 9.295754 8.167841 8.735016 9.326033]\n",
      "Reset environment\n",
      "Episode reward: 3890.768152654171\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.336137  9.325249  9.299353  8.1717005 8.738251  9.329626 ]\n",
      "Reset environment\n",
      "Episode reward: 2341.0897024273872\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.338913 9.328024 9.302128 8.174718 8.740742 9.332402]\n",
      "Reset environment\n",
      "Episode reward: 4796.743058860302\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.343218 9.332331 9.306436 8.179368 8.744594 9.336714]\n",
      "Reset environment\n",
      "Episode reward: 4109.6985703110695\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.346993 9.336087 9.310228 8.183421 8.747999 9.340487]\n",
      "Reset environment\n",
      "Episode reward: 1989.9209998846054\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.348529 9.337588 9.3118   8.185117 8.749341 9.342027]\n",
      "Reset environment\n",
      "Episode reward: 1389.118638575077\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.350258 9.339316 9.313533 8.18703  8.75086  9.343758]\n",
      "Reset environment\n",
      "Episode reward: 1709.793210029602\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.351569 9.340581 9.314897 8.18848  8.752036 9.345069]\n",
      "Reset environment\n",
      "Episode reward: 1631.094867527485\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.352887 9.341899 9.31622  8.189953 8.753204 9.346392]\n",
      "Reset environment\n",
      "Episode reward: 2239.404267013073\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.354756 9.343753 9.318119 8.192003 8.754882 9.348261]\n",
      "Reset environment\n",
      "Episode reward: 1655.9207457304\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.355764 9.344838 9.319045 8.193195 8.755713 9.349271]\n",
      "Reset environment\n",
      "Episode reward: 3273.0982343554497\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.358534 9.347604 9.321811 8.196228 8.758177 9.35204 ]\n",
      "Reset environment\n",
      "Episode reward: 3417.809585481882\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.361618 9.350721 9.32487  8.199586 8.760934 9.355129]\n",
      "Reset environment\n",
      "Episode reward: 3910.1503297388554\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.365175 9.354297 9.328399 8.203437 8.76411  9.358692]\n",
      "Reset environment\n",
      "Episode reward: 5024.677087664604\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.3698635 9.358963  9.333106  8.208481  8.768322  9.363377 ]\n",
      "Reset environment\n",
      "Episode reward: 5247.081813335419\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.374754 9.363858 9.337981 8.21374  8.772755 9.368271]\n",
      "Reset environment\n",
      "Episode reward: 1367.8342018127441\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.375831 9.36495  9.339048 8.214935 8.773723 9.369348]\n",
      "Reset environment\n",
      "Episode reward: 3187.520755827427\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.378589 9.367703 9.341804 8.217926 8.776231 9.3721  ]\n",
      "Reset environment\n",
      "Episode reward: 2229.632183969021\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.3811655 9.370289  9.344367  8.220762  8.778525  9.374676 ]\n",
      "Reset environment\n",
      "Episode reward: 4244.543527245522\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.38505  9.374188 9.348239 8.224942 8.782005 9.378566]\n",
      "Reset environment\n",
      "Episode reward: 974.7390556335449\n",
      "Total Steps: 33\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.385688 9.374803 9.348903 8.225683 8.782573 9.379207]\n",
      "Reset environment\n",
      "Episode reward: 4774.225636661053\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.389936 9.379056 9.353148 8.230275 8.786369 9.383459]\n",
      "Reset environment\n",
      "Episode reward: 3810.5904591828585\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.393359 9.382513 9.356525 8.233978 8.789426 9.386884]\n",
      "Reset environment\n",
      "Episode reward: 1403.5926480293274\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.395108 9.384259 9.358275 8.235906 8.790978 9.388635]\n",
      "Reset environment\n",
      "Episode reward: 4453.44015622139\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.399213  9.388376  9.362355  8.240315  8.7946615 9.392744 ]\n",
      "Reset environment\n",
      "Episode reward: 1772.2887330055237\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.401325 9.39049  9.364468 8.242631 8.796544 9.394859]\n",
      "Reset environment\n",
      "Episode reward: 3492.671128898859\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.404445 9.393638 9.367552 8.246    8.799327 9.397986]\n",
      "Reset environment\n",
      "Episode reward: -685.7352750301361\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.4026785 9.391892  9.3657875 8.244124  8.797718  9.396236 ]\n",
      "Reset environment\n",
      "Episode reward: 1972.5791599750519\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.404958 9.394178 9.368059 8.246626 8.799749 9.398516]\n",
      "Reset environment\n",
      "Episode reward: 1627.58566904068\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.40629   9.395495  9.369407  8.248097  8.8009405 9.399848 ]\n",
      "Reset environment\n",
      "Episode reward: 937.4183416366577\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.406909 9.396103 9.370042 8.248803 8.801491 9.400467]\n",
      "Reset environment\n",
      "Episode reward: 2254.7307785004377\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.408777  9.397943  9.3719425 8.250836  8.803159  9.402337 ]\n",
      "Reset environment\n",
      "Episode reward: 4702.938252568245\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.413116 9.402291 9.376274 8.255495 8.807104 9.406679]\n",
      "Reset environment\n",
      "Episode reward: 2640.848040699959\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.415395 9.404582 9.378546 8.257977 8.809141 9.408959]\n",
      "Reset environment\n",
      "Episode reward: 3083.979388177395\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.418087 9.407257 9.381265 8.260901 8.81157  9.411653]\n",
      "Reset environment\n",
      "Episode reward: 1378.6743790507317\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.4197855 9.408955  9.382969  8.262794  8.813067  9.413354 ]\n",
      "Reset environment\n",
      "Episode reward: 1408.090393126011\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.421531 9.410697 9.384718 8.264717 8.814615 9.415101]\n",
      "Reset environment\n",
      "Episode reward: 1541.6343042850494\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.422654 9.411774 9.385889 8.26597  8.815625 9.416225]\n",
      "Reset environment\n",
      "Episode reward: 1821.8898582458496\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.424159 9.413264 9.387412 8.267627 8.816977 9.417729]\n",
      "Reset environment\n",
      "Episode reward: 5519.152116000652\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.429346 9.418433 9.392625 8.273202 8.821638 9.42292 ]\n",
      "Reset environment\n",
      "Episode reward: 1218.859584569931\n",
      "Total Steps: 39\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.430264 9.419339 9.393558 8.274222 8.822464 9.423841]\n",
      "Reset environment\n",
      "Episode reward: 3461.071615934372\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.433295 9.422346 9.396624 8.27752  8.825192 9.426875]\n",
      "Reset environment\n",
      "Episode reward: 1384.5133693218231\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.434462 9.42353  9.397768 8.278771 8.82632  9.428044]\n",
      "Reset environment\n",
      "Episode reward: 752.0265467166901\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.434336 9.423549 9.397508 8.278608 8.826218 9.427931]\n",
      "Reset environment\n",
      "Episode reward: 2438.02779597044\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.436284 9.425499 9.399448 8.280751 8.827956 9.429878]\n",
      "Reset environment\n",
      "Episode reward: 4760.794719278812\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.440677 9.429903 9.40382  8.285459 8.831929 9.434277]\n",
      "Reset environment\n",
      "Episode reward: 1372.8886655736715\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.44116   9.43051   9.404205  8.28606   8.8322735 9.434776 ]\n",
      "Reset environment\n",
      "Episode reward: 1832.5788261890411\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.44261   9.431948  9.405665  8.287686  8.8335495 9.436225 ]\n",
      "Reset environment\n",
      "Episode reward: 1516.2205904722214\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.44359  9.43299  9.406596 8.288831 8.834376 9.437212]\n",
      "Reset environment\n",
      "Episode reward: 4494.99171847105\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.447661 9.437037 9.410692 8.293234 8.838044 9.441289]\n",
      "Reset environment\n",
      "Episode reward: 1358.1149248480797\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.449351 9.43873  9.412378 8.295105 8.839535 9.442984]\n",
      "Reset environment\n",
      "Episode reward: 2775.0917480550706\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.451944  9.441412  9.4148655 8.297971  8.841956  9.445585 ]\n",
      "Reset environment\n",
      "Episode reward: 2148.444990247488\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.453731 9.443176 9.41668  8.299897 8.843557 9.447373]\n",
      "Reset environment\n",
      "Episode reward: 1433.7999757528305\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.455523 9.444964 9.418466 8.301872 8.845142 9.449163]\n",
      "Reset environment\n",
      "Episode reward: 3440.2434127926826\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.458455 9.447902 9.421397 8.305069 8.847754 9.452098]\n",
      "Reset environment\n",
      "Episode reward: 1919.129060804844\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.460711 9.450156 9.423656 8.307522 8.849767 9.454352]\n",
      "Reset environment\n",
      "Episode reward: 2303.064476966858\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.46333   9.4527855 9.426273  8.310369  8.852129  9.456974 ]\n",
      "Reset environment\n",
      "Episode reward: 3480.949650287628\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.466402 9.455863 9.42932  8.313704 8.854857 9.460045]\n",
      "Reset environment\n",
      "Episode reward: 2540.0711275339127\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.4685755 9.4580145 9.431508  8.31608   8.856823  9.462214 ]\n",
      "Reset environment\n",
      "Episode reward: 3407.642956286669\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.471554 9.460953 9.434544 8.319298 8.859503 9.465191]\n",
      "Reset environment\n",
      "Episode reward: 5314.465332329273\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.477227 9.466622 9.44022  8.325368 8.86463  9.470872]\n",
      "Reset environment\n",
      "Episode reward: 1995.2575139403343\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.479524 9.468922 9.442505 8.327883 8.866683 9.473169]\n",
      "Reset environment\n",
      "Episode reward: 2324.7870520353317\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.481479 9.470876 9.444455 8.330028 8.86844  9.475126]\n",
      "Reset environment\n",
      "Episode reward: 3672.425743073225\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.485032 9.474439 9.448    8.33385  8.871747 9.478686]\n",
      "Reset environment\n",
      "Episode reward: 4882.8616107702255\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.489348 9.478742 9.452312 8.338486 8.875612 9.483006]\n",
      "Reset environment\n",
      "Episode reward: 241.3882611989975\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.488964 9.478381 9.451899 8.337887 8.8753   9.482624]\n",
      "Reset environment\n",
      "Episode reward: 2254.511258482933\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.49085   9.480275  9.4537735 8.339948  8.87698   9.48451  ]\n",
      "Reset environment\n",
      "Episode reward: 4407.77677154541\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.494847 9.484256 9.457794 8.344268 8.880601 9.488512]\n",
      "Reset environment\n",
      "Episode reward: 1962.5935488045216\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.496266 9.485628 9.459263 8.345862 8.881805 9.489933]\n",
      "Reset environment\n",
      "Episode reward: 5788.3114612698555\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.50167  9.491048 9.46466  8.351619 8.886704 9.495346]\n",
      "Reset environment\n",
      "Episode reward: 1819.7839276343584\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.502762  9.4922    9.465706  8.353155  8.88766   9.4964485]\n",
      "Reset environment\n",
      "Episode reward: 3751.2411206662655\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.506105  9.495569  9.4690275 8.3567705 8.890638  9.499798 ]\n",
      "Reset environment\n",
      "Episode reward: 4278.068769156933\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.50995  9.499394 9.472887 8.360925 8.894091 9.503645]\n",
      "Reset environment\n",
      "Episode reward: 3506.5974346995354\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.512984  9.502398  9.4759655 8.364224  8.896831  9.506686 ]\n",
      "Reset environment\n",
      "Episode reward: 3637.158619403839\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.516223 9.505616 9.479228 8.367717 8.899755 9.509925]\n",
      "Reset environment\n",
      "Episode reward: 505.9075046777725\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.515652 9.505173 9.478556 8.367084 8.899264 9.509371]\n",
      "Reset environment\n",
      "Episode reward: 1240.3951722681522\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.516207  9.505837  9.4790125 8.367603  8.899746  9.509938 ]\n",
      "Reset environment\n",
      "Episode reward: 3926.9231346547604\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.5199375 9.509621  9.482694  8.3716345 8.90319   9.513677 ]\n",
      "Reset environment\n",
      "Episode reward: 1867.10672134161\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.52212  9.511796 9.484879 8.374012 8.905135 9.515858]\n",
      "Reset environment\n",
      "Episode reward: 5246.724596619606\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.527652 9.517337 9.490408 8.379931 8.910156 9.521393]\n",
      "Reset environment\n",
      "Episode reward: 2035.4061396121979\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.529978 9.519665 9.492733 8.38247  8.912233 9.523719]\n",
      "Reset environment\n",
      "Episode reward: 2181.9968357160687\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.531371 9.521133 9.494051 8.384102 8.913367 9.525116]\n",
      "Reset environment\n",
      "Episode reward: 2040.3572787344456\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.532913 9.52263  9.495639 8.385812 8.914717 9.526654]\n",
      "Reset environment\n",
      "Episode reward: 3277.8438200354576\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.5357685 9.525491  9.498492  8.388924  8.9172945 9.529507 ]\n",
      "Reset environment\n",
      "Episode reward: 2152.155495584011\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.538224 9.527948 9.500942 8.391593 8.919498 9.531962]\n",
      "Reset environment\n",
      "Episode reward: 6177.358800545335\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.519485 9.509375 9.482167 8.369349 8.904135 9.513163]\n",
      "Reset environment\n",
      "Episode reward: 3982.3566717505455\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.523015 9.512922 9.485682 8.373172 8.907292 9.516697]\n",
      "Reset environment\n",
      "Episode reward: 1513.5055749416351\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.524058 9.514025 9.486662 8.374351 8.90823  9.517742]\n",
      "Reset environment\n",
      "Episode reward: 3085.919147133827\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.527049  9.517075  9.489576  8.3776045 8.910948  9.520739 ]\n",
      "Reset environment\n",
      "Episode reward: 4123.151090055704\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.530747  9.520804  9.493251  8.3815975 8.91425   9.5244465]\n",
      "Reset environment\n",
      "Episode reward: 5470.375635147095\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.535814  9.525884  9.4983015 8.387028  8.918857  9.529525 ]\n",
      "Reset environment\n",
      "Episode reward: 1578.4009564518929\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.537402 9.527469 9.499883 8.388766 8.920318 9.531107]\n",
      "Reset environment\n",
      "Episode reward: 2119.1330140829086\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.539168  9.529228  9.501653  8.3906975 8.921893  9.532872 ]\n",
      "Reset environment\n",
      "Episode reward: 2083.739051938057\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.541529 9.531594 9.504009 8.393279 8.924013 9.535235]\n",
      "Reset environment\n",
      "Episode reward: 1972.2225592136383\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.543108 9.533146 9.505618 8.395    8.925426 9.536813]\n",
      "Reset environment\n",
      "Episode reward: 1499.6788582801819\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.544284 9.534323 9.506798 8.396316 8.926478 9.537991]\n",
      "Reset environment\n",
      "Episode reward: 5472.064394414425\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.549329 9.539375 9.511836 8.401707 8.931071 9.543041]\n",
      "Reset environment\n",
      "Episode reward: -686.1715754270554\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.547558 9.53761  9.51006  8.399899 8.929432 9.541271]\n",
      "Reset environment\n",
      "Episode reward: 4059.968745857477\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.551173 9.541258 9.513633 8.403796 8.932649 9.544893]\n",
      "Reset environment\n",
      "Episode reward: 1714.0919824838638\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.55262  9.542694 9.515088 8.405436 8.934001 9.546344]\n",
      "Reset environment\n",
      "Episode reward: 2739.146388657391\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.554893 9.545018 9.517315 8.40792  8.936037 9.548618]\n",
      "Reset environment\n",
      "Episode reward: 2181.4058176875114\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.556544 9.546723 9.518935 8.409979 8.937714 9.550281]\n",
      "Reset environment\n",
      "Episode reward: 2702.931327342987\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.558809 9.548975 9.52122  8.412451 8.939751 9.552544]\n",
      "Reset environment\n",
      "Episode reward: 2032.172343492508\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.560455 9.5506   9.522887 8.414267 8.941221 9.554186]\n",
      "Reset environment\n",
      "Episode reward: 4496.987034201622\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.564508 9.554644 9.526955 8.418624 8.944889 9.55824 ]\n",
      "Reset environment\n",
      "Episode reward: 1506.0587025284767\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.565659 9.55579  9.528107 8.419908 8.945902 9.559393]\n",
      "Reset environment\n",
      "Episode reward: 3478.6170102357864\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.5687   9.558837 9.531124 8.423201 8.94861  9.562432]\n",
      "Reset environment\n",
      "Episode reward: 3091.122697889805\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.571388 9.561531 9.533807 8.42611  8.951032 9.565122]\n",
      "Reset environment\n",
      "Episode reward: 1674.2740606069565\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.572597  9.5626955 9.53506   8.427449  8.952107  9.566331 ]\n",
      "Reset environment\n",
      "Episode reward: 2484.3910956680775\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.574827 9.564902 9.537315 8.429873 8.954002 9.568571]\n",
      "Reset environment\n",
      "Episode reward: 3746.4734030365944\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.578101 9.568194 9.540566 8.43344  8.956929 9.571851]\n",
      "Reset environment\n",
      "Episode reward: 4731.385361254215\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.582389  9.572467  9.544868  8.438045  8.960819  9.5761385]\n",
      "Reset environment\n",
      "Episode reward: 2729.574493229389\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.58471  9.574797 9.547181 8.44057  8.962896 9.578463]\n",
      "Reset environment\n",
      "Episode reward: 1569.6685338020325\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.585654  9.575801  9.548087  8.441863  8.963859  9.5794115]\n",
      "Reset environment\n",
      "Episode reward: 1547.835726261139\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.586841 9.576985 9.549275 8.443186 8.964903 9.580597]\n",
      "Reset environment\n",
      "Episode reward: 1303.0705695152283\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.587777 9.577891 9.550247 8.444226 8.96574  9.581534]\n",
      "Reset environment\n",
      "Episode reward: 875.2291613817215\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.587683 9.577675 9.550282 8.444106 8.965688 9.581457]\n",
      "Reset environment\n",
      "Episode reward: 6212.985987663269\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.593458 9.583456 9.556049 8.45025  8.970936 9.587234]\n",
      "Reset environment\n",
      "Episode reward: 4774.331489980221\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.597782  9.587784  9.560368  8.454902  8.9748535 9.591564 ]\n",
      "Reset environment\n",
      "Episode reward: 2021.3685319423676\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.59941  9.589421 9.561983 8.456688 8.9763   9.593194]\n",
      "Reset environment\n",
      "Episode reward: 380.41605150699615\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.598699  9.588832  9.5611725 8.4558325 8.975741  9.592496 ]\n",
      "Reset environment\n",
      "Episode reward: 3281.7189056277275\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.601543  9.591712  9.563973  8.4589205 8.978282  9.595342 ]\n",
      "Reset environment\n",
      "Episode reward: 160.42968893051147\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.600702  9.590797  9.563194  8.457784  8.977703  9.5945015]\n",
      "Reset environment\n",
      "Episode reward: 1764.465471982956\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.60273  9.592825 9.565219 8.460026 8.979497 9.596532]\n",
      "Reset environment\n",
      "Episode reward: 901.1858336925507\n",
      "Total Steps: 28\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6033125 9.593404  9.565806  8.460703  8.980009  9.5971155]\n",
      "Reset environment\n",
      "Episode reward: 1194.2205028533936\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.60419  9.594279 9.566683 8.461681 8.980793 9.59799 ]\n",
      "Reset environment\n",
      "Episode reward: 3968.9422255158424\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6077   9.597798 9.570183 8.465475 8.983946 9.6015  ]\n",
      "Reset environment\n",
      "Episode reward: 1724.7676355838776\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.609049 9.599156 9.571517 8.466963 8.985146 9.602852]\n",
      "Reset environment\n",
      "Episode reward: 2028.8292061388493\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.610845 9.60101  9.573236 8.468929 8.986745 9.604649]\n",
      "Reset environment\n",
      "Episode reward: 1572.237648844719\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6120615 9.60222   9.574461  8.470289  8.987828  9.605864 ]\n",
      "Reset environment\n",
      "Episode reward: 4227.060022898018\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.615757 9.605891 9.578188 8.474284 8.991173 9.609561]\n",
      "Reset environment\n",
      "Episode reward: 3527.490386724472\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.618814 9.608927 9.581262 8.477601 8.993927 9.612621]\n",
      "Reset environment\n",
      "Episode reward: 3724.0596753656864\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.622048 9.612142 9.58453  8.481105 8.996852 9.61586 ]\n",
      "Reset environment\n",
      "Episode reward: 2108.281782001257\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.623689 9.613745 9.586194 8.482905 8.998309 9.617499]\n",
      "Reset environment\n",
      "Episode reward: 2507.487826883793\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.625981  9.616113  9.588411  8.48539   9.000436  9.6198015]\n",
      "Reset environment\n",
      "Episode reward: 2510.4399394989014\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.628072 9.618203 9.590489 8.487671 9.002312 9.621892]\n",
      "Reset environment\n",
      "Episode reward: 2748.3317409157753\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.630418 9.620539 9.592842 8.490212 9.004427 9.624238]\n",
      "Reset environment\n",
      "Episode reward: 3525.9028432667255\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.633281 9.623323 9.595786 8.493377 9.006887 9.627103]\n",
      "Reset environment\n",
      "Episode reward: -685.4648166894913\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.631459 9.621507 9.593965 8.491493 9.00523  9.625289]\n",
      "Reset environment\n",
      "Episode reward: 1399.232815682888\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.633159 9.623212 9.595653 8.493369 9.00673  9.626989]\n",
      "Reset environment\n",
      "Episode reward: 3822.780079483986\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.636522  9.626579  9.5990095 8.497014  9.009774  9.630353 ]\n",
      "Reset environment\n",
      "Episode reward: 4947.174572765827\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.641002  9.6310425 9.603507  8.501817  9.013857  9.634835 ]\n",
      "Reset environment\n",
      "Episode reward: 5657.521741330624\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.64612  9.636159 9.608626 8.507339 9.018453 9.63996 ]\n",
      "Reset environment\n",
      "Episode reward: 1368.5918486118317\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.647133 9.637152 9.609661 8.508458 9.019363 9.640974]\n",
      "Reset environment\n",
      "Episode reward: 1988.0102592483163\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.648888 9.638962 9.611349 8.510336 9.020965 9.642733]\n",
      "Reset environment\n",
      "Episode reward: 1406.9522665143013\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.650568 9.64064  9.613033 8.51219  9.022458 9.644416]\n",
      "Reset environment\n",
      "Episode reward: 2523.4330483675003\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.652579 9.642644 9.615054 8.514375 9.024249 9.646428]\n",
      "Reset environment\n",
      "Episode reward: -689.7543419599533\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.650801 9.640867 9.613274 8.51249  9.022613 9.644655]\n",
      "Reset environment\n",
      "Episode reward: 5600.704809963703\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.655897  9.6459465 9.618392  8.51796   9.027227  9.649764 ]\n",
      "Reset environment\n",
      "Episode reward: 1669.9674297571182\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.65784  9.647887 9.620331 8.520095 9.028954 9.651707]\n",
      "Reset environment\n",
      "Episode reward: 1240.157220363617\n",
      "Total Steps: 39\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.658757 9.648805 9.621248 8.521121 9.029775 9.652623]\n",
      "Reset environment\n",
      "Episode reward: 1851.4523721933365\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.660244 9.650285 9.622737 8.522771 9.031111 9.654109]\n",
      "Reset environment\n",
      "Episode reward: 2139.4599865674973\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.662003 9.652038 9.624497 8.524704 9.032677 9.655868]\n",
      "Reset environment\n",
      "Episode reward: 4646.708791553974\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.666161 9.6562   9.628654 8.529169 9.036455 9.660031]\n",
      "Reset environment\n",
      "Episode reward: 1718.8169872760773\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.667522 9.657549 9.630019 8.530675 9.037685 9.661393]\n",
      "Reset environment\n",
      "Episode reward: 3106.991555184126\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.670123  9.6601305 9.632645  8.533492  9.04003   9.663993 ]\n",
      "Reset environment\n",
      "Episode reward: 36.03815546631813\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.668987 9.659156 9.631377 8.532377 9.039025 9.66287 ]\n",
      "Reset environment\n",
      "Episode reward: -353.5709720849991\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.667738 9.657833 9.630205 8.530794 9.037957 9.661632]\n",
      "Reset environment\n",
      "Episode reward: 4620.689286112785\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.671859  9.661943  9.634341  8.535216  9.041691  9.6657505]\n",
      "Reset environment\n",
      "Episode reward: 4804.772020578384\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6761675 9.666259  9.638645  8.539859  9.045603  9.670065 ]\n",
      "Reset environment\n",
      "Episode reward: -199.22075923904777\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.674669 9.664646 9.637258 8.538368 9.044117 9.66858 ]\n",
      "Reset environment\n",
      "Episode reward: 1596.238105982542\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.675864 9.665817 9.638479 8.539687 9.045187 9.669776]\n",
      "Reset environment\n",
      "Episode reward: 4869.869559884071\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.680245 9.670194 9.642876 8.544379 9.049177 9.674162]\n",
      "Reset environment\n",
      "Episode reward: 2450.3586933612823\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.682058  9.6720915 9.644633  8.546652  9.050951  9.675992 ]\n",
      "Reset environment\n",
      "Episode reward: 1458.454617023468\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.683186 9.673222 9.64576  8.547897 9.051966 9.677118]\n",
      "Reset environment\n",
      "Episode reward: 1730.9596286416054\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.684558 9.674593 9.647132 8.549408 9.053194 9.678493]\n",
      "Reset environment\n",
      "Episode reward: 4899.199931502342\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.688965 9.679012 9.65154  8.554121 9.057204 9.682903]\n",
      "Reset environment\n",
      "Episode reward: 1385.1219094395638\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.690611 9.680657 9.653189 8.555943 9.05866  9.684551]\n",
      "Reset environment\n",
      "Episode reward: 2034.6537521481514\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.69207  9.682029 9.654742 8.557505 9.05991  9.686022]\n",
      "Reset environment\n",
      "Episode reward: -690.7507673501968\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6902685 9.68024   9.652936  8.555566  9.058274  9.684235 ]\n",
      "Reset environment\n",
      "Episode reward: 2152.946720957756\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.692663 9.682636 9.655328 8.558175 9.060435 9.686626]\n",
      "Reset environment\n",
      "Episode reward: 1636.8535049557686\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.694277 9.684242 9.656936 8.559971 9.061911 9.688247]\n",
      "Reset environment\n",
      "Episode reward: 3115.472882568836\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.696899  9.6868725 9.659541  8.56282   9.064232  9.690869 ]\n",
      "Reset environment\n",
      "Episode reward: 3375.7063627485186\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.6998005 9.6897335 9.662482  8.565949  9.066853  9.693778 ]\n",
      "Reset environment\n",
      "Episode reward: 5324.469636559486\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.704585 9.694535 9.667256 8.571072 9.071195 9.698568]\n",
      "Reset environment\n",
      "Episode reward: 4396.979383111\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.708442 9.69837  9.671142 8.57524  9.074697 9.702429]\n",
      "Reset environment\n",
      "Episode reward: 2543.496359318495\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.7104225 9.700336  9.6731615 8.577418  9.076444  9.704413 ]\n",
      "Reset environment\n",
      "Episode reward: 2168.657035768032\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.71216  9.702099 9.674874 8.579327 9.077989 9.706152]\n",
      "Reset environment\n",
      "Episode reward: 2192.6911787986755\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.713951  9.703884  9.676668  8.5812845 9.079575  9.707938 ]\n",
      "Reset environment\n",
      "Episode reward: 1252.7907786369324\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.714836 9.704798 9.677521 8.582294 9.080346 9.708821]\n",
      "Reset environment\n",
      "Episode reward: 2431.2776870131493\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.716461 9.706488 9.67911  8.584406 9.081996 9.710454]\n",
      "Reset environment\n",
      "Episode reward: 1628.4542226791382\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.717699 9.707696 9.680378 8.58576  9.083109 9.711691]\n",
      "Reset environment\n",
      "Episode reward: 4767.3670372366905\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.721945 9.711926 9.684633 8.590323 9.086918 9.71594 ]\n",
      "Reset environment\n",
      "Episode reward: 1403.3263755142689\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.722736 9.712614 9.68552  8.591156 9.087499 9.71674 ]\n",
      "Reset environment\n",
      "Episode reward: 3130.6991205736995\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.725313 9.715258 9.688032 8.593991 9.089859 9.719318]\n",
      "Reset environment\n",
      "Episode reward: 1814.8701975345612\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.726709 9.716629 9.689447 8.595515 9.091102 9.720713]\n",
      "Reset environment\n",
      "Episode reward: 3267.403853133321\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.7294655 9.719432  9.692162  8.598534  9.093589  9.723469 ]\n",
      "Reset environment\n",
      "Episode reward: -685.5434365272522\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.727678 9.717644 9.690375 8.596766 9.091908 9.721687]\n",
      "Reset environment\n",
      "Episode reward: 2722.4313731193542\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.729958 9.719926 9.692651 8.599243 9.093959 9.723971]\n",
      "Reset environment\n",
      "Episode reward: 4690.429551720619\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.7340975 9.724045  9.696813  8.6037035 9.097691  9.728113 ]\n",
      "Reset environment\n",
      "Episode reward: -341.4503198862076\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.732801  9.722749  9.695522  8.6024685 9.096396  9.726819 ]\n",
      "Reset environment\n",
      "Episode reward: 1894.4892316162586\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.734132 9.72402  9.696906 8.603972 9.097538 9.728154]\n",
      "Reset environment\n",
      "Episode reward: 1734.9174038171768\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.735475 9.725346 9.698265 8.605441 9.098743 9.729494]\n",
      "Reset environment\n",
      "Episode reward: 5231.864292502403\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.740186 9.730052 9.702988 8.610481 9.103004 9.734211]\n",
      "Reset environment\n",
      "Episode reward: 3549.146374940872\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.743236 9.733107 9.706027 8.613781 9.105758 9.737257]\n",
      "Reset environment\n",
      "Episode reward: 4290.51629960537\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.746982 9.736827 9.709785 8.617811 9.109126 9.741004]\n",
      "Reset environment\n",
      "Episode reward: 2019.5375887155533\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.74862  9.738469 9.711407 8.619606 9.110614 9.742645]\n",
      "Reset environment\n",
      "Episode reward: 1944.5010206699371\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.750807 9.740647 9.7136   8.621993 9.112564 9.744833]\n",
      "Reset environment\n",
      "Episode reward: 2305.6206201314926\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.753333 9.743167 9.716132 8.624724 9.114832 9.747364]\n",
      "Reset environment\n",
      "Episode reward: -685.5881142616272\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.751549 9.74139  9.714347 8.622828 9.113193 9.745589]\n",
      "Reset environment\n",
      "Episode reward: 5758.850629270077\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.756685 9.746542 9.71948  8.628321 9.117852 9.750727]\n",
      "Reset environment\n",
      "Episode reward: 1935.17211633455\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.75799  9.747913 9.720723 8.629816 9.11896  9.752035]\n",
      "Reset environment\n",
      "Episode reward: 1004.4185534715652\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.75826  9.748267 9.720921 8.62998  9.119212 9.752317]\n",
      "Reset environment\n",
      "Episode reward: 2108.398782014847\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.759951 9.749951 9.722618 8.631843 9.12073  9.754009]\n",
      "Reset environment\n",
      "Episode reward: 2704.0880974531174\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.762173 9.752179 9.724833 8.634271 9.122721 9.756232]\n",
      "Reset environment\n",
      "Episode reward: 3519.2937881052494\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.765454 9.755483 9.72808  8.637823 9.125732 9.75952 ]\n",
      "Reset environment\n",
      "Episode reward: -1115.7337812185287\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.762561  9.752351  9.725458  8.6348295 9.123132  9.756661 ]\n",
      "Reset environment\n",
      "Episode reward: 1846.373681306839\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.764007 9.753804 9.726893 8.636422 9.124434 9.758106]\n",
      "Reset environment\n",
      "Episode reward: 3818.416181474924\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.76723  9.756989 9.730161 8.639908 9.127325 9.761327]\n",
      "Reset environment\n",
      "Episode reward: 3952.827460050583\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.770651  9.760395  9.733601  8.643596  9.1304245 9.764747 ]\n",
      "Reset environment\n",
      "Episode reward: 991.4168515205383\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.771439 9.761181 9.734386 8.64442  9.131194 9.765538]\n",
      "Reset environment\n",
      "Episode reward: 1645.0594174265862\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.77329  9.763032 9.736233 8.646469 9.132835 9.76739 ]\n",
      "Reset environment\n",
      "Episode reward: 2250.7840418219566\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.775114 9.764865 9.738044 8.648452 9.134465 9.769214]\n",
      "Reset environment\n",
      "Episode reward: 2863.9305123090744\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.777525 9.767268 9.74046  8.651061 9.136616 9.771622]\n",
      "Reset environment\n",
      "Episode reward: -361.8656095266342\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.776269 9.76601  9.739201 8.649765 9.135363 9.770374]\n",
      "Reset environment\n",
      "Episode reward: 1102.38894033432\n",
      "Total Steps: 34\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.777031 9.766776 9.739961 8.650626 9.136044 9.771136]\n",
      "Reset environment\n",
      "Episode reward: 3248.9062947034836\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.779769 9.769528 9.742689 8.653594 9.138494 9.773877]\n",
      "Reset environment\n",
      "Episode reward: 4125.353150308132\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.783259 9.772992 9.746205 8.657396 9.141631 9.777368]\n",
      "Reset environment\n",
      "Episode reward: 4776.7822450995445\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.787484 9.777212 9.750447 8.661938 9.14546  9.781604]\n",
      "Reset environment\n",
      "Episode reward: 2198.0570201277733\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.789281  9.779018  9.752239  8.663904  9.147096  9.7834015]\n",
      "Reset environment\n",
      "Episode reward: 2041.4008504152298\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.791541 9.781271 9.754503 8.666371 9.149115 9.785663]\n",
      "Reset environment\n",
      "Episode reward: 2516.514402091503\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.793568 9.783288 9.756552 8.668584 9.150941 9.787693]\n",
      "Reset environment\n",
      "Episode reward: 1962.7236261367798\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.795102 9.784828 9.758081 8.67027  9.152319 9.789231]\n",
      "Reset environment\n",
      "Episode reward: 4838.306758284569\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.799363 9.789074 9.762359 8.674843 9.156155 9.793495]\n",
      "Reset environment\n",
      "Episode reward: 1947.7170342504978\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.800776 9.790399 9.763864 8.676281 9.157416 9.794918]\n",
      "Reset environment\n",
      "Episode reward: 2192.950679540634\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.802455 9.792052 9.765571 8.678134 9.15891  9.796598]\n",
      "Reset environment\n",
      "Episode reward: 2135.2399930357933\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.804771 9.794367 9.767896 8.680657 9.16098  9.798917]\n",
      "Reset environment\n",
      "Episode reward: 3033.3615393042564\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.807324 9.796941 9.770417 8.683435 9.163266 9.801472]\n",
      "Reset environment\n",
      "Episode reward: 6428.304819941521\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.812964  9.8025875 9.77605   8.689486  9.168368  9.807116 ]\n",
      "Reset environment\n",
      "Episode reward: 3747.54451829195\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.816165 9.805796 9.77924  8.692927 9.171241 9.810321]\n",
      "Reset environment\n",
      "Episode reward: 2166.3439060673118\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.81756  9.807259 9.780588 8.694786 9.172678 9.811735]\n",
      "Reset environment\n",
      "Episode reward: 3808.8776822835207\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.820736 9.810516 9.783696 8.698271 9.175605 9.814924]\n",
      "Reset environment\n",
      "Episode reward: 4361.50492015481\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.824493 9.814259 9.787476 8.702336 9.179009 9.818682]\n",
      "Reset environment\n",
      "Episode reward: 4410.542286515236\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8283415 9.8181095 9.791324  8.706456  9.182482  9.822535 ]\n",
      "Reset environment\n",
      "Episode reward: 1704.1428180336952\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.829625 9.819409 9.792595 8.707886 9.183624 9.82382 ]\n",
      "Reset environment\n",
      "Episode reward: 2679.871073961258\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8318205 9.821595  9.794795  8.710271  9.185586  9.826016 ]\n",
      "Reset environment\n",
      "Episode reward: 4152.288002669811\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.835386 9.825165 9.798362 8.714134 9.188815 9.829586]\n",
      "Reset environment\n",
      "Episode reward: 1433.0075297355652\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.836453 9.826236 9.799432 8.715334 9.189766 9.830657]\n",
      "Reset environment\n",
      "Episode reward: 1539.4020942896605\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.837599 9.827444 9.800505 8.716525 9.19084  9.831803]\n",
      "Reset environment\n",
      "Episode reward: 1779.2472770810127\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.839563 9.82941  9.802468 8.718692 9.19258  9.833771]\n",
      "Reset environment\n",
      "Episode reward: 1973.5381935238838\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.841722 9.831575 9.804616 8.721059 9.194519 9.83593 ]\n",
      "Reset environment\n",
      "Episode reward: 1967.7798243761063\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.843297 9.833155 9.806183 8.722785 9.195942 9.837505]\n",
      "Reset environment\n",
      "Episode reward: 5176.245215713978\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.847892 9.837751 9.810767 8.727705 9.20011  9.842102]\n",
      "Reset environment\n",
      "Episode reward: 5117.9291261434555\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.853071 9.842935 9.81594  8.733263 9.204814 9.847277]\n",
      "Reset environment\n",
      "Episode reward: 2472.2190210819244\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8550625 9.844931  9.817921  8.735447  9.206586  9.849271 ]\n",
      "Reset environment\n",
      "Episode reward: 3816.0237169265747\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.858332  9.848209  9.8211775 8.738953  9.20953   9.85254  ]\n",
      "Reset environment\n",
      "Episode reward: 2065.9269204437733\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.859937 9.849872 9.822732 8.740889 9.211079 9.854153]\n",
      "Reset environment\n",
      "Episode reward: 1225.7895014286041\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.860721 9.850664 9.823493 8.741725 9.211865 9.854937]\n",
      "Reset environment\n",
      "Episode reward: 945.1847553253174\n",
      "Total Steps: 29\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8613205 9.851266  9.824091  8.742416  9.212397  9.855538 ]\n",
      "Reset environment\n",
      "Episode reward: 3499.500742226839\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.864144  9.8541355 9.826851  8.7455225 9.214924  9.85836  ]\n",
      "Reset environment\n",
      "Episode reward: 4668.29386729002\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8682165 9.858187  9.830929  8.749897  9.218584  9.862428 ]\n",
      "Reset environment\n",
      "Episode reward: 3573.607158064842\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.871244 9.8612   9.833975 8.753161 9.221327 9.865457]\n",
      "Reset environment\n",
      "Episode reward: 3161.1144711375237\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.873852  9.863835  9.836551  8.756016  9.2236395 9.86806  ]\n",
      "Reset environment\n",
      "Episode reward: 3569.846634685993\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.876853  9.866828  9.839576  8.759262  9.2263565 9.871063 ]\n",
      "Reset environment\n",
      "Episode reward: 4890.259346365929\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.880945 9.870914 9.843677 8.763667 9.230014 9.875158]\n",
      "Reset environment\n",
      "Episode reward: 1606.161281004548\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.882018 9.871927 9.844804 8.764731 9.231029 9.876242]\n",
      "Reset environment\n",
      "Episode reward: 1236.1353789567947\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8828335 9.872715  9.845651  8.765639  9.231765  9.87706  ]\n",
      "Reset environment\n",
      "Episode reward: 244.75523555278778\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.881929 9.8717   9.844875 8.764668 9.230953 9.876168]\n",
      "Reset environment\n",
      "Episode reward: 1696.8874007463455\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.883599  9.873338  9.8465605 8.766508  9.232481  9.877835 ]\n",
      "Reset environment\n",
      "Episode reward: 2604.28426361084\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.885556 9.875251 9.848565 8.76869  9.234103 9.879793]\n",
      "Reset environment\n",
      "Episode reward: 3325.3657190799713\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.888212 9.877901 9.851215 8.771567 9.23647  9.882442]\n",
      "Reset environment\n",
      "Episode reward: 2638.1041360348463\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.8903055 9.880036  9.853262  8.773873  9.23831   9.884536 ]\n",
      "Reset environment\n",
      "Episode reward: 1782.2378425002098\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.892269 9.882001 9.85523  8.776036 9.240052 9.8865  ]\n",
      "Reset environment\n",
      "Episode reward: 3632.7671280801296\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.895339  9.885112  9.8582535 8.779391  9.242807  9.889573 ]\n",
      "Reset environment\n",
      "Episode reward: 4296.914252638817\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.89904  9.888822 9.861945 8.78336  9.246165 9.893272]\n",
      "Reset environment\n",
      "Episode reward: 4605.512030720711\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.903034 9.892829 9.865928 8.787639 9.249777 9.897273]\n",
      "Reset environment\n",
      "Episode reward: 1827.4673827067018\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.904057  9.893885  9.866939  8.7890625 9.250887  9.898309 ]\n",
      "Reset environment\n",
      "Episode reward: 1945.1818808242679\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.90544   9.895338  9.868259  8.790613  9.2521105 9.899697 ]\n",
      "Reset environment\n",
      "Episode reward: 3954.290284693241\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.90878  9.898695 9.871577 8.794229 9.255092 9.903041]\n",
      "Reset environment\n",
      "Episode reward: 591.5331044197083\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.908203 9.898254 9.870872 8.793588 9.254618 9.902478]\n",
      "Reset environment\n",
      "Episode reward: 2607.2210361361504\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.910296 9.900355 9.872958 8.795859 9.256498 9.904571]\n",
      "Reset environment\n",
      "Episode reward: 3738.708146043122\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.913856 9.903966 9.87646  8.799815 9.259833 9.908135]\n",
      "Reset environment\n",
      "Episode reward: 1561.541719675064\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.915047 9.905169 9.877648 8.801135 9.260903 9.909329]\n",
      "Reset environment\n",
      "Episode reward: 4259.857381641865\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.918702  9.908836  9.8812895 8.805062  9.264174  9.912979 ]\n",
      "Reset environment\n",
      "Episode reward: 1594.6167221665382\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9205065 9.910636  9.883097  8.807033  9.265774  9.914784 ]\n",
      "Reset environment\n",
      "Episode reward: 1696.6135472357273\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.921393 9.911595 9.883919 8.80811  9.2665   9.915673]\n",
      "Reset environment\n",
      "Episode reward: 3106.500644028187\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.923943 9.914148 9.886457 8.810865 9.268768 9.918224]\n",
      "Reset environment\n",
      "Episode reward: 6051.976612865925\n",
      "Total Steps: 205\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9292755 9.919459  9.891806  8.816596  9.273555  9.923557 ]\n",
      "Reset environment\n",
      "Episode reward: 2098.626997292042\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.931545  9.92173   9.8940735 8.819067  9.275595  9.925828 ]\n",
      "Reset environment\n",
      "Episode reward: 3875.8820739984512\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.934866 9.92506  9.897379 8.822652 9.278585 9.929153]\n",
      "Reset environment\n",
      "Episode reward: 2137.3397548794746\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.937179 9.927371 9.899697 8.825178 9.280651 9.931466]\n",
      "Reset environment\n",
      "Episode reward: 1947.8174480497837\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.938567 9.928831 9.901027 8.826737 9.281879 9.932861]\n",
      "Reset environment\n",
      "Episode reward: 3412.111771777272\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.941183 9.931372 9.903718 8.829658 9.284088 9.93548 ]\n",
      "Reset environment\n",
      "Episode reward: 1010.2074770927429\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.941773 9.931958 9.904307 8.830344 9.28461  9.936071]\n",
      "Reset environment\n",
      "Episode reward: 1699.5595403909683\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.943655  9.933845  9.9061775 8.832418  9.286276  9.937954 ]\n",
      "Reset environment\n",
      "Episode reward: -428.9821529388428\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9421425 9.932411  9.904592  8.83087   9.284908  9.936446 ]\n",
      "Reset environment\n",
      "Episode reward: 2184.1891446113586\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.944512 9.934772 9.906971 8.833422 9.287019 9.938818]\n",
      "Reset environment\n",
      "Episode reward: -206.29840767383575\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.94345  9.933608 9.906003 8.832293 9.286027 9.937766]\n",
      "Reset environment\n",
      "Episode reward: 3500.8593710865825\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.946287 9.936509 9.908771 8.835421 9.288627 9.940608]\n",
      "Reset environment\n",
      "Episode reward: 1620.9966409504414\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.947474  9.9376745 9.909982  8.83672   9.289693  9.941795 ]\n",
      "Reset environment\n",
      "Episode reward: 1727.5579141378403\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.948515 9.938763 9.910995 8.838111 9.290765 9.942844]\n",
      "Reset environment\n",
      "Episode reward: 2683.4270804673433\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.950652  9.940861  9.9131565 8.840425  9.292672  9.944978 ]\n",
      "Reset environment\n",
      "Episode reward: 4918.66911149025\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.954908 9.945119 9.917404 8.844982 9.296516 9.949235]\n",
      "Reset environment\n",
      "Episode reward: 2205.1279880404472\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.956645 9.946862 9.919132 8.846886 9.298075 9.950973]\n",
      "Reset environment\n",
      "Episode reward: 1663.6349281072617\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.957854 9.948066 9.920356 8.848239 9.29915  9.952183]\n",
      "Reset environment\n",
      "Episode reward: 4630.458800077438\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.961818 9.952038 9.924306 8.852505 9.30271  9.956144]\n",
      "Reset environment\n",
      "Episode reward: 2052.3003294467926\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.963378 9.953602 9.925862 8.854225 9.304094 9.957707]\n",
      "Reset environment\n",
      "Episode reward: 5511.163770735264\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.968221 9.958429 9.930722 8.859417 9.308457 9.96255 ]\n",
      "Reset environment\n",
      "Episode reward: 100.16344738006592\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.967332 9.957438 9.929934 8.858254 9.307774 9.961668]\n",
      "Reset environment\n",
      "Episode reward: 2388.597191721201\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.969141 9.959229 9.931772 8.860244 9.30939  9.96348 ]\n",
      "Reset environment\n",
      "Episode reward: 1379.0023741722107\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.970744  9.9608345 9.933376  8.862007  9.31081   9.965084 ]\n",
      "Reset environment\n",
      "Episode reward: 1828.4547519683838\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.97276  9.962848 9.935395 8.864205 9.312608 9.967099]\n",
      "Reset environment\n",
      "Episode reward: 1652.7979465723038\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.973984 9.964063 9.936626 8.865562 9.313684 9.968327]\n",
      "Reset environment\n",
      "Episode reward: 1356.0109274983406\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.97493   9.965028  9.9375515 8.866546  9.314631  9.969278 ]\n",
      "Reset environment\n",
      "Episode reward: 2007.7912846803665\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.976589  9.966741  9.939152  8.868317  9.31617   9.9709425]\n",
      "Reset environment\n",
      "Episode reward: 2422.1212782412767\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.97851  9.968721 9.941019 8.870416 9.317992 9.972881]\n",
      "Reset environment\n",
      "Episode reward: 1802.0135826468468\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.980479  9.970689  9.942989  8.8725815 9.319743  9.974857 ]\n",
      "Reset environment\n",
      "Episode reward: 2302.8688330724835\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.982006  9.972292  9.9444685 8.874524  9.321325  9.976402 ]\n",
      "Reset environment\n",
      "Episode reward: -368.3055924773216\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.980726 9.971008 9.943201 8.873112 9.320076 9.975136]\n",
      "Reset environment\n",
      "Episode reward: 1721.3205222859979\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.981826  9.9721775 9.944258  8.874535  9.321133  9.976251 ]\n",
      "Reset environment\n",
      "Episode reward: 5685.539245218039\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.964358 9.954648 9.926798 8.848502 9.304512 9.958744]\n",
      "Reset environment\n",
      "Episode reward: 1650.4205207824707\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9655905 9.955859  9.928051  8.849841  9.305628  9.959975 ]\n",
      "Reset environment\n",
      "Episode reward: 4081.817135453224\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.969049 9.959325 9.931496 8.853556 9.308738 9.963431]\n",
      "Reset environment\n",
      "Episode reward: 5639.393082439899\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.974009 9.964293 9.936442 8.858845 9.313251 9.968392]\n",
      "Reset environment\n",
      "Episode reward: 1284.3690617084503\n",
      "Total Steps: 41\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.974914 9.965199 9.937348 8.859862 9.314061 9.969299]\n",
      "Reset environment\n",
      "Episode reward: 3423.790544092655\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.977738 9.968066 9.940118 8.862949 9.316596 9.972121]\n",
      "Reset environment\n",
      "Episode reward: 2904.4853982031345\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.980058 9.970373 9.942454 8.865487 9.318688 9.974448]\n",
      "Reset environment\n",
      "Episode reward: 3202.0350613594055\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.982868 9.97321  9.94522  8.868585 9.321298 9.977263]\n",
      "Reset environment\n",
      "Episode reward: 3065.841811686754\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.985368 9.97573  9.947688 8.871291 9.323518 9.979764]\n",
      "Reset environment\n",
      "Episode reward: 2262.987250596285\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.986911 9.977227 9.949286 8.873034 9.324815 9.981312]\n",
      "Reset environment\n",
      "Episode reward: 1837.0125326141715\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.988003 9.978197 9.950508 8.87423  9.325759 9.982412]\n",
      "Reset environment\n",
      "Episode reward: 2306.449896246195\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.989727 9.979903 9.95226  8.876126 9.327301 9.984135]\n",
      "Reset environment\n",
      "Episode reward: 4332.763976804912\n",
      "Total Steps: 1043\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.980919 9.970962 9.94358  8.866429 9.319268 9.975324]\n",
      "Reset environment\n",
      "Episode reward: 2874.0032986700535\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9831705 9.973196  9.94586   8.868883  9.321289  9.977579 ]\n",
      "Reset environment\n",
      "Episode reward: 1383.6768299937248\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.983459 9.973363 9.946269 8.869324 9.321445 9.977877]\n",
      "Reset environment\n",
      "Episode reward: 2293.8734669089317\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.985251  9.975144  9.9480715 8.871299  9.32304   9.979669 ]\n",
      "Reset environment\n",
      "Episode reward: 2185.8463922142982\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.98691  9.976794 9.949737 8.873141 9.324505 9.981331]\n",
      "Reset environment\n",
      "Episode reward: 1664.8296180069447\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.987901 9.977845 9.950676 8.874299 9.325338 9.982329]\n",
      "Reset environment\n",
      "Episode reward: 1873.5974144935608\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.989302 9.97921  9.95211  8.875818 9.326613 9.983729]\n",
      "Reset environment\n",
      "Episode reward: 2097.7799421548843\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.991541 9.981462 9.954339 8.878265 9.328632 9.985972]\n",
      "Reset environment\n",
      "Episode reward: 2042.1635438203812\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.99373  9.983647 9.956529 8.880654 9.330583 9.988159]\n",
      "Reset environment\n",
      "Episode reward: 1899.3936694264412\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.995168 9.985061 9.957988 8.88224  9.331863 9.989597]\n",
      "Reset environment\n",
      "Episode reward: 2300.7568351626396\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.9976015 9.98749   9.960431  8.884874  9.334047  9.992033 ]\n",
      "Reset environment\n",
      "Episode reward: 1787.3135101795197\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [9.998934 9.988812 9.961767 8.886342 9.335226 9.993365]\n",
      "Reset environment\n",
      "Episode reward: 1919.7299262285233\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.00036   9.990222  9.963213  8.88791   9.336496  9.994789]\n",
      "Reset environment\n",
      "Episode reward: 5417.087046265602\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.004975  9.994812  9.967851  8.892896  9.340655  9.99941 ]\n",
      "Reset environment\n",
      "Episode reward: 2973.6641114354134\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.007463  9.99731   9.970321  8.895598  9.342917 10.001896]\n",
      "Reset environment\n",
      "Episode reward: 3139.764962941408\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.01      9.99989   9.972817  8.898374  9.345198 10.004437]\n",
      "Reset environment\n",
      "Episode reward: -125.66211521625519\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.009169  9.999058  9.971988  8.897361  9.344485 10.003611]\n",
      "Reset environment\n",
      "Episode reward: -688.9569400548935\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.007387  9.99729   9.970205  8.895501  9.342847 10.001834]\n",
      "Reset environment\n",
      "Episode reward: 2387.7345651295036\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.009189  9.999135  9.971955  8.897494  9.344485 10.003638]\n",
      "Reset environment\n",
      "Episode reward: 953.2864813804626\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.009772  9.999736  9.972523  8.898167  9.345006 10.004221]\n",
      "Reset environment\n",
      "Episode reward: -686.6414729356766\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.007959  9.997929  9.970706  8.896302  9.343348 10.002409]\n",
      "Reset environment\n",
      "Episode reward: 805.627631187439\n",
      "Total Steps: 25\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.008414  9.998393  9.971152  8.896844  9.343744 10.002864]\n",
      "Reset environment\n",
      "Episode reward: 3618.5165940225124\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.011444 10.001464  9.97413   8.900127  9.346461 10.005896]\n",
      "Reset environment\n",
      "Episode reward: 2336.5891313552856\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.013298 10.003328  9.975975  8.902164  9.34814  10.00775 ]\n",
      "Reset environment\n",
      "Episode reward: 2166.412733376026\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.015578 10.005608  9.978247  8.904658  9.350189 10.010031]\n",
      "Reset environment\n",
      "Episode reward: 4090.0174986720085\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.018842  10.008865   9.981514   8.9081745  9.353117  10.013297 ]\n",
      "Reset environment\n",
      "Episode reward: 2631.917712420225\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.020863 10.010853  9.983562  8.91038   9.354905 10.015316]\n",
      "Reset environment\n",
      "Episode reward: 1434.2725783586502\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.022508 10.012494  9.985204  8.912187  9.356366 10.016959]\n",
      "Reset environment\n",
      "Episode reward: 5129.755376756191\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.026888 10.016861  9.989607  8.916897  9.36035  10.021344]\n",
      "Reset environment\n",
      "Episode reward: 912.3542342185974\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.027304 10.017269  9.990021  8.917328  9.360796 10.021761]\n",
      "Reset environment\n",
      "Episode reward: 2394.4341161102057\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.029037 10.019069  9.991688  8.919249  9.362353 10.023499]\n",
      "Reset environment\n",
      "Episode reward: 3333.908025354147\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.031757 10.021802  9.994391  8.922192  9.364775 10.026222]\n",
      "Reset environment\n",
      "Episode reward: 4161.305683225393\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.0352125 10.025305   9.997781   8.925938   9.36786   10.029671 ]\n",
      "Reset environment\n",
      "Episode reward: 3403.2902918457985\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.038013  10.028124  10.0005665  8.928961   9.370362  10.0324745]\n",
      "Reset environment\n",
      "Episode reward: 1810.4114260673523\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.039387 10.029503 10.001929  8.930462  9.371602 10.033842]\n",
      "Reset environment\n",
      "Episode reward: 235.1232647895813\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.038639 10.028664 10.001264  8.929422  9.371076 10.033101]\n",
      "Reset environment\n",
      "Episode reward: 3001.4384887218475\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.041084 10.031115 10.003701  8.932061  9.373268 10.035548]\n",
      "Reset environment\n",
      "Episode reward: 4652.099495470524\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.044949 10.034955 10.007598  8.93626   9.37678  10.039415]\n",
      "Reset environment\n",
      "Episode reward: 2217.391119003296\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.047284 10.037294 10.009929  8.938798  9.378892 10.04175 ]\n",
      "Reset environment\n",
      "Episode reward: 1629.4169417619705\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.048464 10.03845  10.011129  8.940089  9.379952 10.042929]\n",
      "Reset environment\n",
      "Episode reward: 1454.0263189077377\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.050114 10.040091 10.012788  8.941906  9.381412 10.044579]\n",
      "Reset environment\n",
      "Episode reward: 3892.495634049177\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.053323 10.043281 10.016015  8.945367  9.384313 10.04779 ]\n",
      "Reset environment\n",
      "Episode reward: 303.85797250270844\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.052635  10.042473  10.015459   8.9445305  9.383802  10.047115 ]\n",
      "Reset environment\n",
      "Episode reward: -191.58593082427979\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.051632 10.041381 10.014543  8.943463  9.382873 10.046119]\n",
      "Reset environment\n",
      "Episode reward: 1687.1581127727404\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.05281  10.042529 10.015753  8.944761  9.383919 10.047298]\n",
      "Reset environment\n",
      "Episode reward: 2150.5165766477585\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.055091 10.044806 10.018035  8.947236  9.385957 10.049579]\n",
      "Reset environment\n",
      "Episode reward: 2529.29193764925\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.056815 10.046478 10.019822  8.949185  9.387391 10.05131 ]\n",
      "Reset environment\n",
      "Episode reward: 2905.3240435123444\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.059141  10.0488205 10.022124   8.951725   9.389454  10.053637 ]\n",
      "Reset environment\n",
      "Episode reward: 2164.2096813842654\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.060827 10.050587 10.023732  8.953536  9.390985 10.055328]\n",
      "Reset environment\n",
      "Episode reward: -681.466268658638\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.05906  10.048825 10.021968  8.951675  9.389346 10.053569]\n",
      "Reset environment\n",
      "Episode reward: 3173.404153883457\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.061721 10.051474 10.02463   8.95455   9.391746 10.056227]\n",
      "Reset environment\n",
      "Episode reward: 223.56027960777283\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.0609255 10.05051   10.024008   8.95367    9.390991  10.0554495]\n",
      "Reset environment\n",
      "Episode reward: 2137.6322079896927\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.063156  10.052733  10.0262375  8.9560995  9.392973  10.057673 ]\n",
      "Reset environment\n",
      "Episode reward: 3284.531378865242\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.065841  10.055424  10.0289135  8.958984   9.395396  10.060359 ]\n",
      "Reset environment\n",
      "Episode reward: -708.435240983963\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.063518 10.053285 10.026436  8.956671  9.393335 10.058056]\n",
      "Reset environment\n",
      "Episode reward: 2957.4130367338657\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.065547 10.055387 10.028418  8.958949  9.395224 10.060095]\n",
      "Reset environment\n",
      "Episode reward: 1725.2088908106089\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.06649   10.056396  10.02932    8.959963   9.396116  10.0610485]\n",
      "Reset environment\n",
      "Episode reward: 1974.1036737710238\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.067877 10.057751 10.030741  8.961486  9.397352 10.062436]\n",
      "Reset environment\n",
      "Episode reward: -682.9921435117722\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.066084 10.055968 10.028943  8.959519  9.395722 10.060646]\n",
      "Reset environment\n",
      "Episode reward: 1099.9300849437714\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.066792  10.056664  10.029663   8.9603195  9.396349  10.061354 ]\n",
      "Reset environment\n",
      "Episode reward: 3433.996776819229\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.069595  10.059473  10.032458   8.963345   9.398881  10.0641575]\n",
      "Reset environment\n",
      "Episode reward: 1963.4576333761215\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.071063  10.06095   10.033914   8.9649515  9.4001875 10.065626 ]\n",
      "Reset environment\n",
      "Episode reward: 1355.9139103889465\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.071794 10.061608 10.034714  8.965809  9.400838 10.066356]\n",
      "Reset environment\n",
      "Episode reward: 2416.0592544078827\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.073657 10.063484 10.036549  8.967847  9.402485 10.068222]\n",
      "Reset environment\n",
      "Episode reward: 2883.9799838662148\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.075989 10.065808 10.038889  8.970382  9.404572 10.070551]\n",
      "Reset environment\n",
      "Episode reward: 1328.543562054634\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.07678  10.066666 10.039625  8.971117  9.405347 10.071354]\n",
      "Reset environment\n",
      "Episode reward: 3847.3776733353734\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.079825 10.06979  10.04259   8.97447   9.408151 10.074412]\n",
      "Reset environment\n",
      "Episode reward: 4422.1923815608025\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.083508 10.073452 10.04629   8.978427  9.411465 10.0781  ]\n",
      "Reset environment\n",
      "Episode reward: 2364.637530207634\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.08594   10.075889  10.0487175  8.981077   9.413654  10.080538 ]\n",
      "Reset environment\n",
      "Episode reward: 1416.6410132050514\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.087549 10.077493 10.050321  8.982846  9.415071 10.082148]\n",
      "Reset environment\n",
      "Episode reward: 2322.3488889336586\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.0899725 10.079905  10.05275    8.985473   9.417237  10.08457  ]\n",
      "Reset environment\n",
      "Episode reward: 2697.882650613785\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.09191  10.081912 10.054614  8.987622  9.418984 10.086513]\n",
      "Reset environment\n",
      "Episode reward: 4153.964331641793\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.095302 10.085261 10.058049  8.991276  9.422052 10.08991 ]\n",
      "Reset environment\n",
      "Episode reward: 4260.61986964941\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.09883  10.088769 10.061604  8.995072  9.425245 10.093439]\n",
      "Reset environment\n",
      "Episode reward: 1758.5328100323677\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.1001215 10.090058  10.062894   8.9965105  9.426395  10.09473  ]\n",
      "Reset environment\n",
      "Episode reward: 1837.7488420950249\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.101128 10.091128 10.063862  8.997688  9.427418 10.095756]\n",
      "Reset environment\n",
      "Episode reward: 2738.3788625597954\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.103254 10.093237 10.066007  9.000006  9.429338 10.097882]\n",
      "Reset environment\n",
      "Episode reward: 2019.8442063331604\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.104762 10.094746 10.067505  9.001661  9.430684 10.099388]\n",
      "Reset environment\n",
      "Episode reward: 1837.8237227499485\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.10628  10.096316 10.068969  9.003291  9.432076 10.10091 ]\n",
      "Reset environment\n",
      "Episode reward: 2924.7133156061172\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.108698 10.098737 10.071382  9.005854  9.434361 10.103334]\n",
      "Reset environment\n",
      "Episode reward: 2916.0770328342915\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.110956 10.100969 10.073672  9.008311  9.436404 10.105595]\n",
      "Reset environment\n",
      "Episode reward: 3526.058861374855\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.113679  10.103692  10.076396   9.011262   9.4388275 10.108316 ]\n",
      "Reset environment\n",
      "Episode reward: 1317.6414036750793\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.1145735 10.104616  10.077261   9.012264   9.439623  10.109212 ]\n",
      "Reset environment\n",
      "Episode reward: 1457.4459872245789\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.115581 10.105597 10.078294  9.013372  9.440528 10.11022 ]\n",
      "Reset environment\n",
      "Episode reward: 2987.4354572743177\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.117592 10.107688 10.080248  9.015632  9.442358 10.112245]\n",
      "Reset environment\n",
      "Episode reward: 3931.37775850296\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.120822 10.110903 10.083496  9.019103  9.445275 10.115474]\n",
      "Reset environment\n",
      "Episode reward: 2114.5694807469845\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.122414 10.112534 10.085048  9.020859  9.446707 10.117066]\n",
      "Reset environment\n",
      "Episode reward: 1223.7232261300087\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.123113 10.11328  10.085702  9.021542  9.447368 10.117768]\n",
      "Reset environment\n",
      "Episode reward: 3513.833251103759\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.125893 10.116125 10.088417  9.02459   9.449915 10.120551]\n",
      "Reset environment\n",
      "Episode reward: 1950.9885262846947\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.127349 10.117584 10.089865  9.02619   9.451224 10.122005]\n",
      "Reset environment\n",
      "Episode reward: 2314.2820209264755\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.129746  10.1199875 10.092255   9.028791   9.453386  10.124405 ]\n",
      "Reset environment\n",
      "Episode reward: 3428.5079701542854\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.132552 10.122796 10.095044  9.031809  9.455915 10.12721 ]\n",
      "Reset environment\n",
      "Episode reward: 1998.2968569099903\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.133611  10.123905  10.096074   9.0331135  9.456977  10.128285 ]\n",
      "Reset environment\n",
      "Episode reward: 1872.4848774820566\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.134505 10.124854 10.096948  9.034238  9.457879 10.129194]\n",
      "Reset environment\n",
      "Episode reward: 1378.9345818161964\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.136056  10.126403  10.0984955  9.035946   9.459252  10.130741 ]\n",
      "Reset environment\n",
      "Episode reward: 2038.3326444625854\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.138167 10.128508 10.100611  9.038251  9.461138 10.132852]\n",
      "Reset environment\n",
      "Episode reward: 2268.174945577979\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.139561 10.129966 10.101967  9.040014  9.462463 10.134258]\n",
      "Reset environment\n",
      "Episode reward: 2986.50146985054\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.141914  10.132301  10.104334   9.042572   9.4645815 10.136609 ]\n",
      "Reset environment\n",
      "Episode reward: 1366.320095360279\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.143449 10.133835 10.105868  9.044258  9.465938 10.138145]\n",
      "Reset environment\n",
      "Episode reward: 1528.7551397383213\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.14439  10.134727 10.106854  9.045316  9.466787 10.139087]\n",
      "Reset environment\n",
      "Episode reward: 2612.355292111635\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.146342 10.136647 10.108841  9.047456  9.468535 10.141034]\n",
      "Reset environment\n",
      "Episode reward: 1879.7375229597092\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.147664 10.137954 10.110188  9.048926  9.469711 10.14236 ]\n",
      "Reset environment\n",
      "Episode reward: 4242.7582884430885\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.151205 10.141483 10.113734  9.052721  9.472902 10.145901]\n",
      "Reset environment\n",
      "Episode reward: 5242.94092798233\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.15623  10.146487 10.118759  9.058138  9.477418 10.150925]\n",
      "Reset environment\n",
      "Episode reward: -686.0377591848373\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.154446  10.144718  10.1169615  9.056214   9.47579   10.149143 ]\n",
      "Reset environment\n",
      "Episode reward: 2888.325629711151\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.156734 10.146987 10.119257  9.058703  9.477848 10.151427]\n",
      "Reset environment\n",
      "Episode reward: 2032.2944761067629\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.1579485 10.14826   10.120431   9.060201   9.479058  10.152654 ]\n",
      "Reset environment\n",
      "Episode reward: 2106.1443195343018\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.159542 10.149864 10.122012  9.061947  9.480495 10.154247]\n",
      "Reset environment\n",
      "Episode reward: 3295.771004140377\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.162173 10.152482 10.124651  9.064804  9.482855 10.15688 ]\n",
      "Reset environment\n",
      "Episode reward: 5131.568617880344\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.167103  10.1573925 10.129596   9.070116   9.487292  10.161808 ]\n",
      "Reset environment\n",
      "Episode reward: 1332.8351607322693\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.168575  10.15886   10.131067   9.0717535  9.48859   10.163278 ]\n",
      "Reset environment\n",
      "Episode reward: 5153.976054370403\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.172937 10.163231 10.13542   9.076412  9.492521 10.167643]\n",
      "Reset environment\n",
      "Episode reward: 1707.9196914583445\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.173839 10.164181 10.136294  9.077519  9.493445 10.168559]\n",
      "Reset environment\n",
      "Episode reward: 4941.264276981354\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.177935 10.168251 10.140401  9.081928  9.497118 10.172647]\n",
      "Reset environment\n",
      "Episode reward: 1317.0688503980637\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.178589 10.168973 10.14101   9.082568  9.497776 10.17331 ]\n",
      "Reset environment\n",
      "Episode reward: 2069.5331007242203\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.180177 10.17055  10.142599  9.084292  9.4992   10.174894]\n",
      "Reset environment\n",
      "Episode reward: 709.0463281869888\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.179793 10.170077 10.142306  9.083829  9.49882  10.174524]\n",
      "Reset environment\n",
      "Episode reward: 2640.0536876916885\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.181847 10.172118 10.144363  9.086064  9.500647 10.176571]\n",
      "Reset environment\n",
      "Episode reward: 2209.3825346827507\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.184142 10.174423 10.146651  9.088551  9.502718 10.178868]\n",
      "Reset environment\n",
      "Episode reward: 4337.720501899719\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.187529  10.1778145 10.150043   9.092228   9.505737  10.182259 ]\n",
      "Reset environment\n",
      "Episode reward: 1961.5167563557625\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.189535 10.179828 10.152043  9.094441  9.507525 10.184269]\n",
      "Reset environment\n",
      "Episode reward: 5400.119438350201\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.194076  10.184358  10.156585   9.099308   9.511616  10.1888075]\n",
      "Reset environment\n",
      "Episode reward: 3130.4571403563023\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.196526 10.186777 10.159071  9.101947  9.513823 10.191261]\n",
      "Reset environment\n",
      "Episode reward: 1415.7527849078178\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.1981   10.188349 10.160644  9.103667  9.515217 10.192835]\n",
      "Reset environment\n",
      "Episode reward: 3679.961401849985\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.200987  10.1911955 10.163571   9.106789   9.5178    10.1957245]\n",
      "Reset environment\n",
      "Episode reward: 3081.0652497708797\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.203401 10.193571 10.166016  9.109391  9.519974 10.198133]\n",
      "Reset environment\n",
      "Episode reward: 3875.8964250683784\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.206395 10.196574 10.169013  9.112625  9.522662 10.201131]\n",
      "Reset environment\n",
      "Episode reward: -680.9000271558762\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.204614 10.194804 10.167218  9.11071   9.521021 10.199358]\n",
      "Reset environment\n",
      "Episode reward: 2311.7466656500474\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.206275 10.196429 10.16891   9.11253   9.522501 10.201022]\n",
      "Reset environment\n",
      "Episode reward: 3534.04137711972\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.209081 10.199198 10.171755  9.115555  9.525035 10.203829]\n",
      "Reset environment\n",
      "Episode reward: 1373.814211845398\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.210033 10.200133 10.172725  9.116609  9.525888 10.204782]\n",
      "Reset environment\n",
      "Episode reward: 4527.430850625038\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.213758 10.203834 10.176462  9.120609  9.529239 10.208502]\n",
      "Reset environment\n",
      "Episode reward: 5495.522016823292\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.218381 10.208464 10.181073  9.125539  9.533447 10.213125]\n",
      "Reset environment\n",
      "Episode reward: 2798.0697843432426\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.2205305 10.210654  10.183184   9.127906   9.5353565 10.215278 ]\n",
      "Reset environment\n",
      "Episode reward: 2153.2258977890015\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.222749 10.21288  10.185401  9.130327  9.537357 10.217498]\n",
      "Reset environment\n",
      "Episode reward: 4645.509535014629\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.226571 10.216714 10.189214  9.134425  9.540817 10.221322]\n",
      "Reset environment\n",
      "Episode reward: 4581.235182464123\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.23036   10.220493  10.192994   9.138498   9.5442505 10.225112 ]\n",
      "Reset environment\n",
      "Episode reward: 2410.030981659889\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.232816 10.222938 10.195452  9.141153  9.546435 10.227567]\n",
      "Reset environment\n",
      "Episode reward: -349.6189638376236\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.231499  10.221617  10.194139   9.1398735  9.545146  10.226254 ]\n",
      "Reset environment\n",
      "Episode reward: 5991.068349540234\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.236374 10.226492 10.199024  9.145101  9.549516 10.231132]\n",
      "Reset environment\n",
      "Episode reward: 1728.1357971429825\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.238178 10.228292 10.200826  9.147102  9.551111 10.232937]\n",
      "Reset environment\n",
      "Episode reward: 4351.572325468063\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.241728 10.231837 10.204388  9.150928  9.554311 10.23649 ]\n",
      "Reset environment\n",
      "Episode reward: 2270.930321276188\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.244043 10.234146 10.206716  9.153436  9.556383 10.238806]\n",
      "Reset environment\n",
      "Episode reward: 3819.730977676809\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.2470875 10.2371645 10.209784   9.156727   9.559127  10.241847 ]\n",
      "Reset environment\n",
      "Episode reward: 4780.21358448267\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.251075 10.241158 10.213754  9.160985  9.562728 10.245838]\n",
      "Reset environment\n",
      "Episode reward: 2562.8335505723953\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.253034 10.243121 10.215693  9.163112  9.564471 10.247795]\n",
      "Reset environment\n",
      "Episode reward: 4728.305103778839\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.256731  10.246812  10.219399   9.167107   9.5677805 10.251499 ]\n",
      "Reset environment\n",
      "Episode reward: 2701.4543630480766\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.258732 10.248782 10.221444  9.169284  9.569598 10.2535  ]\n",
      "Reset environment\n",
      "Episode reward: 2185.3672924637794\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.260367  10.250409  10.223086   9.171066   9.571051  10.2551365]\n",
      "Reset environment\n",
      "Episode reward: 5520.733749091625\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.265003 10.255051 10.227713  9.175997  9.575247 10.259775]\n",
      "Reset environment\n",
      "Episode reward: 1834.6809651851654\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.266245  10.2562475 10.229002   9.177364   9.576365  10.261015 ]\n",
      "Reset environment\n",
      "Episode reward: 2095.712068736553\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.267757 10.257745 10.230534  9.179026  9.577715 10.262528]\n",
      "Reset environment\n",
      "Episode reward: 1739.694825410843\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.269594  10.25958   10.2323675  9.181043   9.579346  10.264363 ]\n",
      "Reset environment\n",
      "Episode reward: 5295.1523087620735\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.273823 10.263822 10.2366    9.185594  9.583174 10.268595]\n",
      "Reset environment\n",
      "Episode reward: 1825.5658363699913\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.275131 10.265121 10.237913  9.187049  9.584331 10.269904]\n",
      "Reset environment\n",
      "Episode reward: 1920.7392873167992\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.27713  10.267115 10.239915  9.189233  9.586114 10.271904]\n",
      "Reset environment\n",
      "Episode reward: 2300.127397418022\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.278891 10.268877 10.241673  9.191159  9.587699 10.273667]\n",
      "Reset environment\n",
      "Episode reward: 3898.0810111165047\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.281931 10.271911 10.244721  9.194431  9.590415 10.27671 ]\n",
      "Reset environment\n",
      "Episode reward: 1575.4357554912567\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.283022 10.272993 10.245814  9.195646  9.591377 10.2778  ]\n",
      "Reset environment\n",
      "Episode reward: 2357.231084406376\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.284759 10.274713 10.247565  9.197545  9.592939 10.279532]\n",
      "Reset environment\n",
      "Episode reward: 951.6471819877625\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.285323 10.275278 10.248127  9.198196  9.593433 10.280096]\n",
      "Reset environment\n",
      "Episode reward: 1842.2320174574852\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.286621 10.276586 10.249416  9.199636  9.594584 10.281394]\n",
      "Reset environment\n",
      "Episode reward: 5583.101195514202\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.2913   10.281271 10.25409   9.204628  9.598857 10.286075]\n",
      "Reset environment\n",
      "Episode reward: 2166.064627826214\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.292673 10.282587 10.255524  9.206182  9.600016 10.28745 ]\n",
      "Reset environment\n",
      "Episode reward: 1677.3474719524384\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.292554 10.282414 10.255442  9.206199  9.59967  10.28733 ]\n",
      "Reset environment\n",
      "Episode reward: 1857.4332072138786\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.294478 10.284341 10.257361  9.208313  9.601393 10.28926 ]\n",
      "Reset environment\n",
      "Episode reward: 5372.741631388664\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.299613 10.289483 10.26249   9.213776  9.606074 10.2944  ]\n",
      "Reset environment\n",
      "Episode reward: 1707.913476318121\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.300716 10.290548 10.263631  9.214991  9.607079 10.295502]\n",
      "Reset environment\n",
      "Episode reward: 1324.1418367624283\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.302138 10.291972 10.265046  9.216582  9.608326 10.296924]\n",
      "Reset environment\n",
      "Episode reward: 3172.707531630993\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.304574 10.294426 10.267459  9.219256  9.61049  10.299363]\n",
      "Reset environment\n",
      "Episode reward: 2057.9463698267937\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.306695  10.296545  10.269584   9.2215605  9.612387  10.301485 ]\n",
      "Reset environment\n",
      "Episode reward: 1657.2665497045964\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.307749 10.29768  10.270554  9.222657  9.613351 10.302544]\n",
      "Reset environment\n",
      "Episode reward: 4490.83795440197\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.3113985 10.301323  10.274211   9.226582   9.616684  10.306191 ]\n",
      "Reset environment\n",
      "Episode reward: 2152.1593149900436\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.313586 10.303505 10.276402  9.228969  9.618632 10.308376]\n",
      "Reset environment\n",
      "Episode reward: 1227.4774515628815\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.31441  10.304329 10.277223  9.229878  9.619364 10.309197]\n",
      "Reset environment\n",
      "Episode reward: 2158.4375071525574\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.316623 10.30654  10.279444  9.23227   9.621344 10.311413]\n",
      "Reset environment\n",
      "Episode reward: 3322.0402297973633\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.319228 10.309151 10.28204   9.235089  9.623696 10.314019]\n",
      "Reset environment\n",
      "Episode reward: 3577.080959826708\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.322009 10.311946 10.284798  9.238106  9.626172 10.316799]\n",
      "Reset environment\n",
      "Episode reward: 2115.0861536860466\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.323561 10.313481 10.286354  9.239806  9.627551 10.31835 ]\n",
      "Reset environment\n",
      "Episode reward: 1747.0939210653305\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.325368 10.315299 10.288149  9.241801  9.629156 10.320154]\n",
      "Reset environment\n",
      "Episode reward: 5765.027545571327\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.330199 10.32011  10.292991  9.246956  9.633506 10.324985]\n",
      "Reset environment\n",
      "Episode reward: 1896.4573159217834\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.331523 10.321387 10.294357  9.248436  9.634661 10.326308]\n",
      "Reset environment\n",
      "Episode reward: 1497.7169103622437\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.332517 10.322417 10.295313  9.249554  9.635543 10.327301]\n",
      "Reset environment\n",
      "Episode reward: 2051.5921556949615\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.333599 10.323558 10.296365  9.250895  9.636587 10.328393]\n",
      "Reset environment\n",
      "Episode reward: 597.3335347175598\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.333082 10.323161 10.295735  9.250315  9.636129 10.327883]\n",
      "Reset environment\n",
      "Episode reward: 1971.1088953316212\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.33447  10.32453  10.297142  9.251833  9.637367 10.329271]\n",
      "Reset environment\n",
      "Episode reward: 1415.3901374936104\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.336006 10.326071 10.298677  9.253528  9.638728 10.330812]\n",
      "Reset environment\n",
      "Episode reward: 1920.2265747189522\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.337978 10.32804  10.300652  9.255688  9.640492 10.332782]\n",
      "Reset environment\n",
      "Episode reward: 2191.988901913166\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.339532 10.329564 10.302236  9.25739   9.641874 10.334335]\n",
      "Reset environment\n",
      "Episode reward: 2067.268400525674\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.341101  10.331096  10.303839   9.2590275  9.643368  10.335906 ]\n",
      "Reset environment\n",
      "Episode reward: 4247.393720090389\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.3445   10.334478 10.307255  9.262713  9.646408 10.339305]\n",
      "Reset environment\n",
      "Episode reward: 4080.7472915053368\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.347626 10.337602 10.310386  9.266108  9.649198 10.342432]\n",
      "Reset environment\n",
      "Episode reward: 2531.123080968857\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.349512  10.339493  10.312267   9.268183   9.6508875 10.344318 ]\n",
      "Reset environment\n",
      "Episode reward: 2544.4168615937233\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.35145  10.341451 10.314186  9.270308  9.652648 10.346254]\n",
      "Reset environment\n",
      "Episode reward: 3440.8798360824585\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.354024 10.343974 10.316806  9.273118  9.654921 10.348826]\n",
      "Reset environment\n",
      "Episode reward: 1386.6696237921715\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.3555355 10.345487  10.31832    9.274778   9.656264  10.35034  ]\n",
      "Reset environment\n",
      "Episode reward: 1936.9498236775398\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.356943 10.346892 10.319728  9.276324  9.657511 10.351747]\n",
      "Reset environment\n",
      "Episode reward: 3814.6724560260773\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.359978 10.349942 10.322738  9.279588  9.660222 10.354779]\n",
      "Reset environment\n",
      "Episode reward: 1712.2549316883087\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.3611765 10.351179  10.323903   9.280918   9.661299  10.355976 ]\n",
      "Reset environment\n",
      "Episode reward: 3279.5927755236626\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.363722 10.353708 10.326471  9.283668  9.663605 10.358522]\n",
      "Reset environment\n",
      "Episode reward: 2594.462191656232\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.365261 10.355324 10.327974  9.285502  9.665015 10.360072]\n",
      "Reset environment\n",
      "Episode reward: 4323.471686959267\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.368759  10.358802  10.3314905  9.2892475  9.668168  10.363567 ]\n",
      "Reset environment\n",
      "Episode reward: 4530.507501840591\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.372418 10.362454 10.335161  9.29318   9.671469 10.367225]\n",
      "Reset environment\n",
      "Episode reward: 3084.1560631096363\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.374858 10.364973 10.337534  9.295937  9.673714 10.369678]\n",
      "Reset environment\n",
      "Episode reward: 545.4079306125641\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.374718 10.364862 10.337354  9.295626  9.673629 10.369542]\n",
      "Reset environment\n",
      "Episode reward: 1649.8968921899796\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.375868 10.366018 10.3385    9.296899  9.67466  10.370692]\n",
      "Reset environment\n",
      "Episode reward: 2723.8903890252113\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.377921 10.368068 10.340559  9.299148  9.676486 10.372745]\n",
      "Reset environment\n",
      "Episode reward: 4848.566046237946\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.381849 10.371973 10.344507  9.30339   9.680023 10.376668]\n",
      "Reset environment\n",
      "Episode reward: 2092.66757106781\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.383971 10.374103 10.346625  9.305703  9.681927 10.378797]\n",
      "Reset environment\n",
      "Episode reward: 2097.196241259575\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.386125  10.3762455 10.348783   9.308032   9.683844  10.380951 ]\n",
      "Reset environment\n",
      "Episode reward: 2377.78068870306\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.387735 10.377818 10.350432  9.309824  9.685236 10.382563]\n",
      "Reset environment\n",
      "Episode reward: 2141.6316191256046\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.38891  10.379047 10.351585  9.311372  9.686384 10.383753]\n",
      "Reset environment\n",
      "Episode reward: 4936.261935770512\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.392928  10.38305   10.3556185  9.31571    9.690001  10.387769 ]\n",
      "Reset environment\n",
      "Episode reward: -82.42705726623535\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.392009  10.382011  10.354809   9.314793   9.6890545 10.386851 ]\n",
      "Reset environment\n",
      "Episode reward: 2047.5006878972054\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.393499 10.383483 10.356325  9.316407  9.690397 10.388345]\n",
      "Reset environment\n",
      "Episode reward: 3362.960195168853\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.396106 10.386063 10.358957  9.319213  9.692751 10.39095 ]\n",
      "Reset environment\n",
      "Episode reward: 4414.231408417225\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.399628 10.389573 10.362495  9.323018  9.695943 10.394475]\n",
      "Reset environment\n",
      "Episode reward: 1610.687349319458\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.400751 10.390677 10.363644  9.324244  9.696958 10.395598]\n",
      "Reset environment\n",
      "Episode reward: 1617.3230484127998\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.40194  10.391909 10.364782  9.325495  9.698045 10.39679 ]\n",
      "Reset environment\n",
      "Episode reward: -830.6527674719691\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.399501 10.389659 10.362182  9.323062  9.69583  10.394374]\n",
      "Reset environment\n",
      "Episode reward: 5301.670211493969\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.403886 10.394032 10.366568  9.327748  9.69978  10.398753]\n",
      "Reset environment\n",
      "Episode reward: 3176.9508233070374\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.4063635 10.396494  10.369053   9.330426   9.702025  10.401227 ]\n",
      "Reset environment\n",
      "Episode reward: 3768.6111998558044\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.409419 10.39955  10.372099  9.333723  9.704777 10.404285]\n",
      "Reset environment\n",
      "Episode reward: 2298.786099454388\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.410679 10.400879 10.373321  9.335315  9.70596  10.405559]\n",
      "Reset environment\n",
      "Episode reward: 2175.8660512566566\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.412227  10.402441  10.37486    9.33702    9.7073345 10.407108 ]\n",
      "Reset environment\n",
      "Episode reward: 2939.9006158709526\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.414415 10.404546 10.377134  9.339378  9.709338 10.409302]\n",
      "Reset environment\n",
      "Episode reward: -682.1939988136292\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.412599 10.402715 10.375362  9.337625  9.707686 10.407498]\n",
      "Reset environment\n",
      "Episode reward: 2820.3334067463875\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.414716  10.404847  10.3774605  9.3399315  9.709565  10.409617 ]\n",
      "Reset environment\n",
      "Episode reward: 3055.215472340584\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.417029 10.407232 10.37972   9.342566  9.711708 10.411949]\n",
      "Reset environment\n",
      "Episode reward: 1831.8911377191544\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.418888 10.409098 10.381577  9.344617  9.713362 10.41381 ]\n",
      "Reset environment\n",
      "Episode reward: 2178.310819864273\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.421076 10.411289 10.383762  9.346991  9.715325 10.415995]\n",
      "Reset environment\n",
      "Episode reward: 843.8943448066711\n",
      "Total Steps: 27\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.421528 10.411748 10.384203  9.347523  9.715724 10.416447]\n",
      "Reset environment\n",
      "Episode reward: 2577.308113798499\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.423321  10.4134865 10.386046   9.349489   9.717326  10.4182415]\n",
      "Reset environment\n",
      "Episode reward: 1845.6636667251587\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.424646 10.414795 10.387384  9.350946  9.718504 10.419567]\n",
      "Reset environment\n",
      "Episode reward: 2265.4107826948166\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.426325  10.416485  10.389049   9.35279    9.720004  10.4212475]\n",
      "Reset environment\n",
      "Episode reward: 2780.1087406277657\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.428408 10.418574 10.391134  9.355066  9.721884 10.423334]\n",
      "Reset environment\n",
      "Episode reward: -525.6899604201317\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.426538 10.41654  10.389459  9.353106  9.72019  10.421487]\n",
      "Reset environment\n",
      "Episode reward: 5331.309400439262\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.430722 10.42072  10.393638  9.357615  9.723945 10.425671]\n",
      "Reset environment\n",
      "Episode reward: 3550.1159875765443\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.433442 10.423505 10.396298  9.36059   9.726415 10.428392]\n",
      "Reset environment\n",
      "Episode reward: -339.95990735292435\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.432162 10.422221 10.395012  9.359362  9.725127 10.427106]\n",
      "Reset environment\n",
      "Episode reward: 808.6587476730347\n",
      "Total Steps: 25\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.432599 10.422662 10.395441  9.359873  9.725505 10.427541]\n",
      "Reset environment\n",
      "Episode reward: -337.41977858543396\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.431291 10.421354 10.394137  9.358565  9.724224 10.426231]\n",
      "Reset environment\n",
      "Episode reward: 1584.5974853038788\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.43239  10.422441 10.39525   9.359787  9.725202 10.427331]\n",
      "Reset environment\n",
      "Episode reward: 4887.567576885223\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.436919  10.4269495 10.3997965  9.364667   9.72926   10.43186  ]\n",
      "Reset environment\n",
      "Episode reward: 4587.192667663097\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.440572 10.430615 10.403441  9.368606  9.732531 10.43552 ]\n",
      "Reset environment\n",
      "Episode reward: 2289.322423040867\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.442241 10.432296 10.40511   9.370435  9.734025 10.437194]\n",
      "Reset environment\n",
      "Episode reward: 4265.972896784544\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.445592 10.435678 10.408431  9.374066  9.73703  10.440549]\n",
      "Reset environment\n",
      "Episode reward: 701.1350150108337\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.446033 10.436112 10.408868  9.374497  9.737477 10.440991]\n",
      "Reset environment\n",
      "Episode reward: 2962.2605154514313\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.448268 10.438354 10.411101  9.376938  9.739502 10.443235]\n",
      "Reset environment\n",
      "Episode reward: 2352.2793449908495\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.449958 10.440007 10.412833  9.378766  9.741043 10.444923]\n",
      "Reset environment\n",
      "Episode reward: 4933.167307198048\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.4539175 10.443948  10.4168     9.38302    9.744606  10.448884 ]\n",
      "Reset environment\n",
      "Episode reward: 4665.111042588949\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.457595 10.447616 10.420497  9.386998  9.747943 10.452558]\n",
      "Reset environment\n",
      "Episode reward: 41.81539857387543\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.456449  10.4466095 10.41923    9.385745   9.746925  10.45142  ]\n",
      "Reset environment\n",
      "Episode reward: 2875.4974056482315\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.458615 10.448761 10.421408  9.388099  9.748856 10.453583]\n",
      "Reset environment\n",
      "Episode reward: -343.50199949741364\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.457197  10.447315  10.420083   9.3866005  9.747655  10.452194 ]\n",
      "Reset environment\n",
      "Episode reward: 1381.4160086512566\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.458686 10.448796 10.42157   9.388239  9.748967 10.45368 ]\n",
      "Reset environment\n",
      "Episode reward: 2532.6818786859512\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.460112  10.450281  10.4229765  9.389993   9.750313  10.455116 ]\n",
      "Reset environment\n",
      "Episode reward: 2136.169738858938\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.461296 10.451361 10.424277  9.391309  9.751303 10.456313]\n",
      "Reset environment\n",
      "Episode reward: 2157.0654761195183\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.463456 10.453525 10.426435  9.393648  9.753251 10.458473]\n",
      "Reset environment\n",
      "Episode reward: 3489.972296386957\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.465848 10.455829 10.428911  9.396321  9.755286 10.460865]\n",
      "Reset environment\n",
      "Episode reward: 2827.268236577511\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.4679575 10.457932  10.431032   9.3986225  9.757182  10.462975 ]\n",
      "Reset environment\n",
      "Episode reward: 2086.5548855662346\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.47004  10.460006 10.433114  9.400886  9.759036 10.465055]\n",
      "Reset environment\n",
      "Episode reward: 2758.5843154788017\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.47192  10.461838 10.435049  9.402964  9.760654 10.466939]\n",
      "Reset environment\n",
      "Episode reward: 3596.867290377617\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.474609 10.464603 10.437659  9.405921  9.763097 10.469637]\n",
      "Reset environment\n",
      "Episode reward: 4093.0880559682846\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.477726 10.467798 10.440696  9.409358  9.765938 10.472759]\n",
      "Reset environment\n",
      "Episode reward: 2483.216549679637\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.479599  10.469766  10.442491   9.411397   9.767637  10.4746475]\n",
      "Reset environment\n",
      "Episode reward: 3694.2219734191895\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.482448 10.472635 10.445308  9.41448   9.770203 10.477499]\n",
      "Reset environment\n",
      "Episode reward: 5888.856769859791\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.487266 10.477435 10.450132  9.419634  9.77454  10.482319]\n",
      "Reset environment\n",
      "Episode reward: 1549.1559383869171\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.488342  10.478502  10.45122    9.42083    9.775504  10.4833975]\n",
      "Reset environment\n",
      "Episode reward: 4276.380551427603\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.491731 10.481877 10.454618  9.424482  9.778588 10.486781]\n",
      "Reset environment\n",
      "Episode reward: 999.611636698246\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.49226  10.482407 10.455145  9.425137  9.779035 10.48731 ]\n",
      "Reset environment\n",
      "Episode reward: 1730.341232061386\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.49345  10.483598 10.45633   9.426455  9.780082 10.488501]\n",
      "Reset environment\n",
      "Episode reward: 4452.9217448830605\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.496805 10.486949 10.459695  9.430105  9.783072 10.49186 ]\n",
      "Reset environment\n",
      "Episode reward: -450.5502389073372\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.4950905 10.485056  10.458147   9.428132   9.781546  10.490149 ]\n",
      "Reset environment\n",
      "Episode reward: 3094.99793279171\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.497447 10.48743  10.460489  9.430686  9.78365  10.492508]\n",
      "Reset environment\n",
      "Episode reward: 1521.7800866924226\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.498335  10.488387  10.461302   9.431572   9.784487  10.4934025]\n",
      "Reset environment\n",
      "Episode reward: 2866.1640659570694\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.500538 10.490581 10.463509  9.433949  9.78646  10.495605]\n",
      "Reset environment\n",
      "Episode reward: 2255.666171848774\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.502161 10.49221  10.465129  9.435728  9.787915 10.497231]\n",
      "Reset environment\n",
      "Episode reward: 2703.2646113038063\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.504056 10.494103 10.467018  9.437799  9.789594 10.499123]\n",
      "Reset environment\n",
      "Episode reward: 2931.123006477952\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.506178 10.496201 10.469171  9.440122  9.791484 10.501245]\n",
      "Reset environment\n",
      "Episode reward: 1846.1406027674675\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.50745  10.49748  10.470436  9.441528  9.792623 10.502517]\n",
      "Reset environment\n",
      "Episode reward: 2013.3709876909852\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.508805 10.498804 10.47183   9.443031  9.793829 10.50387 ]\n",
      "Reset environment\n",
      "Episode reward: 3332.277043759823\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.511335 10.501356 10.474334  9.445777  9.79611  10.506401]\n",
      "Reset environment\n",
      "Episode reward: 1981.846304833889\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.513318 10.503337 10.476322  9.447945  9.797877 10.508386]\n",
      "Reset environment\n",
      "Episode reward: 2407.46240401268\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.515036 10.505068 10.478028  9.449825  9.799406 10.510102]\n",
      "Reset environment\n",
      "Episode reward: 1384.1779467463493\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.516524  10.506555  10.479513   9.4514475  9.800728  10.511591 ]\n",
      "Reset environment\n",
      "Episode reward: 47.88546550273895\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.515166 10.505098 10.478273  9.450126  9.799462 10.510247]\n",
      "Reset environment\n",
      "Episode reward: 5379.844412624836\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.519559 10.509494 10.482662  9.454805  9.803482 10.51464 ]\n",
      "Reset environment\n",
      "Episode reward: 3743.2780522704124\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.522446 10.512374 10.485555  9.457922  9.806067 10.517525]\n",
      "Reset environment\n",
      "Episode reward: 1034.6348087787628\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.52288  10.512857 10.485935  9.458292  9.806478 10.517966]\n",
      "Reset environment\n",
      "Episode reward: 3800.550920575857\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.525782 10.515797 10.488798  9.46144   9.809101 10.52087 ]\n",
      "Reset environment\n",
      "Episode reward: 4529.17327028513\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.529387 10.519391 10.492413  9.465302  9.812346 10.524472]\n",
      "Reset environment\n",
      "Episode reward: 1989.8530176877975\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.530739  10.520729  10.4937725  9.466814   9.813545  10.525823 ]\n",
      "Reset environment\n",
      "Episode reward: 2785.871637225151\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.532807 10.522797 10.495836  9.469082  9.81539  10.527892]\n",
      "Reset environment\n",
      "Episode reward: 3226.5099911242723\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.535543 10.525564 10.498544  9.472146  9.817906 10.530635]\n",
      "Reset environment\n",
      "Episode reward: 4275.231283187866\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.538905 10.52891  10.501904  9.475761  9.820923 10.533995]\n",
      "Reset environment\n",
      "Episode reward: 1539.1487360596657\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.539945 10.529936 10.502956  9.47692   9.821853 10.535036]\n",
      "Reset environment\n",
      "Episode reward: 2753.1210464835167\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.54202  10.532    10.505036  9.479165  9.823708 10.537113]\n",
      "Reset environment\n",
      "Episode reward: 2057.6467756032944\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.544068 10.534041 10.507087  9.4814    9.825524 10.539164]\n",
      "Reset environment\n",
      "Episode reward: 3753.8423919677734\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.546855 10.536822 10.509865  9.484412  9.828013 10.54195 ]\n",
      "Reset environment\n",
      "Episode reward: 1374.274153649807\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.548299 10.538265 10.511305  9.486015  9.829286 10.543391]\n",
      "Reset environment\n",
      "Episode reward: 2568.7327547594905\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.549884 10.539917 10.512842  9.487971  9.830783 10.544984]\n",
      "Reset environment\n",
      "Episode reward: 3482.3055030703545\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.552502 10.54255  10.515439  9.490817  9.833106 10.547602]\n",
      "Reset environment\n",
      "Episode reward: 1874.363481476903\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.553489 10.543601 10.516382  9.491971  9.833955 10.548593]\n",
      "Reset environment\n",
      "Episode reward: 4119.181927919388\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.556708 10.546813 10.519608  9.495423  9.836852 10.551813]\n",
      "Reset environment\n",
      "Episode reward: 5771.442387163639\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.561178 10.551291 10.524077  9.500221  9.840907 10.556285]\n",
      "Reset environment\n",
      "Episode reward: 1629.300283908844\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.562321 10.552438 10.52522   9.501478  9.841937 10.557427]\n",
      "Reset environment\n",
      "Episode reward: 2150.139691531658\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.563823 10.553911 10.526743  9.503104  9.843277 10.558924]\n",
      "Reset environment\n",
      "Episode reward: 4291.860928237438\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.5672245 10.5573225 10.530137   9.506742   9.846366  10.56233  ]\n",
      "Reset environment\n",
      "Episode reward: 5310.044923484325\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.5721035 10.562189  10.535024   9.511957   9.850742  10.567208 ]\n",
      "Reset environment\n",
      "Episode reward: 1744.4321370720863\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.573841  10.563934  10.5367565  9.513879   9.852291  10.568948 ]\n",
      "Reset environment\n",
      "Episode reward: 3963.3500041365623\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.576902 10.567019 10.539787  9.517174  9.855061 10.572009]\n",
      "Reset environment\n",
      "Episode reward: 2186.3420796990395\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.578886 10.568993 10.541779  9.519341  9.856827 10.573993]\n",
      "Reset environment\n",
      "Episode reward: 2115.847270190716\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.580359 10.57045  10.543273  9.520956  9.858148 10.575471]\n",
      "Reset environment\n",
      "Episode reward: -682.9585297107697\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.578556 10.568668 10.541457  9.519042  9.856483 10.573672]\n",
      "Reset environment\n",
      "Episode reward: 2089.6776582598686\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.580597  10.570714  10.543492   9.52128    9.8583145 10.575714 ]\n",
      "Reset environment\n",
      "Episode reward: 2168.5407755970955\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.582103 10.572205 10.545018  9.522936  9.859664 10.577218]\n",
      "Reset environment\n",
      "Episode reward: 1697.4700816869736\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.583213 10.573347 10.546091  9.524173  9.860658 10.578327]\n",
      "Reset environment\n",
      "Episode reward: 3897.833358824253\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.586205 10.576311 10.549111  9.527393  9.863357 10.58132 ]\n",
      "Reset environment\n",
      "Episode reward: 2530.2279372811317\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.588031 10.57814  10.550933  9.529388  9.864994 10.583142]\n",
      "Reset environment\n",
      "Episode reward: 4774.429216265678\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.591764 10.581852 10.55469   9.533408  9.868361 10.586875]\n",
      "Reset environment\n",
      "Episode reward: 3749.117383182049\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.594635 10.584719 10.557568  9.536509  9.870941 10.589743]\n",
      "Reset environment\n",
      "Episode reward: 3050.843039881438\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.596869 10.587011 10.559755  9.53898   9.872966 10.591984]\n",
      "Reset environment\n",
      "Episode reward: 2762.972406208515\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.598944 10.58907  10.561842  9.54123   9.874841 10.594057]\n",
      "Reset environment\n",
      "Episode reward: 4849.038781225681\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.602752 10.59286  10.565665  9.545328  9.878273 10.597863]\n",
      "Reset environment\n",
      "Episode reward: 2572.5457932651043\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.604423 10.594418 10.567456  9.547151  9.879732 10.59954 ]\n",
      "Reset environment\n",
      "Episode reward: 1653.605899631977\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.605553  10.595537  10.568592   9.548397   9.8807335 10.60067  ]\n",
      "Reset environment\n",
      "Episode reward: 1950.3418167829514\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.607456  10.597436  10.570502   9.550488   9.8824215 10.602572 ]\n",
      "Reset environment\n",
      "Episode reward: 2706.7077397704124\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.609484  10.599443  10.57255    9.552687   9.8842535 10.604602 ]\n",
      "Reset environment\n",
      "Episode reward: 4505.6554445028305\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.612823  10.602775  10.5758915  9.556295   9.887238  10.607944 ]\n",
      "Reset environment\n",
      "Episode reward: 2033.7980846464634\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.614227 10.604197 10.577271  9.557843  9.888482 10.60935 ]\n",
      "Reset environment\n",
      "Episode reward: 1934.4274827241898\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.615526 10.605479 10.578592  9.559285  9.889638 10.610654]\n",
      "Reset environment\n",
      "Episode reward: 1605.2456395626068\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.616426  10.606452  10.579423   9.560325   9.8904295 10.611557 ]\n",
      "Reset environment\n",
      "Episode reward: 3435.465038895607\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.619017 10.609031 10.582026  9.563133  9.892758 10.614146]\n",
      "Reset environment\n",
      "Episode reward: 1689.5272421836853\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.620149 10.610157 10.583166  9.564388  9.893764 10.615276]\n",
      "Reset environment\n",
      "Episode reward: 3973.317173331976\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.623238 10.613271 10.586228  9.567714  9.89655  10.618363]\n",
      "Reset environment\n",
      "Episode reward: -686.1248596906662\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.621459  10.611502  10.584439   9.565772   9.89492   10.6165905]\n",
      "Reset environment\n",
      "Episode reward: 3936.361002802849\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.624454 10.614483 10.587445  9.569013  9.897601 10.619585]\n",
      "Reset environment\n",
      "Episode reward: 2212.8247428536415\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.625989 10.616003 10.588992  9.570698  9.898975 10.62112 ]\n",
      "Reset environment\n",
      "Episode reward: 2788.109190955758\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.628244 10.618308 10.591199  9.573283  9.901082 10.623385]\n",
      "Reset environment\n",
      "Episode reward: 2822.7525824308395\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.630334  10.620389  10.593296   9.5755415  9.90295   10.625473 ]\n",
      "Reset environment\n",
      "Episode reward: 1465.0156124830246\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.631899 10.62195  10.59486   9.577258  9.904347 10.627038]\n",
      "Reset environment\n",
      "Episode reward: 886.3557571172714\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.631978 10.622102 10.594876  9.577214  9.904446 10.627126]\n",
      "Reset environment\n",
      "Episode reward: 5098.629945337772\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.635986 10.626079 10.598898  9.581533  9.908048 10.631134]\n",
      "Reset environment\n",
      "Episode reward: 4877.773384451866\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.639824 10.629899 10.602749  9.58566   9.9115   10.634972]\n",
      "Reset environment\n",
      "Episode reward: 1520.5008236169815\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.640833  10.630914  10.60375    9.586787   9.9124155 10.635982 ]\n",
      "Reset environment\n",
      "Episode reward: 5194.305752277374\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.644925 10.635019 10.607837  9.591188  9.916162 10.640081]\n",
      "Reset environment\n",
      "Episode reward: 3429.318206489086\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.647497 10.637606 10.610394  9.593973  9.918447 10.642654]\n",
      "Reset environment\n",
      "Episode reward: 1879.7057758271694\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.64827   10.638443  10.611127   9.594961   9.9191885 10.643443 ]\n",
      "Reset environment\n",
      "Episode reward: 3409.221119582653\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.650814 10.641012 10.613648  9.597733  9.921478 10.645987]\n",
      "Reset environment\n",
      "Episode reward: 3058.368493536487\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.6529455 10.643205  10.615717   9.600113   9.923407  10.64812  ]\n",
      "Reset environment\n",
      "Episode reward: 1945.9555929899216\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.654313  10.644573  10.617082   9.6016245  9.924637  10.649489 ]\n",
      "Reset environment\n",
      "Episode reward: 3747.846429824829\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.657041 10.647299 10.619813  9.604571  9.927078 10.65222 ]\n",
      "Reset environment\n",
      "Episode reward: 2993.04725676775\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.659294  10.649546  10.622072   9.607018   9.929119  10.6544695]\n",
      "Reset environment\n",
      "Episode reward: 1508.3503444194794\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.6603   10.650536 10.623096  9.608119  9.930023 10.655476]\n",
      "Reset environment\n",
      "Episode reward: 6609.342426396906\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.642822 10.633037 10.60574   9.584471  9.914415 10.638039]\n",
      "Reset environment\n",
      "Episode reward: 3029.505094587803\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.644994 10.635193 10.607936  9.586853  9.916363 10.640214]\n",
      "Reset environment\n",
      "Episode reward: 2575.020645789802\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.646827 10.636993 10.609807  9.588838  9.918007 10.642046]\n",
      "Reset environment\n",
      "Episode reward: 3886.433970719576\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.649761  10.639963  10.612715   9.592031   9.9206705 10.644985 ]\n",
      "Reset environment\n",
      "Episode reward: 2089.056020040065\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.651185 10.641362 10.614165  9.593583  9.921945 10.64641 ]\n",
      "Reset environment\n",
      "Episode reward: 2695.108354270458\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.6531315 10.643295  10.616122   9.595712   9.923686  10.6483555]\n",
      "Reset environment\n",
      "Episode reward: 1386.2272552251816\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.654588  10.64475   10.617575   9.597301   9.9249735 10.649811 ]\n",
      "Reset environment\n",
      "Episode reward: 1104.5365287065506\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.654677 10.644922 10.617577  9.597277  9.925036 10.649902]\n",
      "Reset environment\n",
      "Episode reward: -180.256374001503\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.653721 10.643947 10.616643  9.59613   9.924204 10.648953]\n",
      "Reset environment\n",
      "Episode reward: 6008.355001628399\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.6585455 10.648768  10.621466   9.601266   9.928601  10.653775 ]\n",
      "Reset environment\n",
      "Episode reward: 1843.8393877744675\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.6603565 10.650577  10.623282   9.603255   9.93021   10.655589 ]\n",
      "Reset environment\n",
      "Episode reward: 2564.9105305969715\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.662273 10.652541 10.625144  9.605333  9.931993 10.657517]\n",
      "Reset environment\n",
      "Episode reward: 5156.11388027668\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.666385 10.65666  10.629248  9.609714  9.935731 10.66163 ]\n",
      "Reset environment\n",
      "Episode reward: 4622.816300690174\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.669981 10.660249 10.632859  9.613567  9.93897  10.665229]\n",
      "Reset environment\n",
      "Episode reward: -417.79882245510817\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.668245 10.658342 10.631302  9.611726  9.937413 10.663513]\n",
      "Reset environment\n",
      "Episode reward: -300.10614079236984\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.666655 10.656599 10.629873  9.610042  9.935964 10.661934]\n",
      "Reset environment\n",
      "Episode reward: 4492.253446102142\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.670144 10.660097 10.633355  9.613783  9.939151 10.665423]\n",
      "Reset environment\n",
      "Episode reward: 1772.5460504889488\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.671909 10.661859 10.635119  9.615717  9.940716 10.667186]\n",
      "Reset environment\n",
      "Episode reward: 1448.3778083920479\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.673389 10.663336 10.636602  9.617338  9.942029 10.668667]\n",
      "Reset environment\n",
      "Episode reward: 3412.1020850390196\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.675789 10.6657   10.639031  9.619965  9.944148 10.671061]\n",
      "Reset environment\n",
      "Episode reward: 4304.970049083233\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.67904  10.668922 10.642313  9.623497  9.94708  10.674313]\n",
      "Reset environment\n",
      "Episode reward: 5169.131480038166\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.683719 10.673585 10.646996  9.628506  9.951285 10.678987]\n",
      "Reset environment\n",
      "Episode reward: 1838.258682757616\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.684914 10.674763 10.648209  9.629829  9.95235  10.680183]\n",
      "Reset environment\n",
      "Episode reward: 4549.390454411507\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.688426  10.678265  10.651737   9.633606   9.9555435 10.6837   ]\n",
      "Reset environment\n",
      "Episode reward: 6048.787075698376\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.69326   10.683106  10.656561   9.6387615  9.959948  10.688536 ]\n",
      "Reset environment\n",
      "Episode reward: 3499.8076649308205\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.695876 10.68571  10.65919   9.641584  9.962309 10.691152]\n",
      "Reset environment\n",
      "Episode reward: -130.63061186671257\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.694524 10.684217 10.65799   9.640211  9.961023 10.68982 ]\n",
      "Reset environment\n",
      "Episode reward: 2206.6195760965347\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.696658  10.686355  10.660119   9.642522   9.962946  10.6919565]\n",
      "Reset environment\n",
      "Episode reward: 1478.2415319383144\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.69708  10.686826 10.660499  9.642981  9.963331 10.692382]\n",
      "Reset environment\n",
      "Episode reward: 2228.591813504696\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.698623  10.688381  10.662032   9.64467    9.9647045 10.693927 ]\n",
      "Reset environment\n",
      "Episode reward: 1624.0568778514862\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.699689 10.689458 10.663083  9.645843  9.965662 10.694993]\n",
      "Reset environment\n",
      "Episode reward: -197.96031627058983\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.698054 10.687941 10.661344  9.644141  9.964121 10.693374]\n",
      "Reset environment\n",
      "Episode reward: 2527.3745858818293\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.69955  10.689493 10.662802  9.645986  9.965478 10.694877]\n",
      "Reset environment\n",
      "Episode reward: 1381.7962146997452\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.700989 10.69093  10.664242  9.647566  9.966751 10.696316]\n",
      "Reset environment\n",
      "Episode reward: 1452.0337390899658\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.7019205 10.69186   10.665181   9.648622   9.967582  10.697248 ]\n",
      "Reset environment\n",
      "Episode reward: 4884.448074758053\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.705695  10.6956215 10.66896    9.652688   9.970975  10.701026 ]\n",
      "Reset environment\n",
      "Episode reward: 2225.6337817907333\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.707251 10.697173 10.67052   9.654384  9.972362 10.702581]\n",
      "Reset environment\n",
      "Episode reward: 2617.801002472639\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.709157 10.699108 10.672386  9.65647   9.974082 10.704488]\n",
      "Reset environment\n",
      "Episode reward: 4161.218205034733\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.712373  10.702326  10.675588   9.659915   9.976973  10.7077055]\n",
      "Reset environment\n",
      "Episode reward: 3120.6055323928595\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.714594 10.7045   10.677845  9.66233   9.978964 10.709922]\n",
      "Reset environment\n",
      "Episode reward: 2210.831852912903\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.716734 10.706635 10.679989  9.66464   9.980874 10.712063]\n",
      "Reset environment\n",
      "Episode reward: -368.0942165851593\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.715339 10.705368 10.67848   9.663381  9.979369 10.71068 ]\n",
      "Reset environment\n",
      "Episode reward: 4207.891566991806\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.718522  10.708565  10.681651   9.666828   9.9822235 10.713865 ]\n",
      "Reset environment\n",
      "Episode reward: 1314.810427904129\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.719856 10.709895 10.682984  9.668312  9.983395 10.7152  ]\n",
      "Reset environment\n",
      "Episode reward: 3210.508486330509\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.722161  10.712202  10.685283   9.6708145  9.985454  10.7175045]\n",
      "Reset environment\n",
      "Episode reward: 5646.650338292122\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.7266445 10.716699  10.689757   9.675595   9.989548  10.72199  ]\n",
      "Reset environment\n",
      "Episode reward: 3422.183050572872\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.729058 10.719115 10.69217   9.678224  9.991689 10.724408]\n",
      "Reset environment\n",
      "Episode reward: 2849.9546922296286\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.731229 10.721339 10.694286  9.680612  9.993706 10.726592]\n",
      "Reset environment\n",
      "Episode reward: -682.742205619812\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.729429 10.719527 10.692532  9.678852  9.992072 10.724808]\n",
      "Reset environment\n",
      "Episode reward: -268.2976192831993\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.728253 10.718341 10.691372  9.677744  9.990919 10.723635]\n",
      "Reset environment\n",
      "Episode reward: 740.9565665721893\n",
      "Total Steps: 25\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.728577 10.718667 10.691686  9.678158  9.991184 10.723956]\n",
      "Reset environment\n",
      "Episode reward: 2826.7350116968155\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.730525 10.720611 10.693631  9.680284  9.992912 10.725903]\n",
      "Reset environment\n",
      "Episode reward: 3185.615649729967\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.733048  10.723167  10.696123   9.6830225  9.995235  10.728431 ]\n",
      "Reset environment\n",
      "Episode reward: 4701.052531182766\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.736492 10.726603 10.699566  9.686737  9.998317 10.73187 ]\n",
      "Reset environment\n",
      "Episode reward: 3661.751520395279\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.739223 10.729342 10.702268  9.689675 10.000763 10.734594]\n",
      "Reset environment\n",
      "Episode reward: -358.9054452031851\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.737909 10.728015 10.700952  9.68834   9.99948  10.733286]\n",
      "Reset environment\n",
      "Episode reward: 1619.6083948016167\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.738906  10.7289715 10.701982   9.689442  10.000377  10.734277 ]\n",
      "Reset environment\n",
      "Episode reward: 3077.723295211792\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.741155 10.731227 10.704227  9.691888 10.002408 10.736528]\n",
      "Reset environment\n",
      "Episode reward: 1601.2093903422356\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.742193  10.73225   10.705269   9.69304   10.0033245 10.737565 ]\n",
      "Reset environment\n",
      "Episode reward: 1846.681308209896\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.743422 10.733486 10.706491  9.694389 10.004421 10.738795]\n",
      "Reset environment\n",
      "Episode reward: 4104.457987993956\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.746523  10.736603  10.709552   9.6977215 10.0072    10.741892 ]\n",
      "Reset environment\n",
      "Episode reward: 1656.355801820755\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.747529 10.737564 10.710614  9.698833 10.00814  10.742896]\n",
      "Reset environment\n",
      "Episode reward: 2444.7854120135307\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.749255 10.739298 10.712335  9.700716 10.009696 10.744621]\n",
      "Reset environment\n",
      "Episode reward: 1195.2829778194427\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.750005  10.7400465 10.71309    9.701559  10.0103655 10.74537  ]\n",
      "Reset environment\n",
      "Episode reward: 1103.349536895752\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.750641 10.740705 10.713702  9.702282 10.010932 10.746004]\n",
      "Reset environment\n",
      "Episode reward: 2763.699140191078\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.752704 10.742772 10.715753  9.704509 10.012797 10.748064]\n",
      "Reset environment\n",
      "Episode reward: 2355.9924280643463\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.754344 10.744399 10.717407  9.706304 10.014257 10.749705]\n",
      "Reset environment\n",
      "Episode reward: -205.4217345714569\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.752888 10.742819 10.716099  9.704766 10.012998 10.74827 ]\n",
      "Reset environment\n",
      "Episode reward: 178.51184916496277\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.752126 10.742115 10.715264  9.703782 10.012371 10.747513]\n",
      "Reset environment\n",
      "Episode reward: 1386.1494683027267\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.753557 10.743549 10.716694  9.705345 10.013638 10.748944]\n",
      "Reset environment\n",
      "Episode reward: 1784.2304054498672\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.754771 10.744759 10.717914  9.706669 10.014718 10.750159]\n",
      "Reset environment\n",
      "Episode reward: 2656.3577279746532\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.756633 10.746601 10.719792  9.708629 10.016343 10.752033]\n",
      "Reset environment\n",
      "Episode reward: 2506.3100437670946\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.758116  10.748138  10.721256   9.710422  10.0177145 10.753535 ]\n",
      "Reset environment\n",
      "Episode reward: 1873.6741660237312\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.759394 10.749414 10.722536  9.711837 10.018851 10.754813]\n",
      "Reset environment\n",
      "Episode reward: 1940.9017644524574\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.76073  10.750751 10.723875  9.7133   10.020032 10.756148]\n",
      "Reset environment\n",
      "Episode reward: 1969.081588447094\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.761772 10.751826 10.724893  9.71458  10.021026 10.7572  ]\n",
      "Reset environment\n",
      "Episode reward: 3095.301150262356\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.764027 10.75409  10.727135  9.717007 10.023035 10.759456]\n",
      "Reset environment\n",
      "Episode reward: 6038.803498983383\n",
      "Total Steps: 206\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.76879  10.75882  10.731892  9.722078 10.027297 10.764213]\n",
      "Reset environment\n",
      "Episode reward: 4368.336631774902\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.772133 10.76216  10.735233  9.725641 10.030283 10.767553]\n",
      "Reset environment\n",
      "Episode reward: 2781.430225685239\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.774083 10.764156 10.737135  9.727793 10.032045 10.769514]\n",
      "Reset environment\n",
      "Episode reward: 4539.17409992218\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.777618 10.767704 10.74066   9.731572 10.035259 10.773047]\n",
      "Reset environment\n",
      "Episode reward: 2368.985669605434\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.779224 10.769355 10.742221  9.733347 10.036723 10.774656]\n",
      "Reset environment\n",
      "Episode reward: 1376.784798026085\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.780615 10.770743 10.743616  9.734881 10.037937 10.776046]\n",
      "Reset environment\n",
      "Episode reward: 2177.642427921295\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.782698  10.772828  10.7456875  9.737141  10.039812  10.778127 ]\n",
      "Reset environment\n",
      "Episode reward: 4020.504512667656\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.785731 10.77585  10.748722  9.740387 10.042539 10.781156]\n",
      "Reset environment\n",
      "Episode reward: 4787.004681169987\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.789449 10.77957  10.752429  9.744352 10.045909 10.784866]\n",
      "Reset environment\n",
      "Episode reward: 3371.1455124616623\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.791839 10.781961 10.754818  9.746916 10.048022 10.787254]\n",
      "Reset environment\n",
      "Episode reward: -322.61426118016243\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.790528 10.780608 10.753536  9.745583 10.046848 10.785935]\n",
      "Reset environment\n",
      "Episode reward: 2043.5388605594635\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.791813 10.781944 10.754772  9.747013 10.047989 10.787223]\n",
      "Reset environment\n",
      "Episode reward: 2874.992483496666\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.7709675 10.761012  10.733912   9.7144575 10.028741  10.766323 ]\n",
      "Reset environment\n",
      "Episode reward: 1373.205030798912\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.771791  10.7618265 10.734756   9.715385  10.029472  10.767148 ]\n",
      "Reset environment\n",
      "Episode reward: 4496.770455360413\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.775227  10.7652445 10.738203   9.71906   10.032556  10.770579 ]\n",
      "Reset environment\n",
      "Episode reward: 1350.2366055846214\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.776593 10.766618 10.739565  9.720577 10.033758 10.771949]\n",
      "Reset environment\n",
      "Episode reward: -236.07041233964264\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.775284 10.765422 10.738158  9.719332 10.0325   10.77065 ]\n",
      "Reset environment\n",
      "Episode reward: 2046.6438397169113\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.776685 10.766807 10.739574  9.720873 10.033741 10.772053]\n",
      "Reset environment\n",
      "Episode reward: 318.5188503265381\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.775892 10.765889 10.738922  9.720034 10.033047 10.771277]\n",
      "Reset environment\n",
      "Episode reward: 1923.280415289104\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.776678 10.76672  10.73968   9.720945 10.033784 10.772076]\n",
      "Reset environment\n",
      "Episode reward: 2110.902398765087\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.778119 10.768179 10.741092  9.722532 10.035067 10.773515]\n",
      "Reset environment\n",
      "Episode reward: 5359.766196966171\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.782278 10.77235  10.74524   9.726967 10.038843 10.77766 ]\n",
      "Reset environment\n",
      "Episode reward: 3216.4511946439743\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.784645 10.774722 10.747599  9.729505 10.040964 10.780026]\n",
      "Reset environment\n",
      "Episode reward: 4652.313525319099\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.788053 10.778128 10.751007  9.73314  10.044007 10.783432]\n",
      "Reset environment\n",
      "Episode reward: -338.9001359939575\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.786679  10.776716  10.7497015  9.731636  10.042835  10.782072 ]\n",
      "Reset environment\n",
      "Episode reward: 2938.0783682614565\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.788756 10.778858 10.751743  9.733931 10.044727 10.784148]\n",
      "Reset environment\n",
      "Episode reward: 3723.7371997237206\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.79134   10.781394  10.7543745  9.7367525 10.047021  10.786736 ]\n",
      "Reset environment\n",
      "Episode reward: 5256.893065333366\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.795412  10.785461  10.75843    9.741118  10.0506735 10.790802 ]\n",
      "Reset environment\n",
      "Episode reward: 4774.191355109215\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.799073 10.789111 10.762107  9.745049 10.053971 10.794464]\n",
      "Reset environment\n",
      "Episode reward: 3253.2456588745117\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.801694 10.791761 10.764689  9.747968 10.056404 10.797092]\n",
      "Reset environment\n",
      "Episode reward: 3295.4423391222954\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.804114 10.794167 10.767126  9.75057  10.058578 10.799517]\n",
      "Reset environment\n",
      "Episode reward: 1658.9174087047577\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.805228 10.795266 10.768261  9.751796 10.059579 10.800632]\n",
      "Reset environment\n",
      "Episode reward: 4577.272614479065\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.808559 10.798587 10.771587  9.755344 10.062553 10.803968]\n",
      "Reset environment\n",
      "Episode reward: 3856.301624417305\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.811434 10.801446 10.774467  9.75844  10.065125 10.806841]\n",
      "Reset environment\n",
      "Episode reward: 1766.243469119072\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.812584  10.802589  10.775621   9.7597065 10.066143  10.807991 ]\n",
      "Reset environment\n",
      "Episode reward: 3443.8528460264206\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.815103 10.805095 10.778148  9.762415 10.068413 10.810507]\n",
      "Reset environment\n",
      "Episode reward: 5246.264001727104\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.819136 10.809118 10.782171  9.766734 10.072037 10.814536]\n",
      "Reset environment\n",
      "Episode reward: 2873.4836934804916\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.821143 10.811098 10.784197  9.768891 10.073871 10.81655 ]\n",
      "Reset environment\n",
      "Episode reward: 1360.03715634346\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.822504 10.812467 10.785562  9.770394 10.075075 10.817915]\n",
      "Reset environment\n",
      "Episode reward: 4297.021552681923\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.825774 10.815735 10.78883   9.773904 10.078045 10.821183]\n",
      "Reset environment\n",
      "Episode reward: 2550.4094932079315\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.827564 10.817531 10.790609  9.775843 10.079646 10.822972]\n",
      "Reset environment\n",
      "Episode reward: 5307.541595458984\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.831648 10.821619 10.794688  9.780204 10.083377 10.827056]\n",
      "Reset environment\n",
      "Episode reward: 1477.4366369247437\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.832282  10.8221855 10.795396   9.780978  10.083947  10.82769  ]\n",
      "Reset environment\n",
      "Episode reward: 3049.6754410266876\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.834452 10.824363 10.797553  9.783343 10.085894 10.829861]\n",
      "Reset environment\n",
      "Episode reward: 5803.643800854683\n",
      "Total Steps: 219\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.839516  10.829411  10.802628   9.788772  10.090488  10.8349285]\n",
      "Reset environment\n",
      "Episode reward: 2122.6048996448517\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.840869 10.830717 10.804034  9.790255 10.091741 10.836281]\n",
      "Reset environment\n",
      "Episode reward: 2212.4049084186554\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.842968 10.832808 10.806139  9.792505 10.093615 10.83838 ]\n",
      "Reset environment\n",
      "Episode reward: 2059.6418808698654\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.844932 10.834764 10.80811   9.794637 10.095364 10.840347]\n",
      "Reset environment\n",
      "Episode reward: 1725.9595594406128\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.84599  10.835788 10.809208  9.795808 10.096328 10.841406]\n",
      "Reset environment\n",
      "Episode reward: 2850.2582329809666\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.848077 10.837918 10.811247  9.7981   10.098237 10.843498]\n",
      "Reset environment\n",
      "Episode reward: 1772.119263291359\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.849774 10.839614 10.812944  9.799967 10.099733 10.845197]\n",
      "Reset environment\n",
      "Episode reward: 2107.5374815016985\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.850947 10.840855 10.814054  9.801308 10.100763 10.846368]\n",
      "Reset environment\n",
      "Episode reward: 3025.8561547398567\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.852974 10.842926 10.816005  9.803537 10.102506 10.848395]\n",
      "Reset environment\n",
      "Episode reward: 3126.8007197529078\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.855346  10.845331  10.8183155  9.806176  10.104708  10.850771 ]\n",
      "Reset environment\n",
      "Episode reward: 3889.6241839528084\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.8582115 10.848177  10.8212     9.809274  10.1073065 10.853641 ]\n",
      "Reset environment\n",
      "Episode reward: -217.32295989990234\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.856726 10.846844 10.819592  9.807714 10.105988 10.852165]\n",
      "Reset environment\n",
      "Episode reward: 1516.241383254528\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.857688 10.847802 10.820555  9.808788 10.106839 10.853129]\n",
      "Reset environment\n",
      "Episode reward: 4117.636140525341\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.860802  10.8509035 10.823688   9.812123  10.109653  10.856249 ]\n",
      "Reset environment\n",
      "Episode reward: 2646.5268030166626\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.862705 10.852831 10.825577  9.814196 10.111386 10.858164]\n",
      "Reset environment\n",
      "Episode reward: 5857.309161067009\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.867263 10.857372 10.830139  9.819056 10.115495 10.862723]\n",
      "Reset environment\n",
      "Episode reward: 2204.078976005316\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.868749 10.858843 10.831649  9.820677 10.116833 10.864209]\n",
      "Reset environment\n",
      "Episode reward: 3865.8280521929264\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.871594 10.86165  10.834519  9.823729 10.119371 10.867059]\n",
      "Reset environment\n",
      "Episode reward: 1717.580008983612\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.872715 10.862771 10.835646  9.824969 10.120369 10.868179]\n",
      "Reset environment\n",
      "Episode reward: 1409.1863458156586\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.873572 10.863607 10.836524  9.825931 10.121131 10.86904 ]\n",
      "Reset environment\n",
      "Episode reward: 2628.457473754883\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.8754225 10.865446  10.838382   9.827936  10.122781  10.870889 ]\n",
      "Reset environment\n",
      "Episode reward: 3653.589443027973\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.87803  10.868027 10.841012  9.830778 10.125128 10.873494]\n",
      "Reset environment\n",
      "Episode reward: 2206.1692687422037\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.8794565 10.869425  10.842468   9.832345  10.126406  10.874917 ]\n",
      "Reset environment\n",
      "Episode reward: 240.1838310956955\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.878368 10.868459 10.841275  9.831116 10.125481 10.873838]\n",
      "Reset environment\n",
      "Episode reward: 2615.9796475172043\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.880209  10.870304  10.843104   9.833109  10.1271105 10.875683 ]\n",
      "Reset environment\n",
      "Episode reward: 66.32153868675232\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.879364 10.869548 10.842178  9.832085 10.126354 10.874843]\n",
      "Reset environment\n",
      "Episode reward: 535.7506341934204\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.879216 10.869434 10.841983  9.831855 10.126262 10.874696]\n",
      "Reset environment\n",
      "Episode reward: 2047.0194259881973\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.881143 10.87137  10.843909  9.833951 10.127977 10.876627]\n",
      "Reset environment\n",
      "Episode reward: 2411.1575839966536\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.882387  10.872667  10.845152   9.835466  10.1291685 10.877888 ]\n",
      "Reset environment\n",
      "Episode reward: 4808.999393105507\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.88599  10.876246 10.848761  9.839338 10.132419 10.881485]\n",
      "Reset environment\n",
      "Episode reward: 1931.0845136642456\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.887197 10.877492 10.849933  9.840689 10.133505 10.882691]\n",
      "Reset environment\n",
      "Episode reward: 2981.0426172614098\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.889349 10.87964  10.852083  9.843032 10.135441 10.884843]\n",
      "Reset environment\n",
      "Episode reward: 1948.2362115979195\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.89062  10.8809   10.85337   9.844431 10.136562 10.886111]\n",
      "Reset environment\n",
      "Episode reward: 3636.727659165859\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.893356 10.88362  10.856107  9.847374 10.139022 10.888845]\n",
      "Reset environment\n",
      "Episode reward: 2168.305374801159\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.895379  10.8856535 10.858124   9.849571  10.140838  10.890869 ]\n",
      "Reset environment\n",
      "Episode reward: 4766.183265686035\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.899001 10.88927  10.861741  9.853447 10.144125 10.894483]\n",
      "Reset environment\n",
      "Episode reward: 5123.063147723675\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.902888 10.893148 10.865641  9.857612 10.147644 10.898379]\n",
      "Reset environment\n",
      "Episode reward: 4023.4982823729515\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.906098 10.896365 10.868839  9.861054 10.150598 10.901588]\n",
      "Reset environment\n",
      "Episode reward: 2519.2388145774603\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.90746  10.897772 10.870163  9.862733 10.151882 10.902969]\n",
      "Reset environment\n",
      "Episode reward: 3151.839017868042\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.90972  10.900021 10.872442  9.865177 10.15391  10.905233]\n",
      "Reset environment\n",
      "Episode reward: 4133.447470128536\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.912632 10.902924 10.875355  9.86833  10.156502 10.908146]\n",
      "Reset environment\n",
      "Episode reward: 2932.958742380142\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.914752 10.905047 10.877468  9.870617 10.158415 10.91027 ]\n",
      "Reset environment\n",
      "Episode reward: 1392.268953204155\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.916143 10.90644  10.87886   9.872151 10.159641 10.911662]\n",
      "Reset environment\n",
      "Episode reward: 4465.9486810564995\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.919498 10.909803 10.882171  9.875733 10.162646 10.915007]\n",
      "Reset environment\n",
      "Episode reward: 1845.1435194015503\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.920714 10.911004 10.883405  9.877067 10.163734 10.916224]\n",
      "Reset environment\n",
      "Episode reward: 4923.443705350161\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.9244175 10.914707  10.88708    9.8810425 10.167095  10.919924 ]\n",
      "Reset environment\n",
      "Episode reward: 3786.2847477793694\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.9270735 10.917346  10.889737   9.8839    10.16944   10.922576 ]\n",
      "Reset environment\n",
      "Episode reward: 694.7705297470093\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.926569 10.916757 10.889342  9.883346 10.169011 10.92209 ]\n",
      "Reset environment\n",
      "Episode reward: 3289.8270644545555\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.928889  10.9190445 10.891697   9.885847  10.171089  10.924411 ]\n",
      "Reset environment\n",
      "Episode reward: 2190.7754348814487\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.930303 10.920476 10.893076  9.887418 10.172345 10.925826]\n",
      "Reset environment\n",
      "Episode reward: -680.9001884460449\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.928521  10.918709  10.891282   9.885554  10.170698  10.9240465]\n",
      "Reset environment\n",
      "Episode reward: 624.3471219539642\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.928326 10.918583 10.891017  9.885185 10.170558 10.923855]\n",
      "Reset environment\n",
      "Episode reward: 1936.4780103564262\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.929895 10.92015  10.89257   9.886906 10.171993 10.925417]\n",
      "Reset environment\n",
      "Episode reward: 453.8425295352936\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.929579 10.919869 10.892224  9.886405 10.171768 10.925104]\n",
      "Reset environment\n",
      "Episode reward: -174.1106344461441\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.928468 10.918822 10.891065  9.885156 10.170799 10.923994]\n",
      "Reset environment\n",
      "Episode reward: 2211.2363469302654\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.929635 10.920046 10.892209  9.886507 10.171922 10.925182]\n",
      "Reset environment\n",
      "Episode reward: 4399.587444603443\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.932873 10.923263 10.895465  9.889994 10.174857 10.928416]\n",
      "Reset environment\n",
      "Episode reward: 1484.2429847717285\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.933783 10.924149 10.896392  9.890998 10.175663 10.929326]\n",
      "Reset environment\n",
      "Episode reward: 5186.294824361801\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.937671  10.9280205 10.900291   9.895181  10.179166  10.933217 ]\n",
      "Reset environment\n",
      "Episode reward: 4111.0391781926155\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.940701 10.931041 10.903325  9.898451 10.181915 10.936252]\n",
      "Reset environment\n",
      "Episode reward: 4832.387913942337\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.944294 10.934615 10.90693   9.902326 10.18515  10.939846]\n",
      "Reset environment\n",
      "Episode reward: 4463.905138254166\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.947481 10.937805 10.910112  9.905753 10.187989 10.943037]\n",
      "Reset environment\n",
      "Episode reward: 3597.0638358592987\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.950078 10.940384 10.912717  9.908546 10.190338 10.945631]\n",
      "Reset environment\n",
      "Episode reward: 1322.8187410086393\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.950624  10.940878  10.913314   9.909008  10.1909075 10.9461775]\n",
      "Reset environment\n",
      "Episode reward: 5115.523823916912\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.955047  10.9452915 10.917698   9.913732  10.19493   10.950588 ]\n",
      "Reset environment\n",
      "Episode reward: -685.5554311275482\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.953268  10.9435215 10.915903   9.9118    10.193282  10.948816 ]\n",
      "Reset environment\n",
      "Episode reward: 3454.9052611887455\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.955766 10.94601  10.918417  9.914496 10.195542 10.95132 ]\n",
      "Reset environment\n",
      "Episode reward: 3666.779071033001\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.958442 10.948694 10.921072  9.917349 10.19791  10.953987]\n",
      "Reset environment\n",
      "Episode reward: 3349.0338965654373\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.960832 10.951084 10.923458  9.919949 10.200077 10.95638 ]\n",
      "Reset environment\n",
      "Episode reward: 3299.502898976207\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.963326  10.953609  10.925895   9.922642  10.2023735 10.958874 ]\n",
      "Reset environment\n",
      "Episode reward: 2725.292693734169\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.965242 10.955511 10.927814  9.924722 10.204095 10.960787]\n",
      "Reset environment\n",
      "Episode reward: 2136.02705681324\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.967224  10.957497  10.9297905  9.926884  10.205868  10.962768 ]\n",
      "Reset environment\n",
      "Episode reward: 3000.184748649597\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.969312 10.959592 10.931872  9.929164 10.207748 10.964857]\n",
      "Reset environment\n",
      "Episode reward: 1833.43979331851\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.970449 10.960706 10.933027  9.930407 10.208751 10.965993]\n",
      "Reset environment\n",
      "Episode reward: 1811.0342633128166\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.97176  10.962029 10.934301  9.931796 10.209987 10.967304]\n",
      "Reset environment\n",
      "Episode reward: 1705.641095161438\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.973384 10.963645 10.935929  9.933584 10.21142  10.968927]\n",
      "Reset environment\n",
      "Episode reward: 1415.449902832508\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.974792 10.965051 10.937339  9.935119 10.212654 10.970334]\n",
      "Reset environment\n",
      "Episode reward: 1777.4202649071813\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.975747  10.966058  10.938238   9.936215  10.213494  10.9712925]\n",
      "Reset environment\n",
      "Episode reward: 4416.859731733799\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.979079 10.969382 10.941562  9.939762 10.216512 10.974624]\n",
      "Reset environment\n",
      "Episode reward: 2365.300027549267\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.980689 10.970991 10.943166  9.941512 10.217947 10.976233]\n",
      "Reset environment\n",
      "Episode reward: 2171.5086240172386\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.982061  10.972368  10.944534   9.9430065 10.219169  10.977607 ]\n",
      "Reset environment\n",
      "Episode reward: 2074.053255677223\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.983978 10.974283 10.946455  9.945095 10.220872 10.979525]\n",
      "Reset environment\n",
      "Episode reward: 5643.3061255812645\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.988287 10.978562 10.950759  9.949675 10.224729 10.983825]\n",
      "Reset environment\n",
      "Episode reward: 3334.885211825371\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.990652 10.980928 10.953127  9.952239 10.226854 10.986196]\n",
      "Reset environment\n",
      "Episode reward: 3271.557672858238\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.99299  10.983267 10.955456  9.954746 10.228949 10.988534]\n",
      "Reset environment\n",
      "Episode reward: 1944.6832874715328\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.9940605 10.984377  10.9564905  9.955912  10.229971  10.989607 ]\n",
      "Reset environment\n",
      "Episode reward: 2299.587407410145\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.99559   10.985893  10.9580345  9.957579  10.231335  10.9911375]\n",
      "Reset environment\n",
      "Episode reward: 236.02660834789276\n",
      "Total Steps: 22\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.995347 10.985656 10.957788  9.957351 10.231109 10.990894]\n",
      "Reset environment\n",
      "Episode reward: 1772.0662857610732\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.99619  10.986442 10.95869   9.958339 10.231827 10.991739]\n",
      "Reset environment\n",
      "Episode reward: 3371.2087811529636\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.998402 10.988606 10.960952  9.960777 10.233777 10.993955]\n",
      "Reset environment\n",
      "Episode reward: -686.7233854532242\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.9966345 10.98685   10.959184   9.958834  10.232152  10.992193 ]\n",
      "Reset environment\n",
      "Episode reward: 4442.657600045204\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [10.999908  10.990115  10.9624605  9.962347  10.2351055 10.995464 ]\n",
      "Reset environment\n",
      "Episode reward: 3840.7668828891474\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.002673 10.992846 10.965258  9.965316 10.237606 10.998236]\n",
      "Reset environment\n",
      "Episode reward: 2036.1791256070137\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.004549 10.994728 10.967128  9.967361 10.239276 11.000116]\n",
      "Reset environment\n",
      "Episode reward: 2276.2503956258297\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.00604  10.996239 10.968613  9.969016 10.240594 11.001612]\n",
      "Reset environment\n",
      "Episode reward: 1478.0004982948303\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.006981 10.997186 10.969551  9.970068 10.241437 11.002553]\n",
      "Reset environment\n",
      "Episode reward: 1960.0942380428314\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.008807 10.999011 10.97138   9.972059 10.243061 11.004383]\n",
      "Reset environment\n",
      "Episode reward: 2068.3897743821144\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.010169  11.000382  10.972737   9.973554  10.2442875 11.005747 ]\n",
      "Reset environment\n",
      "Episode reward: 3273.7194988429546\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.012563 11.002804 10.975099  9.976176 10.246481 11.008155]\n",
      "Reset environment\n",
      "Episode reward: 4439.08878493309\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.015895 11.006142 10.978418  9.979745 10.249509 11.011482]\n",
      "Reset environment\n",
      "Episode reward: 2780.387577831745\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0178385 11.008078  10.980369   9.981834  10.251232  11.013424 ]\n",
      "Reset environment\n",
      "Episode reward: 1731.5580543875694\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.019487 11.009727 10.98202   9.98364  10.252693 11.015073]\n",
      "Reset environment\n",
      "Episode reward: 1410.095945239067\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.02088  11.011116 10.983414  9.985166 10.253923 11.016461]\n",
      "Reset environment\n",
      "Episode reward: 929.5870399475098\n",
      "Total Steps: 29\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.02137  11.011607 10.9839    9.985734 10.254351 11.016952]\n",
      "Reset environment\n",
      "Episode reward: 1100.9791719913483\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.021907 11.012138 10.984432  9.986259 10.254876 11.017491]\n",
      "Reset environment\n",
      "Episode reward: 1614.0051607489586\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.023449  11.013675  10.9859705  9.987943  10.256237  11.019032 ]\n",
      "Reset environment\n",
      "Episode reward: 3628.6810511648655\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.026031 11.016272 10.988537  9.990744 10.258555 11.02162 ]\n",
      "Reset environment\n",
      "Episode reward: 1518.7586540281773\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.026897  11.01718   10.98934    9.9916315 10.259359  11.022484 ]\n",
      "Reset environment\n",
      "Episode reward: 4690.764045894146\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.030367 11.020647 10.992819  9.995355 10.262474 11.025961]\n",
      "Reset environment\n",
      "Episode reward: -259.9402229487896\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0289755 11.019373  10.9913225  9.993876  10.261237  11.024579 ]\n",
      "Reset environment\n",
      "Episode reward: 1647.2106393575668\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.030013  11.020406  10.992362   9.9950285 10.2621565 11.025614 ]\n",
      "Reset environment\n",
      "Episode reward: 3077.6390360593796\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.032168 11.022566 10.994511  9.997367 10.264105 11.02777 ]\n",
      "Reset environment\n",
      "Episode reward: 3635.9549543857574\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.03469  11.025089 10.997032 10.000096 10.266348 11.030294]\n",
      "Reset environment\n",
      "Episode reward: 2759.7208120822906\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.036582  11.026993  10.998904  10.0021515 10.268039  11.032189 ]\n",
      "Reset environment\n",
      "Episode reward: 2245.708666741848\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.038057 11.028486 11.000362 10.003764 10.269337 11.033666]\n",
      "Reset environment\n",
      "Episode reward: 3653.826834857464\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.040611  11.031038  11.0029335 10.006549  10.271648  11.036216 ]\n",
      "Reset environment\n",
      "Episode reward: 2759.6090101599693\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.042512  11.032943  11.004834  10.008616  10.273363  11.0381155]\n",
      "Reset environment\n",
      "Episode reward: 3476.0596287697554\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.044979 11.035456 11.007234 10.011302 10.275597 11.040581]\n",
      "Reset environment\n",
      "Episode reward: 257.2379231452942\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.044298 11.034854 11.006468 10.010456 10.275009 11.039901]\n",
      "Reset environment\n",
      "Episode reward: 2904.367513656616\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0463295 11.036892  11.008488  10.012655  10.276844  11.041929 ]\n",
      "Reset environment\n",
      "Episode reward: 4254.97245618701\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.049388 11.039954 11.011513 10.015955 10.279565 11.044988]\n",
      "Reset environment\n",
      "Episode reward: 3295.3604031205177\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.051691 11.042263 11.01379  10.01845  10.28161  11.047284]\n",
      "Reset environment\n",
      "Episode reward: -693.0119047164917\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.049918 11.040463 11.012069 10.016722 10.279998 11.045535]\n",
      "Reset environment\n",
      "Episode reward: 4963.376355171204\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0536175 11.044167  11.01576   10.020671  10.28336   11.049232 ]\n",
      "Reset environment\n",
      "Episode reward: 5717.198128223419\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.05791  11.048453 11.020037 10.025234 10.28725  11.053517]\n",
      "Reset environment\n",
      "Episode reward: 3423.9159271121025\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.060314 11.050843 11.022454 10.027837 10.289405 11.055926]\n",
      "Reset environment\n",
      "Episode reward: 2194.261263847351\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.06176  11.052297 11.023895 10.029423 10.290707 11.057371]\n",
      "Reset environment\n",
      "Episode reward: 3200.615335975541\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.063833 11.054343 11.02601  10.031707 10.292579 11.059441]\n",
      "Reset environment\n",
      "Episode reward: 2542.0632427334785\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.065548 11.056047 11.027727 10.033587 10.294106 11.061155]\n",
      "Reset environment\n",
      "Episode reward: 1349.06644064188\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.066851 11.057353 11.029028 10.035041 10.295249 11.062459]\n",
      "Reset environment\n",
      "Episode reward: 3822.0980872511864\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.069591 11.060062 11.031784 10.037988 10.297704 11.065196]\n",
      "Reset environment\n",
      "Episode reward: 1838.0535330176353\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.071068 11.061536 11.033263 10.039601 10.299056 11.066683]\n",
      "Reset environment\n",
      "Episode reward: 1939.8563501238823\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.072314 11.06277  11.034513 10.040971 10.300153 11.067928]\n",
      "Reset environment\n",
      "Episode reward: 3564.090796291828\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.074853 11.065309 11.037048 10.043719 10.302453 11.070468]\n",
      "Reset environment\n",
      "Episode reward: 1861.081241428852\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.076588 11.067045 11.038784 10.045614 10.304003 11.072204]\n",
      "Reset environment\n",
      "Episode reward: 2291.9899041950703\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.077922 11.068433 11.040097 10.047161 10.305266 11.073553]\n",
      "Reset environment\n",
      "Episode reward: 2642.5412422418594\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0803   11.070809 11.042483 10.049721 10.307383 11.075935]\n",
      "Reset environment\n",
      "Episode reward: -0.6446672677993774\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.078987 11.069362 11.041304 10.0484   10.306191 11.074626]\n",
      "Reset environment\n",
      "Episode reward: 3397.8715738505125\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.081166  11.0714855 11.043543  10.050811  10.308127  11.076806 ]\n",
      "Reset environment\n",
      "Episode reward: 1907.9964183568954\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.082798 11.073115 11.045176 10.052583 10.309558 11.078438]\n",
      "Reset environment\n",
      "Episode reward: 1681.010860837996\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.083576 11.073835 11.046009 10.0535   10.310236 11.079218]\n",
      "Reset environment\n",
      "Episode reward: 2069.708682477474\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.085464 11.075728 11.0479   10.055557 10.311924 11.081111]\n",
      "Reset environment\n",
      "Episode reward: 5366.972111821175\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.089425 11.079664 11.051882 10.05981  10.315495 11.085079]\n",
      "Reset environment\n",
      "Episode reward: 3965.216232895851\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.092263 11.082475 11.05474  10.062867 10.318051 11.087921]\n",
      "Reset environment\n",
      "Episode reward: 2832.0795729756355\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.09423   11.08443   11.0567045 10.064995  10.319812  11.08989  ]\n",
      "Reset environment\n",
      "Episode reward: 1434.1831008195877\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.0951185 11.085316  11.057592  10.06598   10.320604  11.090778 ]\n",
      "Reset environment\n",
      "Episode reward: 2020.9632224589586\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.096377 11.086623 11.058802 10.067292 10.321773 11.092041]\n",
      "Reset environment\n",
      "Episode reward: 2097.1773381233215\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.098301 11.088542 11.060735 10.069376 10.323491 11.093965]\n",
      "Reset environment\n",
      "Episode reward: 2211.1600859984756\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.099754  11.090034  11.0621395 10.070972  10.324816  11.095416 ]\n",
      "Reset environment\n",
      "Episode reward: 4142.75888133049\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.10268  11.092956 11.065075 10.074112 10.327441 11.098341]\n",
      "Reset environment\n",
      "Episode reward: 3192.4882539212704\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.105116  11.095413  11.067461  10.0767145 10.329655  11.100773 ]\n",
      "Reset environment\n",
      "Episode reward: 4019.5544466376305\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.108006 11.098279 11.070356 10.079802 10.332247 11.103655]\n",
      "Reset environment\n",
      "Episode reward: 1978.9197162687778\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.109033 11.099343 11.071356 10.080955 10.333227 11.104691]\n",
      "Reset environment\n",
      "Episode reward: 3101.5304842591286\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.111043 11.101323 11.073406 10.083159 10.335057 11.106705]\n",
      "Reset environment\n",
      "Episode reward: 819.0789704322815\n",
      "Total Steps: 25\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.111438  11.101721  11.073803  10.083626  10.335403  11.1071005]\n",
      "Reset environment\n",
      "Episode reward: 3344.1163598299026\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.113816  11.1041155 11.076162  10.086184  10.337545  11.109478 ]\n",
      "Reset environment\n",
      "Episode reward: 1376.1830224990845\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.114578 11.104838 11.076958 10.08704  10.338218 11.110243]\n",
      "Reset environment\n",
      "Episode reward: 2177.8747692108154\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.116551 11.10681  11.078934 10.089187 10.339996 11.112216]\n",
      "Reset environment\n",
      "Episode reward: 1744.3006601929665\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.118191 11.108447 11.080572 10.090964 10.341448 11.113853]\n",
      "Reset environment\n",
      "Episode reward: 2452.115242358297\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.119761 11.109986 11.082191 10.092677 10.342851 11.11543 ]\n",
      "Reset environment\n",
      "Episode reward: 2265.18623316288\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.1212435 11.111449  11.08369   10.094288  10.344173  11.116911 ]\n",
      "Reset environment\n",
      "Episode reward: 1392.1491482257843\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.122089 11.112277 11.084549 10.095219 10.344928 11.117755]\n",
      "Reset environment\n",
      "Episode reward: 1911.3638409934938\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.123302 11.113526 11.085713 10.096488 10.346067 11.118973]\n",
      "Reset environment\n",
      "Episode reward: 2050.3098192214966\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.125181 11.115398 11.087597 10.098528 10.347736 11.120851]\n",
      "Reset environment\n",
      "Episode reward: 2136.018245637417\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.126551 11.116761 11.088977 10.100028 10.348947 11.122217]\n",
      "Reset environment\n",
      "Episode reward: 4366.776567935944\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.129711  11.1199045 11.092141  10.10342   10.351784  11.125381 ]\n",
      "Reset environment\n",
      "Episode reward: 2022.7112714648247\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.131559 11.121746 11.093978 10.105418 10.353415 11.127226]\n",
      "Reset environment\n",
      "Episode reward: 1462.3466206789017\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.132385 11.122601 11.094776 10.106346 10.354156 11.128055]\n",
      "Reset environment\n",
      "Episode reward: 5015.885609149933\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.13593   11.126135  11.098318  10.110107  10.357287  11.1315975]\n",
      "Reset environment\n",
      "Episode reward: 3723.6778725385666\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.138577 11.128769 11.100984 10.112957 10.359663 11.134246]\n",
      "Reset environment\n",
      "Episode reward: 2522.4791240394115\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.139966 11.130197 11.102362 10.114611 10.360954 11.135649]\n",
      "Reset environment\n",
      "Episode reward: 2846.5610436201096\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.141945 11.132161 11.104341 10.116739 10.362718 11.137623]\n",
      "Reset environment\n",
      "Episode reward: 2160.4001200199127\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.1439   11.134118 11.106304 10.118862 10.364465 11.139581]\n",
      "Reset environment\n",
      "Episode reward: 4132.306578934193\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.146854 11.137057 11.109277 10.122034 10.367128 11.142539]\n",
      "Reset environment\n",
      "Episode reward: 2283.943818449974\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.148344 11.138535 11.110781 10.123656 10.36846  11.144034]\n",
      "Reset environment\n",
      "Episode reward: 2233.359274983406\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.14973  11.139908 11.112187 10.12518  10.36967  11.145421]\n",
      "Reset environment\n",
      "Episode reward: 2820.81266066432\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.15154  11.141781 11.113944 10.127175 10.371312 11.147238]\n",
      "Reset environment\n",
      "Episode reward: 4314.913576871157\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.154656  11.144903  11.117029  10.1305065 10.3741    11.150356 ]\n",
      "Reset environment\n",
      "Episode reward: 2202.260085463524\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.156648 11.146889 11.119025 10.132658 10.37588  11.152348]\n",
      "Reset environment\n",
      "Episode reward: 1988.0689100027084\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.157953 11.148194 11.120329 10.134096 10.37705  11.153652]\n",
      "Reset environment\n",
      "Episode reward: 2157.7300884127617\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.159916 11.150153 11.122297 10.136216 10.378818 11.155615]\n",
      "Reset environment\n",
      "Episode reward: 940.7170796394348\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.1599   11.15021  11.122205 10.136075 10.378817 11.155605]\n",
      "Reset environment\n",
      "Episode reward: 2317.4392727017403\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.161979 11.152294 11.124272 10.138332 10.38069  11.157682]\n",
      "Reset environment\n",
      "Episode reward: 2050.4654328748584\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.163235 11.153519 11.125566 10.139711 10.381804 11.158937]\n",
      "Reset environment\n",
      "Episode reward: 2591.204309388995\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.164769 11.15507  11.127105 10.141379 10.383196 11.160482]\n",
      "Reset environment\n",
      "Episode reward: 1808.873636007309\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.166443 11.156741 11.128778 10.143213 10.384667 11.162154]\n",
      "Reset environment\n",
      "Episode reward: 2044.6285738945007\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.167711  11.157974  11.130089  10.144591  10.385844  11.1634245]\n",
      "Reset environment\n",
      "Episode reward: 3267.5927108824253\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.169875  11.160186  11.132162  10.146971  10.3877735 11.165594 ]\n",
      "Reset environment\n",
      "Episode reward: 2253.934837937355\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.171332 11.161625 11.13365  10.148559 10.389078 11.167055]\n",
      "Reset environment\n",
      "Episode reward: 2580.029257237911\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.173058 11.163337 11.135389 10.150443 10.390654 11.168782]\n",
      "Reset environment\n",
      "Episode reward: 2943.527217268944\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.175068 11.165337 11.137405 10.152623 10.392441 11.170786]\n",
      "Reset environment\n",
      "Episode reward: 2726.715782523155\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.176885  11.167156  11.1392145 10.154596  10.394057  11.172603 ]\n",
      "Reset environment\n",
      "Episode reward: 1834.8922983314842\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.177978  11.168279  11.1402645 10.155818  10.395053  11.173697 ]\n",
      "Reset environment\n",
      "Episode reward: 3120.7676932811737\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.180141 11.170446 11.142413 10.158153 10.396993 11.175859]\n",
      "Reset environment\n",
      "Episode reward: 2859.044211268425\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.182072 11.172371 11.144357 10.160267 10.398723 11.17779 ]\n",
      "Reset environment\n",
      "Episode reward: 5207.185968518257\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.185721 11.176031 11.148004 10.164197 10.401963 11.181445]\n",
      "Reset environment\n",
      "Episode reward: 2094.0628501176834\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.187123  11.177431  11.149416  10.1657295 10.403236  11.182848 ]\n",
      "Reset environment\n",
      "Episode reward: 3042.4068192243576\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.189223 11.179524 11.151516 10.168008 10.405144 11.184948]\n",
      "Reset environment\n",
      "Episode reward: 112.34115296602249\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.18856   11.178841  11.150864  10.167265  10.4045725 11.184286 ]\n",
      "Reset environment\n",
      "Episode reward: 4091.674486398697\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.1915   11.181755 11.153832 10.170426 10.407248 11.187231]\n",
      "Reset environment\n",
      "Episode reward: 2716.19109749794\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.193313 11.183545 11.155662 10.172389 10.408892 11.189044]\n",
      "Reset environment\n",
      "Episode reward: 3181.743679298088\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.195427 11.185718 11.157725 10.174723 10.410823 11.191163]\n",
      "Reset environment\n",
      "Episode reward: 4404.911591947079\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.198598 11.188886 11.1609   10.178147 10.413667 11.194336]\n",
      "Reset environment\n",
      "Episode reward: 5462.940825104713\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.202655  11.1929455 11.164955  10.182474  10.417344  11.198399 ]\n",
      "Reset environment\n",
      "Episode reward: 4530.863777399063\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.205915 11.196194 11.168214 10.185963 10.420278 11.201657]\n",
      "Reset environment\n",
      "Episode reward: 2140.724882185459\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.207819 11.198102 11.170108 10.188036 10.421993 11.203554]\n",
      "Reset environment\n",
      "Episode reward: 2152.400583922863\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.209741 11.200015 11.172038 10.190115 10.423688 11.205476]\n",
      "Reset environment\n",
      "Episode reward: 2727.0664902180433\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.211478 11.201708 11.173813 10.191999 10.425234 11.207212]\n",
      "Reset environment\n",
      "Episode reward: 1634.5955265164375\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.212485  11.202716  11.174816  10.1931305 10.4261265 11.208221 ]\n",
      "Reset environment\n",
      "Episode reward: 4794.951882660389\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.215818 11.206035 11.17816  10.196679 10.429085 11.211559]\n",
      "Reset environment\n",
      "Episode reward: 315.1879245042801\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.215301 11.205551 11.177621 10.195921 10.428685 11.211045]\n",
      "Reset environment\n",
      "Episode reward: 1150.4985333681107\n",
      "Total Steps: 39\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.215912 11.206155 11.178246 10.196626 10.429231 11.211657]\n",
      "Reset environment\n",
      "Episode reward: 5163.60083848238\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.219538 11.209784 11.181866 10.200508 10.432455 11.215282]\n",
      "Reset environment\n",
      "Episode reward: 3808.547759084031\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.222144  11.2124605 11.184418  10.203364  10.434854  11.217896 ]\n",
      "Reset environment\n",
      "Episode reward: 2556.1833414137363\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.223849 11.21415  11.18614  10.205202 10.436375 11.219607]\n",
      "Reset environment\n",
      "Episode reward: 188.45662260055542\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.222938  11.213351  11.185118  10.204116  10.4355545 11.218701 ]\n",
      "Reset environment\n",
      "Episode reward: 1594.4613461494446\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.22376  11.214118 11.185995 10.205046 10.436305 11.219523]\n",
      "Reset environment\n",
      "Episode reward: 3911.14250344038\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.226493  11.21682   11.188748  10.2079935 10.438771  11.222255 ]\n",
      "Reset environment\n",
      "Episode reward: 2484.0440240576863\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.228152 11.218517 11.190383 10.209833 10.440328 11.22392 ]\n",
      "Reset environment\n",
      "Episode reward: 1987.5300383716822\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.229111  11.219525  11.191294  10.210901  10.4412365 11.224888 ]\n",
      "Reset environment\n",
      "Episode reward: 2707.0661033391953\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.23097  11.221395 11.193122 10.212916 10.442907 11.226742]\n",
      "Reset environment\n",
      "Episode reward: 4320.8547385931015\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.234068 11.224495 11.196224 10.216237 10.445711 11.229849]\n",
      "Reset environment\n",
      "Episode reward: 702.45640873909\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.23405  11.224513 11.196169 10.216061 10.445734 11.229831]\n",
      "Reset environment\n",
      "Episode reward: 1382.2717002630234\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.235378 11.225844 11.197496 10.217525 10.446891 11.231159]\n",
      "Reset environment\n",
      "Episode reward: 4694.964735329151\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.238751 11.22921  11.200881 10.221144 10.449899 11.234535]\n",
      "Reset environment\n",
      "Episode reward: 1623.3335061073303\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.240249 11.230705 11.202385 10.222805 10.451228 11.236035]\n",
      "Reset environment\n",
      "Episode reward: 1897.5835475325584\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.241944 11.232399 11.204088 10.224664 10.452721 11.237738]\n",
      "Reset environment\n",
      "Episode reward: 5098.496619522572\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.246207 11.236637 11.20836  10.229213 10.456545 11.242001]\n",
      "Reset environment\n",
      "Episode reward: 2792.4839539527893\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.248082  11.2385025 11.210245  10.23124   10.458229  11.243879 ]\n",
      "Reset environment\n",
      "Episode reward: 3582.5997028946877\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.25056   11.240982  11.212714  10.233928  10.4604645 11.246354 ]\n",
      "Reset environment\n",
      "Episode reward: 2277.4611917994916\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.251967 11.242353 11.214149 10.23546  10.461711 11.247761]\n",
      "Reset environment\n",
      "Episode reward: 2825.794443845749\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.25373  11.24408  11.215949 10.237403 10.463321 11.249527]\n",
      "Reset environment\n",
      "Episode reward: 458.8140618801117\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.252866  11.243344  11.214966  10.236448  10.462564  11.2486725]\n",
      "Reset environment\n",
      "Episode reward: 4709.16858112812\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.256253 11.246719 11.218362 10.240061 10.465582 11.252061]\n",
      "Reset environment\n",
      "Episode reward: 3187.42291867733\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.258426 11.248899 11.220522 10.24241  10.467527 11.254235]\n",
      "Reset environment\n",
      "Episode reward: 3385.532562404871\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.260727 11.251227 11.222792 10.244895 10.469604 11.256543]\n",
      "Reset environment\n",
      "Episode reward: -689.1876522302628\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.258975  11.249488  11.221015  10.2429905 10.467995  11.254786 ]\n",
      "Reset environment\n",
      "Episode reward: 1910.9113038778305\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.260166 11.250685 11.222202 10.244306 10.469068 11.255977]\n",
      "Reset environment\n",
      "Episode reward: 2402.07388818264\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.26172   11.25223   11.223767  10.2459955 10.470433  11.257529 ]\n",
      "Reset environment\n",
      "Episode reward: 3242.2584217190742\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.263805 11.254316 11.225851 10.248268 10.472279 11.259614]\n",
      "Reset environment\n",
      "Episode reward: 3284.1797727942467\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.266014 11.256552 11.228038 10.250676 10.474262 11.261823]\n",
      "Reset environment\n",
      "Episode reward: 2144.708960711956\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.267923 11.258456 11.229952 10.252744 10.475965 11.263733]\n",
      "Reset environment\n",
      "Episode reward: 1906.0693132132292\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.268931 11.259402 11.231017 10.253889 10.476846 11.264742]\n",
      "Reset environment\n",
      "Episode reward: 2924.9746081233025\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.270929  11.261453  11.232959  10.2560835 10.478658  11.266745 ]\n",
      "Reset environment\n",
      "Episode reward: 1217.4223146438599\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.271435  11.2619915 11.233403  10.256542  10.47915   11.267253 ]\n",
      "Reset environment\n",
      "Episode reward: 5600.636449873447\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.275501 11.266057 11.237463 10.260876 10.48284  11.271319]\n",
      "Reset environment\n",
      "Episode reward: -683.5340155363083\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.273751 11.264315 11.235709 10.258945 10.481232 11.26958 ]\n",
      "Reset environment\n",
      "Episode reward: 1861.5441329479218\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.275426 11.265987 11.237387 10.260787 10.482708 11.271255]\n",
      "Reset environment\n",
      "Episode reward: 3203.7354167848825\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.27753  11.268144 11.239426 10.263111 10.484579 11.273364]\n",
      "Reset environment\n",
      "Episode reward: 2809.1789488196373\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.279435 11.270042 11.241336 10.265185 10.486255 11.275269]\n",
      "Reset environment\n",
      "Episode reward: 2036.6499918699265\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.280716 11.271346 11.242595 10.266592 10.487406 11.27655 ]\n",
      "Reset environment\n",
      "Episode reward: 1455.0345153808594\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.2816105 11.2722225 11.243497  10.267575  10.4881935 11.277444 ]\n",
      "Reset environment\n",
      "Episode reward: 2366.839577972889\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.283137 11.273734 11.245023 10.269264 10.489538 11.278969]\n",
      "Reset environment\n",
      "Episode reward: 2047.3634331822395\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.28442  11.275017 11.246317 10.27068  10.490688 11.280253]\n",
      "Reset environment\n",
      "Episode reward: 2265.2286445498466\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.285826 11.276425 11.247723 10.272225 10.491925 11.281661]\n",
      "Reset environment\n",
      "Episode reward: 2194.6792500019073\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.287755 11.278364 11.24965  10.27433  10.493683 11.283593]\n",
      "Reset environment\n",
      "Episode reward: 3334.087959088385\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.289861 11.280531 11.251689 10.276708 10.495599 11.285705]\n",
      "Reset environment\n",
      "Episode reward: 1954.8325122594833\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.291615 11.282275 11.25345  10.278618 10.497144 11.287453]\n",
      "Reset environment\n",
      "Episode reward: 4582.118531703949\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.294851 11.28549  11.256702 10.28211  10.500033 11.290689]\n",
      "Reset environment\n",
      "Episode reward: 1800.9321222305298\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.295977 11.286618 11.257829 10.283354 10.501038 11.291814]\n",
      "Reset environment\n",
      "Episode reward: 4410.138898521662\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.29911  11.28976  11.260958 10.286698 10.503879 11.294948]\n",
      "Reset environment\n",
      "Episode reward: 695.3968234062195\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.299015 11.289703 11.260829 10.286445 10.503833 11.29486 ]\n",
      "Reset environment\n",
      "Episode reward: 3827.2334682047367\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.301685 11.292396 11.263461 10.289309 10.506205 11.297527]\n",
      "Reset environment\n",
      "Episode reward: 2635.1222618818283\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.303393 11.294111 11.265166 10.291186 10.507735 11.299234]\n",
      "Reset environment\n",
      "Episode reward: 2183.154732942581\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.305318 11.296037 11.267092 10.293292 10.509463 11.30116 ]\n",
      "Reset environment\n",
      "Episode reward: 2151.560658991337\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.306677 11.297392 11.268455 10.294787 10.510665 11.30252 ]\n",
      "Reset environment\n",
      "Episode reward: 5018.149680554867\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.310807 11.301501 11.272589 10.299209 10.514358 11.306647]\n",
      "Reset environment\n",
      "Episode reward: 2155.794495999813\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.312724 11.303412 11.274513 10.301289 10.516069 11.308562]\n",
      "Reset environment\n",
      "Episode reward: 2187.6994057893753\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.31465  11.305342 11.276434 10.303388 10.517811 11.310485]\n",
      "Reset environment\n",
      "Episode reward: 1641.3127796649933\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.315663 11.306329 11.277457 10.304507 10.518706 11.311498]\n",
      "Reset environment\n",
      "Episode reward: 2958.979570746422\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.317666 11.308333 11.279454 10.306676 10.52052  11.313493]\n",
      "Reset environment\n",
      "Episode reward: 879.9692168235779\n",
      "Total Steps: 27\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.318102 11.308773 11.279889 10.307185 10.520898 11.313929]\n",
      "Reset environment\n",
      "Episode reward: 4774.9487126767635\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.321539  11.312193  11.283345  10.310905  10.524026  11.3173685]\n",
      "Reset environment\n",
      "Episode reward: 1404.1413398385048\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.322862  11.313514  11.284676  10.3123665 10.525169  11.318691 ]\n",
      "Reset environment\n",
      "Episode reward: 4858.25548428297\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.326325 11.316966 11.288143 10.31608  10.528265 11.322157]\n",
      "Reset environment\n",
      "Episode reward: 3441.3294553756714\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.32866  11.319324 11.290465 10.318624 10.530376 11.3245  ]\n",
      "Reset environment\n",
      "Episode reward: 2468.0149250626564\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.330298  11.3209505 11.292109  10.320396  10.531837  11.32614  ]\n",
      "Reset environment\n",
      "Episode reward: 3485.4279076196253\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.332823 11.323498 11.294632 10.323217 10.534214 11.328666]\n",
      "Reset environment\n",
      "Episode reward: 2359.075871348381\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.334343 11.325003 11.296168 10.324875 10.535591 11.330184]\n",
      "Reset environment\n",
      "Episode reward: 5050.048961400986\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.337982 11.328626 11.299797 10.328768 10.538847 11.333829]\n",
      "Reset environment\n",
      "Episode reward: 1784.5304153561592\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.339032 11.329655 11.300865 10.329921 10.539772 11.334877]\n",
      "Reset environment\n",
      "Episode reward: 1640.52790671587\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.339986 11.330602 11.301825 10.330994 10.540617 11.33583 ]\n",
      "Reset environment\n",
      "Episode reward: 1931.1867396235466\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.341233 11.331856 11.303067 10.332361 10.54175  11.337077]\n",
      "Reset environment\n",
      "Episode reward: 5017.680150210857\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.34468  11.335297 11.306521 10.336062 10.544776 11.340526]\n",
      "Reset environment\n",
      "Episode reward: 4206.91687899828\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.347622 11.338236 11.309458 10.339235 10.547446 11.343473]\n",
      "Reset environment\n",
      "Episode reward: 3934.1289380192757\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.350391 11.340984 11.312238 10.342223 10.549972 11.346245]\n",
      "Reset environment\n",
      "Episode reward: 3983.6897583007812\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.353134  11.343733  11.3149605 10.3452015 10.552461  11.348986 ]\n",
      "Reset environment\n",
      "Episode reward: 1162.5995728969574\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.3532   11.343697 11.315099 10.345212 10.552522 11.349058]\n",
      "Reset environment\n",
      "Episode reward: 2649.860231101513\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.354767 11.345226 11.316699 10.346895 10.55397  11.350628]\n",
      "Reset environment\n",
      "Episode reward: 2985.01999771595\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.356764  11.347231  11.3186865 10.349056  10.555775  11.352623 ]\n",
      "Reset environment\n",
      "Episode reward: 3369.069562703371\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.359017 11.349493 11.320919 10.3515   10.557767 11.354869]\n",
      "Reset environment\n",
      "Episode reward: 2383.111342251301\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.360949 11.351424 11.322847 10.353603 10.559504 11.356794]\n",
      "Reset environment\n",
      "Episode reward: 1782.5814747214317\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.362057  11.352535  11.3239565 10.35482   10.560507  11.357903 ]\n",
      "Reset environment\n",
      "Episode reward: 3588.080094397068\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.364516 11.355001 11.326412 10.357475 10.562719 11.360365]\n",
      "Reset environment\n",
      "Episode reward: 2008.3119177818298\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.365745 11.356242 11.327633 10.358824 10.563786 11.361597]\n",
      "Reset environment\n",
      "Episode reward: 4358.8535079956055\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.3688135 11.359302  11.330709  10.362125  10.566534  11.364666 ]\n",
      "Reset environment\n",
      "Episode reward: 2359.6683531552553\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.370248 11.360701 11.332169 10.363685 10.567805 11.366093]\n",
      "Reset environment\n",
      "Episode reward: 2170.6302819624543\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.371529  11.362027  11.33341   10.365109  10.568963  11.3673725]\n",
      "Reset environment\n",
      "Episode reward: 2214.053499519825\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.373472 11.363982 11.33535  10.367221 10.570726 11.369321]\n",
      "Reset environment\n",
      "Episode reward: 2646.901741027832\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.375184  11.3656845 11.337076  10.369082  10.572263  11.371033 ]\n",
      "Reset environment\n",
      "Episode reward: 2566.8593780733645\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.376576 11.367019 11.338529 10.37058  10.573513 11.372432]\n",
      "Reset environment\n",
      "Episode reward: 1345.326706647873\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.377382 11.367833 11.33933  10.371473 10.574244 11.373238]\n",
      "Reset environment\n",
      "Episode reward: -502.0715996026993\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.37592  11.36636  11.33786  10.36958  10.572985 11.371774]\n",
      "Reset environment\n",
      "Episode reward: 3574.3696669340134\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.378349  11.368794  11.3402815 10.37221   10.575178  11.374201 ]\n",
      "Reset environment\n",
      "Episode reward: 3102.91561037302\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.380258 11.370758 11.342135 10.374313 10.576864 11.376117]\n",
      "Reset environment\n",
      "Episode reward: 3928.9622930288315\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.382956 11.373438 11.344848 10.377233 10.579312 11.378814]\n",
      "Reset environment\n",
      "Episode reward: 2460.0633793473244\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.38455  11.375046 11.346429 10.378983 10.580745 11.380405]\n",
      "Reset environment\n",
      "Episode reward: 4310.438696026802\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.387528 11.378008 11.349422 10.382181 10.583447 11.383382]\n",
      "Reset environment\n",
      "Episode reward: 2751.6891281604767\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.389337 11.379821 11.351227 10.384152 10.585082 11.385184]\n",
      "Reset environment\n",
      "Episode reward: 2106.5588003098965\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.3905945 11.381057  11.352505  10.385535  10.586183  11.386442 ]\n",
      "Reset environment\n",
      "Episode reward: 2379.1875410079956\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.392129 11.382585 11.354045 10.387208 10.587547 11.387976]\n",
      "Reset environment\n",
      "Episode reward: -807.1640628827736\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.390252  11.3807955 11.352079  10.385176  10.585813  11.3861065]\n",
      "Reset environment\n",
      "Episode reward: 4988.85484457016\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.393797 11.384325 11.355636 10.388967 10.588965 11.389647]\n",
      "Reset environment\n",
      "Episode reward: 2762.9886416196823\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.395618 11.386158 11.357454 10.390951 10.59062  11.391468]\n",
      "Reset environment\n",
      "Episode reward: 2834.827942714095\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.397439 11.388018 11.359237 10.392964 10.592266 11.393296]\n",
      "Reset environment\n",
      "Episode reward: 2123.0054862499237\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.399291 11.389863 11.361096 10.394983 10.593918 11.395148]\n",
      "Reset environment\n",
      "Episode reward: 1685.292904138565\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.400015  11.390662  11.3617325 10.395834  10.59455   11.395873 ]\n",
      "Reset environment\n",
      "Episode reward: -703.456629037857\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.39824  11.388893 11.359946 10.393886 10.592943 11.394105]\n",
      "Reset environment\n",
      "Episode reward: 1359.0065902471542\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.3995075 11.3901615 11.361216  10.395286  10.5940695 11.395373 ]\n",
      "Reset environment\n",
      "Episode reward: 1747.9842491298914\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.400495  11.391117  11.362231  10.3963785 10.594946  11.396359 ]\n",
      "Reset environment\n",
      "Episode reward: 2128.5597866624594\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.401461 11.392135 11.363129 10.3975   10.595793 11.397327]\n",
      "Reset environment\n",
      "Episode reward: 2193.2792295268737\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.4026375 11.393352  11.364251  10.398791  10.596887  11.398512 ]\n",
      "Reset environment\n",
      "Episode reward: 2188.3481072187424\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.404567 11.395269 11.366181 10.400862 10.598606 11.400435]\n",
      "Reset environment\n",
      "Episode reward: 2063.749539490789\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.405796 11.396538 11.367369 10.402217 10.599721 11.401667]\n",
      "Reset environment\n",
      "Episode reward: 3442.754038631916\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.408107 11.398851 11.369681 10.404726 10.601811 11.403979]\n",
      "Reset environment\n",
      "Episode reward: 3774.1639053225517\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.410718  11.401467  11.372283  10.407522  10.604144  11.4065895]\n",
      "Reset environment\n",
      "Episode reward: 856.200336098671\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.410083 11.400735 11.37173  10.406945 10.60349  11.405955]\n",
      "Reset environment\n",
      "Episode reward: 1829.3546351790428\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.411688 11.402341 11.373337 10.408718 10.604909 11.407559]\n",
      "Reset environment\n",
      "Episode reward: 4828.551379144192\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.415104 11.405739 11.376755 10.412376 10.607946 11.410976]\n",
      "Reset environment\n",
      "Episode reward: 2062.261849939823\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.416899 11.407542 11.378544 10.414346 10.609561 11.412773]\n",
      "Reset environment\n",
      "Episode reward: 2180.8697839975357\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.418795 11.409434 11.380438 10.416407 10.611244 11.414668]\n",
      "Reset environment\n",
      "Episode reward: 1990.586575627327\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.420545 11.411184 11.382192 10.418316 10.612793 11.416421]\n",
      "Reset environment\n",
      "Episode reward: 4088.1695004701614\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.423376 11.414027 11.385005 10.421351 10.615306 11.419251]\n",
      "Reset environment\n",
      "Episode reward: 2171.6951840519905\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.424772  11.4154215 11.386404  10.4228735 10.616552  11.420648 ]\n",
      "Reset environment\n",
      "Episode reward: 2452.523601770401\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.426372 11.417031 11.38799  10.424605 10.617984 11.422247]\n",
      "Reset environment\n",
      "Episode reward: 2210.7575591802597\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.427774 11.418422 11.389413 10.426142 10.619246 11.423649]\n",
      "Reset environment\n",
      "Episode reward: 4234.504543244839\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.430761  11.42142   11.392395  10.429343  10.6219225 11.426637 ]\n",
      "Reset environment\n",
      "Episode reward: 4612.335203170776\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.433993 11.42465  11.395631 10.432818 10.624855 11.42987 ]\n",
      "Reset environment\n",
      "Episode reward: 2218.89850962162\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.435368 11.426017 11.397008 10.434333 10.626062 11.43124 ]\n",
      "Reset environment\n",
      "Episode reward: 2155.8258869051933\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.437254  11.4279    11.398892  10.4363785 10.627763  11.433128 ]\n",
      "Reset environment\n",
      "Episode reward: 1960.2629533410072\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.438946 11.429602 11.400576 10.438239 10.629283 11.43482 ]\n",
      "Reset environment\n",
      "Episode reward: 1435.9896577596664\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.439781 11.430441 11.401406 10.439164 10.630039 11.435657]\n",
      "Reset environment\n",
      "Episode reward: 1383.5737144351006\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.441071  11.431733  11.4026985 10.440582  10.631187  11.436946 ]\n",
      "Reset environment\n",
      "Episode reward: 1742.9679635763168\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.4426365 11.433298  11.404268  10.442298  10.632559  11.438513 ]\n",
      "Reset environment\n",
      "Episode reward: 2956.114892244339\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.44461  11.435255 11.406251 10.444441 10.634322 11.440489]\n",
      "Reset environment\n",
      "Episode reward: 1878.7311770319939\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.446255 11.436903 11.407894 10.446247 10.635776 11.442133]\n",
      "Reset environment\n",
      "Episode reward: 5135.8448095321655\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.449862 11.440495 11.411488 10.450111 10.63902  11.445739]\n",
      "Reset environment\n",
      "Episode reward: -690.224774479866\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.448081  11.43872   11.409697  10.448259  10.6374035 11.443962 ]\n",
      "Reset environment\n",
      "Episode reward: 1330.9657807946205\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.449283 11.439918 11.410896 10.449606 10.638441 11.445161]\n",
      "Reset environment\n",
      "Episode reward: 2157.163089454174\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.451143 11.44179  11.412753 10.451634 10.64011  11.447024]\n",
      "Reset environment\n",
      "Episode reward: 3945.355633854866\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.453852 11.444486 11.415467 10.45455  10.642544 11.449728]\n",
      "Reset environment\n",
      "Episode reward: 2073.330529510975\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.455151 11.445775 11.416781 10.455974 10.643716 11.451027]\n",
      "Reset environment\n",
      "Episode reward: 2008.307914018631\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.456922 11.447543 11.418559 10.457898 10.645287 11.452798]\n",
      "Reset environment\n",
      "Episode reward: 3742.8416650891304\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.459474 11.450075 11.421128 10.460652 10.6476   11.455348]\n",
      "Reset environment\n",
      "Episode reward: 4426.221794337034\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.462524 11.453155 11.424155 10.46398  10.650389 11.458405]\n",
      "Reset environment\n",
      "Episode reward: 2786.338617503643\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.464289  11.454954  11.4258795 10.465906  10.65197   11.4601755]\n",
      "Reset environment\n",
      "Episode reward: 2120.2338585853577\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.466107 11.456775 11.427685 10.467905 10.653611 11.461995]\n",
      "Reset environment\n",
      "Episode reward: 3110.954960465431\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.468168 11.458888 11.429689 10.470164 10.655485 11.464069]\n",
      "Reset environment\n",
      "Episode reward: 3071.6350934505463\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.470191 11.460917 11.431706 10.472344 10.657296 11.466094]\n",
      "Reset environment\n",
      "Episode reward: 3077.3634023740888\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.471571 11.462192 11.433179 10.473974 10.658452 11.467472]\n",
      "Reset environment\n",
      "Episode reward: 2914.7408416867256\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.47344  11.464045 11.435071 10.476008 10.66012  11.469336]\n",
      "Reset environment\n",
      "Episode reward: 2155.348921954632\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.4753065 11.465917  11.436941  10.478047  10.661806  11.471206 ]\n",
      "Reset environment\n",
      "Episode reward: 4349.207621455193\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.478309  11.468921  11.4399395 10.481282  10.664505  11.474211 ]\n",
      "Reset environment\n",
      "Episode reward: 2489.6180322766304\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.479766  11.4703865 11.4413805 10.482887  10.665802  11.475669 ]\n",
      "Reset environment\n",
      "Episode reward: 3484.2651467323303\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.482116 11.472714 11.443751 10.485418 10.667882 11.478018]\n",
      "Reset environment\n",
      "Episode reward: 5975.942625463009\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.48639  11.47699  11.448018 10.489969 10.671773 11.482289]\n",
      "Reset environment\n",
      "Episode reward: 6153.663383722305\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.490791  11.481356  11.4524355 10.494698  10.6757555 11.486691 ]\n",
      "Reset environment\n",
      "Episode reward: 4188.626989364624\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.493685 11.484256 11.455322 10.497799 10.678361 11.489583]\n",
      "Reset environment\n",
      "Episode reward: 2249.963184386492\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.495089 11.485637 11.456745 10.499324 10.679617 11.490984]\n",
      "Reset environment\n",
      "Episode reward: 1805.5621398687363\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.494939 11.485427 11.456651 10.499343 10.679298 11.490839]\n",
      "Reset environment\n",
      "Episode reward: 1542.1977509856224\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.495787  11.486298  11.45747   10.5002575 10.680096  11.491688 ]\n",
      "Reset environment\n",
      "Episode reward: 2846.143235716969\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.497479  11.48804   11.45911   10.502139  10.6816025 11.4933815]\n",
      "Reset environment\n",
      "Episode reward: 961.1893600225449\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.497473 11.488098 11.459033 10.501986 10.681622 11.493384]\n",
      "Reset environment\n",
      "Episode reward: -269.3672757744789\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.496205 11.486905 11.457688 10.500668 10.680467 11.492118]\n",
      "Reset environment\n",
      "Episode reward: 5194.74774312973\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.49989  11.490596 11.461361 10.504591 10.683792 11.495803]\n",
      "Reset environment\n",
      "Episode reward: 3396.67210239172\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.502136 11.492826 11.463631 10.507032 10.685796 11.498055]\n",
      "Reset environment\n",
      "Episode reward: 1884.9675915539265\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.503191 11.493852 11.464718 10.508189 10.686736 11.49911 ]\n",
      "Reset environment\n",
      "Episode reward: 3245.8191574811935\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.505336 11.495983 11.466881 10.510514 10.688674 11.501259]\n",
      "Reset environment\n",
      "Episode reward: 5332.7703093886375\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.509098 11.499723 11.470644 10.514559 10.692048 11.505023]\n",
      "Reset environment\n",
      "Episode reward: 1414.0284751057625\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.510402 11.501028 11.471951 10.515992 10.693182 11.506326]\n",
      "Reset environment\n",
      "Episode reward: 1716.2780957221985\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.5113945 11.501996  11.472956  10.51709   10.69406   11.507317 ]\n",
      "Reset environment\n",
      "Episode reward: 1528.9964710474014\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.512264 11.502878 11.473813 10.518063 10.694832 11.508186]\n",
      "Reset environment\n",
      "Episode reward: 3049.711448520422\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.514169 11.504828 11.475619 10.520165 10.696544 11.510098]\n",
      "Reset environment\n",
      "Episode reward: 1482.4779475927353\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.5150385 11.505694  11.476495  10.5211315 10.697328  11.510969 ]\n",
      "Reset environment\n",
      "Episode reward: 1674.895188331604\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.516224 11.506886 11.477671 10.522448 10.698425 11.512156]\n",
      "Reset environment\n",
      "Episode reward: 2695.7055281102657\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.517967 11.508639 11.479388 10.524335 10.699979 11.513898]\n",
      "Reset environment\n",
      "Episode reward: 5498.556552648544\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.5218525 11.5125265 11.483247  10.528473  10.703492  11.517782 ]\n",
      "Reset environment\n",
      "Episode reward: 1602.722863316536\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.522781 11.513468 11.484164 10.529515 10.704322 11.51871 ]\n",
      "Reset environment\n",
      "Episode reward: 5519.85026204586\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.526678  11.517356  11.488051  10.5336685 10.707831  11.52261  ]\n",
      "Reset environment\n",
      "Episode reward: 1843.9415849745274\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.527765 11.51848  11.489101 10.534885 10.708813 11.523694]\n",
      "Reset environment\n",
      "Episode reward: 1247.9218287467957\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.528313 11.519048 11.489601 10.535371 10.709341 11.524244]\n",
      "Reset environment\n",
      "Episode reward: 5239.390660941601\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.532009  11.5227375 11.493282  10.539311  10.712663  11.527939 ]\n",
      "Reset environment\n",
      "Episode reward: 2683.520358800888\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.533724 11.524456 11.494994 10.541182 10.714196 11.529655]\n",
      "Reset environment\n",
      "Episode reward: 467.4348565340042\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.533289 11.524078 11.494507 10.540548 10.713863 11.529224]\n",
      "Reset environment\n",
      "Episode reward: 1673.4772295951843\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.534265 11.525051 11.495491 10.541641 10.714739 11.530201]\n",
      "Reset environment\n",
      "Episode reward: 2012.3850578665733\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.5360155 11.526797  11.497249  10.543552  10.716283  11.531959 ]\n",
      "Reset environment\n",
      "Episode reward: 4868.747743189335\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.539363 11.530137 11.500576 10.547175 10.719288 11.535298]\n",
      "Reset environment\n",
      "Episode reward: 4536.5837779045105\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.542514  11.533276  11.503715  10.5505495 10.722131  11.538444 ]\n",
      "Reset environment\n",
      "Episode reward: 4043.4411858320236\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.545288 11.536058 11.506469 10.553522 10.724606 11.541222]\n",
      "Reset environment\n",
      "Episode reward: 1991.0511998534203\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.546524 11.537266 11.507719 10.554874 10.725707 11.542459]\n",
      "Reset environment\n",
      "Episode reward: 1635.2742562294006\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.547533  11.538277  11.508725  10.5559845 10.726624  11.543467 ]\n",
      "Reset environment\n",
      "Episode reward: 1546.6092348098755\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.548596 11.539359 11.509769 10.557177 10.727589 11.544538]\n",
      "Reset environment\n",
      "Episode reward: 1320.7406290471554\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.548972 11.539803 11.51008  10.557497 10.727931 11.544919]\n",
      "Reset environment\n",
      "Episode reward: 1934.325739979744\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.55063   11.541463  11.511737  10.55932   10.729404  11.5465765]\n",
      "Reset environment\n",
      "Episode reward: 3720.8586203455925\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.552954  11.5437765 11.51409   10.561861  10.731522  11.548902 ]\n",
      "Reset environment\n",
      "Episode reward: 1413.6773303747177\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.554246 11.545071 11.515383 10.563282 10.732656 11.550194]\n",
      "Reset environment\n",
      "Episode reward: 1563.521541878581\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.555093 11.545893 11.516255 10.564231 10.733404 11.551038]\n",
      "Reset environment\n",
      "Episode reward: 2780.034431155771\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.556486 11.547314 11.517609 10.565823 10.734712 11.552439]\n",
      "Reset environment\n",
      "Episode reward: 2603.9606159329414\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.558148 11.548968 11.519274 10.567653 10.736175 11.554101]\n",
      "Reset environment\n",
      "Episode reward: 3864.2647763490677\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.560774 11.551601 11.521895 10.570477 10.73854  11.556727]\n",
      "Reset environment\n",
      "Episode reward: 297.91979598999023\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.560337  11.551157  11.5214615 10.569878  10.73818   11.556291 ]\n",
      "Reset environment\n",
      "Episode reward: 2928.2301865816116\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.56224  11.55305  11.523368 10.571939 10.739878 11.558193]\n",
      "Reset environment\n",
      "Episode reward: 2019.7201367914677\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.563456  11.554245  11.524597  10.5732765 10.740959  11.559408 ]\n",
      "Reset environment\n",
      "Episode reward: 1367.138540327549\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.564222 11.555013 11.52536  10.574134 10.741647 11.560174]\n",
      "Reset environment\n",
      "Episode reward: 3068.9809533953667\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.566222 11.557025 11.527356 10.576303 10.743449 11.562175]\n",
      "Reset environment\n",
      "Episode reward: 5114.278147339821\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.569786 11.560596 11.530915 10.580113 10.746683 11.565739]\n",
      "Reset environment\n",
      "Episode reward: 1654.0067172646523\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.570483 11.561344 11.531561 10.580935 10.747281 11.566441]\n",
      "Reset environment\n",
      "Episode reward: 1737.4571567475796\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.571473  11.562326  11.532569  10.582031  10.748161  11.5674305]\n",
      "Reset environment\n",
      "Episode reward: 3484.040534108877\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.573754  11.56464   11.534821  10.584508  10.750218  11.5697155]\n",
      "Reset environment\n",
      "Episode reward: -313.93005442619324\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.572651  11.563528  11.533737  10.583112  10.7493105 11.568622 ]\n",
      "Reset environment\n",
      "Episode reward: 3443.0799920260906\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.574924 11.565816 11.535989 10.585566 10.751337 11.570893]\n",
      "Reset environment\n",
      "Episode reward: 6196.861945986748\n",
      "Total Steps: 210\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.5792885 11.57015   11.5403805 10.59024   10.755304  11.575269 ]\n",
      "Reset environment\n",
      "Episode reward: 2062.0596812963486\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.581068 11.571927 11.542166 10.592171 10.756878 11.577047]\n",
      "Reset environment\n",
      "Episode reward: 3179.1850350797176\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.583055 11.573954 11.544087 10.594349 10.758658 11.579039]\n",
      "Reset environment\n",
      "Episode reward: 2327.319492459297\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.584483 11.575366 11.545532 10.59591  10.759932 11.58047 ]\n",
      "Reset environment\n",
      "Episode reward: 1491.8387101888657\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.585329 11.576211 11.546379 10.596853 10.760676 11.581317]\n",
      "Reset environment\n",
      "Episode reward: 4764.062026143074\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.588607 11.579488 11.549649 10.600381 10.763645 11.584597]\n",
      "Reset environment\n",
      "Episode reward: 3855.6642441153526\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.591203 11.582088 11.552238 10.603172 10.765949 11.587189]\n",
      "Reset environment\n",
      "Episode reward: -243.60768616199493\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.589658 11.580684 11.550569 10.601474 10.764559 11.585657]\n",
      "Reset environment\n",
      "Episode reward: 2296.9101904034615\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.591601 11.582622 11.552516 10.603576 10.766282 11.587603]\n",
      "Reset environment\n",
      "Episode reward: 2375.5744297504425\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.593075 11.584091 11.553992 10.605188 10.767578 11.589073]\n",
      "Reset environment\n",
      "Episode reward: 1786.0740550458431\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.59401  11.58498  11.554972 10.606232 10.768422 11.590002]\n",
      "Reset environment\n",
      "Episode reward: 5409.889087259769\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.597757 11.588736 11.558708 10.610239 10.771835 11.593751]\n",
      "Reset environment\n",
      "Episode reward: 4214.553395688534\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.600597 11.591582 11.561539 10.613295 10.774372 11.596591]\n",
      "Reset environment\n",
      "Episode reward: 1386.6225298643112\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.601861 11.592847 11.562804 10.614681 10.775501 11.597856]\n",
      "Reset environment\n",
      "Episode reward: 2477.1231617331505\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.603442 11.594412 11.564397 10.616401 10.776901 11.599433]\n",
      "Reset environment\n",
      "Episode reward: 2331.78671503067\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.604873  11.595826  11.565841  10.617965  10.778193  11.6008625]\n",
      "Reset environment\n",
      "Episode reward: 4259.920833647251\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6078205 11.5987425 11.568784  10.621119  10.7808695 11.603798 ]\n",
      "Reset environment\n",
      "Episode reward: 1733.3395413942635\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.608545  11.599523  11.569459  10.6219845 10.781489  11.604528 ]\n",
      "Reset environment\n",
      "Episode reward: 2087.8484996259212\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.609772 11.60079  11.570655 10.623368 10.782615 11.605752]\n",
      "Reset environment\n",
      "Episode reward: 4283.0296449661255\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.612695 11.603705 11.573579 10.626506 10.785232 11.608676]\n",
      "Reset environment\n",
      "Episode reward: 1623.433078110218\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.613613  11.604604  11.574512  10.627532  10.786049  11.6095915]\n",
      "Reset environment\n",
      "Episode reward: 1334.6478412151337\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.614375 11.605375 11.575265 10.628384 10.786741 11.610355]\n",
      "Reset environment\n",
      "Episode reward: 1749.9499077796936\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.615364 11.606371 11.576254 10.629488 10.787634 11.611347]\n",
      "Reset environment\n",
      "Episode reward: 3953.225070297718\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.618026 11.60903  11.578925 10.632346 10.790015 11.614013]\n",
      "Reset environment\n",
      "Episode reward: 438.9476420879364\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.617482  11.6084385 11.578423  10.631628  10.789627  11.613471 ]\n",
      "Reset environment\n",
      "Episode reward: 3623.3780487775803\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.619945 11.610874 11.580889 10.634276 10.791841 11.615933]\n",
      "Reset environment\n",
      "Episode reward: 2269.3198050260544\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.62137  11.612315 11.5823   10.635843 10.793129 11.617358]\n",
      "Reset environment\n",
      "Episode reward: 3715.021933555603\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.623829 11.614806 11.584732 10.638512 10.795369 11.619819]\n",
      "Reset environment\n",
      "Episode reward: 4855.3894047141075\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.627243 11.618213 11.588136 10.642128 10.798468 11.623229]\n",
      "Reset environment\n",
      "Episode reward: 5035.826376795769\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.630563 11.621528 11.591466 10.645692 10.8014   11.626553]\n",
      "Reset environment\n",
      "Episode reward: 1940.513524711132\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.631542 11.622459 11.592486 10.646788 10.802268 11.627524]\n",
      "Reset environment\n",
      "Episode reward: 2156.8440866470337\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.632848 11.623743 11.593814 10.648211 10.803438 11.62883 ]\n",
      "Reset environment\n",
      "Episode reward: 3165.4628908634186\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.634829 11.625688 11.595835 10.650356 10.805237 11.630808]\n",
      "Reset environment\n",
      "Episode reward: 1691.1846486330032\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.636333  11.627197  11.597334  10.651997  10.8065605 11.632314 ]\n",
      "Reset environment\n",
      "Episode reward: 1113.84639275074\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.636214 11.626997 11.597288 10.651829 10.806468 11.632202]\n",
      "Reset environment\n",
      "Episode reward: 1741.5444902777672\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.637215 11.627995 11.598294 10.652939 10.807362 11.633203]\n",
      "Reset environment\n",
      "Episode reward: 4864.820100069046\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.640442 11.631228 11.601522 10.656403 10.81025  11.636431]\n",
      "Reset environment\n",
      "Episode reward: 1649.429439485073\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.641392 11.632176 11.602475 10.657451 10.811082 11.63738 ]\n",
      "Reset environment\n",
      "Episode reward: 1849.4074507430196\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6423235 11.633062  11.603455  10.658496  10.811942  11.63831  ]\n",
      "Reset environment\n",
      "Episode reward: 4133.241568297148\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6451025 11.635863  11.606196  10.6615    10.814493  11.6410885]\n",
      "Reset environment\n",
      "Episode reward: 5407.689471364021\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.648851 11.639591 11.609941 10.665503 10.817845 11.644831]\n",
      "Reset environment\n",
      "Episode reward: 2917.1144690327346\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.650477 11.641207 11.611587 10.667346 10.819388 11.646457]\n",
      "Reset environment\n",
      "Episode reward: -276.6975518018007\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6488085 11.639402  11.610052  10.665626  10.817867  11.644799 ]\n",
      "Reset environment\n",
      "Episode reward: 4809.534442484379\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.652644 11.643234 11.613871 10.669728 10.821302 11.648634]\n",
      "Reset environment\n",
      "Episode reward: -680.9001227617264\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.65086  11.641474 11.612075 10.667768 10.81969  11.646874]\n",
      "Reset environment\n",
      "Episode reward: 2128.851063013077\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.651965 11.642529 11.613227 10.668992 10.820703 11.64798 ]\n",
      "Reset environment\n",
      "Episode reward: 3023.8861516714096\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6538925 11.644435  11.61518   10.671092  10.82244   11.649907 ]\n",
      "Reset environment\n",
      "Episode reward: 1842.4102881457657\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.654948 11.645459 11.616264 10.672251 10.823389 11.650962]\n",
      "Reset environment\n",
      "Episode reward: 5273.707023561001\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.658576 11.649082 11.619883 10.676117 10.826654 11.654588]\n",
      "Reset environment\n",
      "Episode reward: -682.6624720096588\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.656808 11.647282 11.61817  10.674343 10.825053 11.652845]\n",
      "Reset environment\n",
      "Episode reward: 2169.1315841823816\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.657954 11.648481 11.619267 10.675633 10.826063 11.654   ]\n",
      "Reset environment\n",
      "Episode reward: 1650.1087688207626\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.659402 11.64993  11.620718 10.677225 10.827343 11.655448]\n",
      "Reset environment\n",
      "Episode reward: 288.6120488643646\n",
      "Total Steps: 8\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.659375 11.649905 11.620691 10.677234 10.827303 11.655421]\n",
      "Reset environment\n",
      "Episode reward: 3796.999047636986\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.661936 11.652457 11.623258 10.679979 10.829593 11.657987]\n",
      "Reset environment\n",
      "Episode reward: 5343.412015378475\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.665605 11.656091 11.626919 10.683905 10.832862 11.661654]\n",
      "Reset environment\n",
      "Episode reward: 2424.7517140954733\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.666853  11.657273  11.6282215 10.685317  10.83395   11.662896 ]\n",
      "Reset environment\n",
      "Episode reward: 3275.156856149435\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.668955 11.659433 11.630277 10.687625 10.835893 11.664993]\n",
      "Reset environment\n",
      "Episode reward: 4784.157287597656\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6721945 11.662655  11.633523  10.69111   10.838812  11.668235 ]\n",
      "Reset environment\n",
      "Episode reward: 2051.541638493538\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.673901 11.664355 11.635237 10.692977 10.840326 11.669942]\n",
      "Reset environment\n",
      "Episode reward: 1488.2969970703125\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.67476   11.6651945 11.636108  10.693925  10.841098  11.670797 ]\n",
      "Reset environment\n",
      "Episode reward: 5102.927084445953\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.678737 11.669144 11.640108 10.698227 10.844688 11.674784]\n",
      "Reset environment\n",
      "Episode reward: 5602.799220979214\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.682642 11.673049 11.644002 10.702363 10.848209 11.678686]\n",
      "Reset environment\n",
      "Episode reward: 2584.652908205986\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.684272 11.674671 11.645635 10.704133 10.84965  11.680312]\n",
      "Reset environment\n",
      "Episode reward: 4449.5296196341515\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.687296 11.6777   11.648652 10.707363 10.852365 11.68334 ]\n",
      "Reset environment\n",
      "Episode reward: 1401.9258441925049\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.688075 11.678473 11.649435 10.708238 10.853048 11.684118]\n",
      "Reset environment\n",
      "Episode reward: 1882.3290793001652\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.688927 11.679273 11.650334 10.709216 10.853805 11.684969]\n",
      "Reset environment\n",
      "Episode reward: 2014.5881222486496\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.690102 11.68046  11.651497 10.710515 10.85487  11.686145]\n",
      "Reset environment\n",
      "Episode reward: 16.110634565353394\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.688614  11.6791315 11.649886  10.708963  10.853554  11.684667 ]\n",
      "Reset environment\n",
      "Episode reward: 2060.3445475697517\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.69035  11.680873 11.651612 10.710866 10.855114 11.686404]\n",
      "Reset environment\n",
      "Episode reward: 3049.079469628632\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.69226   11.682827  11.653471  10.712971  10.8568535 11.688312 ]\n",
      "Reset environment\n",
      "Episode reward: 2642.3729162216187\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.694423 11.684983 11.655639 10.71532  10.858783 11.690476]\n",
      "Reset environment\n",
      "Episode reward: 1996.824800863862\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.695531 11.686067 11.656781 10.716548 10.85979  11.691589]\n",
      "Reset environment\n",
      "Episode reward: 2934.804996676743\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.6974745 11.688042  11.6586685 10.718632  10.861601  11.693538 ]\n",
      "Reset environment\n",
      "Episode reward: 2418.4789793491364\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.699477 11.690043 11.660667 10.720805 10.863403 11.695543]\n",
      "Reset environment\n",
      "Episode reward: 2381.3513483405113\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.700914 11.691475 11.662113 10.72238  10.864687 11.696981]\n",
      "Reset environment\n",
      "Episode reward: 1386.6756454706192\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.702167 11.692727 11.663363 10.72375  10.865796 11.698233]\n",
      "Reset environment\n",
      "Episode reward: 656.4919122457504\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.701579 11.692045 11.662862 10.723086 10.865317 11.697651]\n",
      "Reset environment\n",
      "Episode reward: 5070.662014901638\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.705024 11.695482 11.6663   10.726771 10.868405 11.701097]\n",
      "Reset environment\n",
      "Episode reward: 3613.713976278901\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.707341 11.697834 11.668578 10.729284 10.870495 11.70342 ]\n",
      "Reset environment\n",
      "Episode reward: 4111.3982155919075\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.710081 11.700552 11.671317 10.732227 10.87295  11.706154]\n",
      "Reset environment\n",
      "Episode reward: 2307.2390077114105\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.711481 11.701946 11.672725 10.733762 10.874189 11.707554]\n",
      "Reset environment\n",
      "Episode reward: 3028.9926012456417\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.713305 11.703805 11.674477 10.735772 10.875807 11.709377]\n",
      "Reset environment\n",
      "Episode reward: 4592.536690592766\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.716416 11.706905 11.677596 10.739111 10.878576 11.712494]\n",
      "Reset environment\n",
      "Episode reward: 5267.053898572922\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.72056   11.711047  11.6817255 10.7435055 10.882316  11.716633 ]\n",
      "Reset environment\n",
      "Episode reward: -117.6571753025055\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.719192 11.709537 11.680499 10.742069 10.881081 11.715278]\n",
      "Reset environment\n",
      "Episode reward: 1663.650361418724\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.720131 11.710456 11.681455 10.743113 10.881924 11.716218]\n",
      "Reset environment\n",
      "Episode reward: 4562.02359354496\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.723218 11.713517 11.684552 10.746407 10.884693 11.719295]\n",
      "Reset environment\n",
      "Episode reward: 5334.8823764920235\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.726883  11.717172  11.688211  10.7503195 10.887972  11.722959 ]\n",
      "Reset environment\n",
      "Episode reward: 2961.396951943636\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.728652 11.718908 11.690018 10.752246 10.889556 11.724727]\n",
      "Reset environment\n",
      "Episode reward: 1694.8862788677216\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.729625  11.719889  11.690988  10.753327  10.89044   11.7256975]\n",
      "Reset environment\n",
      "Episode reward: 1876.6205291450024\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.730695 11.720984 11.692016 10.754514 10.891382 11.726766]\n",
      "Reset environment\n",
      "Episode reward: 4015.2698551416397\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.733339 11.723618 11.694676 10.757367 10.89378  11.729416]\n",
      "Reset environment\n",
      "Episode reward: 4372.820974886417\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.736284 11.726547 11.697635 10.760531 10.89641  11.732367]\n",
      "Reset environment\n",
      "Episode reward: 1717.3599801659584\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.737724  11.727989  11.699078  10.7621565 10.897682  11.733807 ]\n",
      "Reset environment\n",
      "Episode reward: 1847.8156049251556\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.738687 11.728995 11.699988 10.76325  10.898542 11.734771]\n",
      "Reset environment\n",
      "Episode reward: 3004.594904065132\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.740522  11.730835  11.701824  10.765243  10.900166  11.7366085]\n",
      "Reset environment\n",
      "Episode reward: 2291.525279343128\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.7419    11.732221  11.703197  10.766751  10.9014015 11.7379875]\n",
      "Reset environment\n",
      "Episode reward: 2943.6308107972145\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.743746 11.734051 11.705064 10.768762 10.903064 11.739837]\n",
      "Reset environment\n",
      "Episode reward: 1749.49077546224\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.744571 11.734905 11.705859 10.769575 10.903905 11.740668]\n",
      "Reset environment\n",
      "Episode reward: 2542.1936227828264\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.745918 11.736205 11.707262 10.771087 10.905109 11.742017]\n",
      "Reset environment\n",
      "Episode reward: 1507.3005821108818\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.746469 11.736819 11.707753 10.771622 10.905629 11.742572]\n",
      "Reset environment\n",
      "Episode reward: 3874.768424630165\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.749015 11.739372 11.71027  10.774335 10.907897 11.745118]\n",
      "Reset environment\n",
      "Episode reward: 1602.061654806137\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.749769  11.740183  11.710976  10.775203  10.9085655 11.745875 ]\n",
      "Reset environment\n",
      "Episode reward: 5007.740766048431\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.753165  11.743559  11.714389  10.77884   10.911662  11.7492695]\n",
      "Reset environment\n",
      "Episode reward: 1473.6015567183495\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.754503 11.744896 11.71573  10.780298 10.912851 11.750609]\n",
      "Reset environment\n",
      "Episode reward: 1860.2803011536598\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.755566 11.745959 11.716786 10.781479 10.913807 11.751671]\n",
      "Reset environment\n",
      "Episode reward: 1967.6487435996532\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.7566   11.746958 11.717855 10.782623 10.914727 11.75271 ]\n",
      "Reset environment\n",
      "Episode reward: 1654.1920325756073\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.757456 11.747776 11.71876  10.783564 10.915523 11.753568]\n",
      "Reset environment\n",
      "Episode reward: 1868.5321307256818\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.758215 11.748576 11.719479 10.784338 10.916194 11.754348]\n",
      "Reset environment\n",
      "Episode reward: 3843.9457389712334\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.7607355 11.751072  11.722025  10.787057  10.918446  11.756869 ]\n",
      "Reset environment\n",
      "Episode reward: 4784.393622279167\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.763964 11.7543   11.725248 10.790518 10.92136  11.760102]\n",
      "Reset environment\n",
      "Episode reward: 1410.8443045020103\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.765215 11.755554 11.726499 10.791896 10.922465 11.761354]\n",
      "Reset environment\n",
      "Episode reward: 4889.73560833931\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.768483 11.75882  11.729754 10.795411 10.925409 11.764616]\n",
      "Reset environment\n",
      "Episode reward: 5637.439754724503\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.772361 11.76268  11.733642 10.799555 10.9289   11.768496]\n",
      "Reset environment\n",
      "Episode reward: 1378.6288199424744\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.773009 11.763289 11.734332 10.800284 10.9295   11.769141]\n",
      "Reset environment\n",
      "Episode reward: 1352.121911406517\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.774193 11.764476 11.735516 10.801595 10.930532 11.770326]\n",
      "Reset environment\n",
      "Episode reward: 1907.3785367012024\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.7753525 11.765631  11.736678  10.802861  10.931586  11.771483 ]\n",
      "Reset environment\n",
      "Episode reward: 1396.8363342285156\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.776586 11.766863 11.73791  10.804215 10.932685 11.772715]\n",
      "Reset environment\n",
      "Episode reward: 2892.4818377420306\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.778384 11.768689 11.739695 10.806233 10.9344   11.77452 ]\n",
      "Reset environment\n",
      "Episode reward: 2845.4886027127504\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.780216 11.770553 11.741477 10.808225 10.936122 11.776357]\n",
      "Reset environment\n",
      "Episode reward: 4033.4640131890774\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.782875  11.773188  11.744151  10.811093  10.938509  11.7790165]\n",
      "Reset environment\n",
      "Episode reward: 3293.0920607447624\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.785075 11.775419 11.74629  10.813514 10.940539 11.781223]\n",
      "Reset environment\n",
      "Episode reward: 2188.0465413331985\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.78688  11.777233 11.748094 10.815485 10.94217  11.78303 ]\n",
      "Reset environment\n",
      "Episode reward: 5983.260106086731\n",
      "Total Steps: 201\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.791009 11.781354 11.752206 10.819849 10.945892 11.787155]\n",
      "Reset environment\n",
      "Episode reward: 2006.161084631458\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.791905 11.78219  11.75317  10.820893 10.946679 11.788053]\n",
      "Reset environment\n",
      "Episode reward: 1277.2046294212341\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.792337 11.78256  11.753675 10.821414 10.947083 11.788482]\n",
      "Reset environment\n",
      "Episode reward: 2191.9381912350655\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.793653 11.783872 11.754998 10.82286  10.948254 11.789801]\n",
      "Reset environment\n",
      "Episode reward: 2189.743478892371\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.79492  11.785113 11.756297 10.824242 10.949374 11.791066]\n",
      "Reset environment\n",
      "Episode reward: 4643.960938692093\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.798015 11.788209 11.759368 10.827562 10.952133 11.794157]\n",
      "Reset environment\n",
      "Episode reward: 2999.723628461361\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.799912 11.790119 11.761243 10.829624 10.953845 11.796053]\n",
      "Reset environment\n",
      "Episode reward: 3793.343792170286\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.80244  11.79265  11.763748 10.832354 10.956118 11.798576]\n",
      "Reset environment\n",
      "Episode reward: 3757.0319356918335\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.804785 11.794989 11.766103 10.834895 10.958177 11.800924]\n",
      "Reset environment\n",
      "Episode reward: 1769.666285276413\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.805844 11.796053 11.767153 10.836053 10.959149 11.801982]\n",
      "Reset environment\n",
      "Episode reward: 1388.7692392468452\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.807069  11.797279  11.7683735 10.837399  10.960234  11.803207 ]\n",
      "Reset environment\n",
      "Episode reward: 173.7155418395996\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.806441 11.796643 11.767753 10.836516 10.959704 11.802581]\n",
      "Reset environment\n",
      "Episode reward: 1075.3003027439117\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.806956 11.797183 11.768243 10.837102 10.960161 11.803091]\n",
      "Reset environment\n",
      "Episode reward: 1393.1799478530884\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.807728  11.797959  11.76901   10.837968  10.9608555 11.803863 ]\n",
      "Reset environment\n",
      "Episode reward: 1076.8805906772614\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.807871 11.798135 11.769126 10.837993 10.961037 11.804013]\n",
      "Reset environment\n",
      "Episode reward: 4039.609836399555\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.810482 11.800764 11.771706 10.840807 10.963381 11.806621]\n",
      "Reset environment\n",
      "Episode reward: 1371.515567958355\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.81169  11.801976 11.772913 10.842152 10.964449 11.807827]\n",
      "Reset environment\n",
      "Episode reward: 1757.3731149435043\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.812713 11.802998 11.773937 10.843282 10.965358 11.80885 ]\n",
      "Reset environment\n",
      "Episode reward: 4196.789313554764\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.815482 11.805749 11.776716 10.846256 10.96786  11.81162 ]\n",
      "Reset environment\n",
      "Episode reward: 2780.9282200336456\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.817017  11.807335  11.7781925 10.847992  10.969253  11.813162 ]\n",
      "Reset environment\n",
      "Episode reward: 1561.071002960205\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.81793   11.8082485 11.779113  10.849     10.970071  11.814076 ]\n",
      "Reset environment\n",
      "Episode reward: 2085.336839556694\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.819646 11.809963 11.780827 10.850869 10.971588 11.815791]\n",
      "Reset environment\n",
      "Episode reward: 1914.8485587835312\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.821246 11.811571 11.782425 10.852621 10.973004 11.81739 ]\n",
      "Reset environment\n",
      "Episode reward: 2628.5998140871525\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.82287   11.813228  11.783979  10.85438   10.974484  11.8190155]\n",
      "Reset environment\n",
      "Episode reward: 2688.5974227190018\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.824397 11.81481  11.785441 10.856068 10.975846 11.820542]\n",
      "Reset environment\n",
      "Episode reward: 3572.808494567871\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.826653 11.817065 11.787685 10.858537 10.977876 11.822799]\n",
      "Reset environment\n",
      "Episode reward: 1383.8605243563652\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.827863  11.8182745 11.788895  10.859873  10.978927  11.824009 ]\n",
      "Reset environment\n",
      "Episode reward: 2155.6150381565094\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.829095 11.819495 11.790149 10.861234 10.980034 11.825239]\n",
      "Reset environment\n",
      "Episode reward: 1688.6350518465042\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.83008   11.820478  11.791138  10.86232   10.9809265 11.826224 ]\n",
      "Reset environment\n",
      "Episode reward: 1697.9249060451984\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.8309355 11.821302  11.792032  10.863276  10.981688  11.827081 ]\n",
      "Reset environment\n",
      "Episode reward: 2011.0726751685143\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.832589 11.822955 11.793686 10.865088 10.983146 11.828735]\n",
      "Reset environment\n",
      "Episode reward: 4736.884792625904\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.835589  11.825959  11.796686  10.868304  10.985788  11.8317375]\n",
      "Reset environment\n",
      "Episode reward: 1585.0268416404724\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.836514 11.826883 11.797611 10.86933  10.98663  11.832663]\n",
      "Reset environment\n",
      "Episode reward: 3996.490846693516\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.838996 11.829373 11.800094 10.872006 10.988825 11.835147]\n",
      "Reset environment\n",
      "Episode reward: 2115.12425160408\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.840307 11.830688 11.801401 10.873434 10.990025 11.836462]\n",
      "Reset environment\n",
      "Episode reward: 1439.3199732899666\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.84158  11.831967 11.80267  10.874828 10.991143 11.837737]\n",
      "Reset environment\n",
      "Episode reward: 1309.3282942771912\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.842047 11.832496 11.803081 10.875402 10.991543 11.838207]\n",
      "Reset environment\n",
      "Episode reward: 6287.987014234066\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.823463 11.81394  11.784567 10.846483 10.974083 11.819647]\n",
      "Reset environment\n",
      "Episode reward: 482.2593330144882\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.823217  11.813687  11.7843075 10.846088  10.973873  11.8194   ]\n",
      "Reset environment\n",
      "Episode reward: -49.69730019569397\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.821858  11.81248   11.782811  10.8446245 10.972627  11.818053 ]\n",
      "Reset environment\n",
      "Episode reward: -252.61999893188477\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.820493 11.810991 11.781559 10.8433   10.971261 11.816696]\n",
      "Reset environment\n",
      "Episode reward: 3528.8109724521637\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.822745 11.813229 11.783826 10.845734 10.973281 11.818945]\n",
      "Reset environment\n",
      "Episode reward: 2197.5566823482513\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.824079 11.814563 11.785159 10.847194 10.974472 11.82028 ]\n",
      "Reset environment\n",
      "Episode reward: 2716.4789318218827\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.825833 11.816346 11.786889 10.84913  10.976132 11.822038]\n",
      "Reset environment\n",
      "Episode reward: 1371.5399407744408\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.827024 11.817538 11.788081 10.850442 10.977167 11.823232]\n",
      "Reset environment\n",
      "Episode reward: 2162.7739704549313\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.82827  11.818824 11.789289 10.851813 10.978318 11.824478]\n",
      "Reset environment\n",
      "Episode reward: 1875.615767121315\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.829384  11.8199215 11.790418  10.853034  10.979336  11.825596 ]\n",
      "Reset environment\n",
      "Episode reward: 4905.7089993953705\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.8327   11.823242 11.793727 10.856566 10.982348 11.828907]\n",
      "Reset environment\n",
      "Episode reward: 2304.6114063858986\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.833923 11.824428 11.794988 10.857924 10.983445 11.830133]\n",
      "Reset environment\n",
      "Episode reward: 3644.168489217758\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.836279 11.826794 11.797342 10.860459 10.985539 11.832493]\n",
      "Reset environment\n",
      "Episode reward: 3955.391693532467\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.838817 11.829346 11.799877 10.863206 10.987896 11.835042]\n",
      "Reset environment\n",
      "Episode reward: 3234.180047005415\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.840865  11.831382  11.801932  10.865428  10.989736  11.8370905]\n",
      "Reset environment\n",
      "Episode reward: 3078.6251053214073\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.842783 11.833293 11.803861 10.867508 10.991454 11.83901 ]\n",
      "Reset environment\n",
      "Episode reward: 1584.7237488552928\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.843287  11.833853  11.804319  10.868149  10.991872  11.8395195]\n",
      "Reset environment\n",
      "Episode reward: 3318.2183378487825\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.845367 11.835913 11.806423 10.870398 10.993741 11.841598]\n",
      "Reset environment\n",
      "Episode reward: 1543.1357766389847\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.846223 11.83676  11.807292 10.871351 10.994505 11.842453]\n",
      "Reset environment\n",
      "Episode reward: 2783.6904084831476\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.847842 11.838425 11.808876 10.873185 10.996    11.844073]\n",
      "Reset environment\n",
      "Episode reward: 146.27391028404236\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.847022 11.837557 11.808101 10.872122 10.99536  11.843257]\n",
      "Reset environment\n",
      "Episode reward: 5085.791155695915\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.850415 11.840938 11.811508 10.875774 10.998467 11.846647]\n",
      "Reset environment\n",
      "Episode reward: 2839.7442023158073\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.8520355 11.842628  11.813076  10.877565  10.999919  11.848276 ]\n",
      "Reset environment\n",
      "Episode reward: 1733.934327363968\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.852998 11.843562 11.814067 10.878622 11.000787 11.849238]\n",
      "Reset environment\n",
      "Episode reward: 1664.204973936081\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.854397 11.844965 11.815467 10.880157 11.00202  11.850638]\n",
      "Reset environment\n",
      "Episode reward: 4369.894853115082\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.857274  11.847841  11.818341  10.883247  11.0046215 11.8535185]\n",
      "Reset environment\n",
      "Episode reward: 2027.2923467569053\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.858391 11.848921 11.819496 10.884478 11.00561  11.854634]\n",
      "Reset environment\n",
      "Episode reward: 5.825040698051453\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.857561 11.848099 11.818651 10.883443 11.004891 11.853806]\n",
      "Reset environment\n",
      "Episode reward: 3255.3292495012283\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.859624 11.850162 11.820699 10.885659 11.006738 11.855868]\n",
      "Reset environment\n",
      "Episode reward: 1400.45603938587\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.85997  11.850557 11.820995 10.88598  11.007061 11.856226]\n",
      "Reset environment\n",
      "Episode reward: 1740.1031477451324\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.861432 11.852022 11.822454 10.887579 11.008356 11.857688]\n",
      "Reset environment\n",
      "Episode reward: 2260.8117104768753\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.86278  11.853356 11.823813 10.889052 11.009538 11.859033]\n",
      "Reset environment\n",
      "Episode reward: 2340.3901473283768\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.864201 11.854776 11.825239 10.890599 11.010808 11.860455]\n",
      "Reset environment\n",
      "Episode reward: 817.3313534259796\n",
      "Total Steps: 25\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.864551  11.855123  11.825588  10.891012  11.011116  11.8608055]\n",
      "Reset environment\n",
      "Episode reward: 3486.5169823765755\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.866789 11.857341 11.827845 10.893424 11.013132 11.863041]\n",
      "Reset environment\n",
      "Episode reward: 4594.9898045659065\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.869845 11.860382 11.830909 10.896698 11.015878 11.8661  ]\n",
      "Reset environment\n",
      "Episode reward: 2876.644321024418\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.871587  11.862108  11.832665  10.8985815 11.017427  11.867843 ]\n",
      "Reset environment\n",
      "Episode reward: 3148.104136645794\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.873346 11.863837 11.834473 10.900543 11.019016 11.869604]\n",
      "Reset environment\n",
      "Episode reward: 3448.665306057781\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.875301 11.865799 11.836454 10.902735 11.020886 11.871567]\n",
      "Reset environment\n",
      "Episode reward: 5663.735505640507\n",
      "Total Steps: 193\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.879093  11.869566  11.840238  10.906803  11.0242815 11.875361 ]\n",
      "Reset environment\n",
      "Episode reward: 3168.6375123262405\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.881081 11.871546 11.842231 10.908951 11.02605  11.877349]\n",
      "Reset environment\n",
      "Episode reward: 2693.9984669685364\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.882715  11.873166  11.843879  10.910734  11.0275135 11.878983 ]\n",
      "Reset environment\n",
      "Episode reward: 3090.509797513485\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.884641 11.875102 11.845797 10.912827 11.029251 11.880912]\n",
      "Reset environment\n",
      "Episode reward: 2429.242149785161\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.886032 11.87647  11.847211 10.914341 11.030488 11.882303]\n",
      "Reset environment\n",
      "Episode reward: -686.7140321731567\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.884307 11.874736 11.845459 10.91249  11.028891 11.880581]\n",
      "Reset environment\n",
      "Episode reward: 2294.961349129677\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.886162 11.876584 11.84732  10.914499 11.030536 11.882437]\n",
      "Reset environment\n",
      "Episode reward: 1607.1251359581947\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.887039 11.877467 11.848189 10.915475 11.031323 11.883311]\n",
      "Reset environment\n",
      "Episode reward: 3462.6132145114243\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.889178 11.879573 11.850366 10.917792 11.033238 11.88545 ]\n",
      "Reset environment\n",
      "Episode reward: 879.653461933136\n",
      "Total Steps: 27\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.889578 11.879976 11.850763 10.918258 11.033594 11.88585 ]\n",
      "Reset environment\n",
      "Episode reward: 629.6311182975769\n",
      "Total Steps: 19\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.88979  11.880184 11.850974 10.918525 11.03377  11.886062]\n",
      "Reset environment\n",
      "Episode reward: 5281.019349217415\n",
      "Total Steps: 184\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.893255 11.88362  11.854439 10.922262 11.036849 11.889532]\n",
      "Reset environment\n",
      "Episode reward: 3676.7176635861397\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.895626 11.885976 11.856818 10.92481  11.038953 11.891905]\n",
      "Reset environment\n",
      "Episode reward: -614.1932909488678\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.8939705 11.884261  11.855214  10.922819  11.037484  11.890261 ]\n",
      "Reset environment\n",
      "Episode reward: 2277.9598255306482\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.895253  11.885502  11.856534  10.924224  11.038621  11.8915415]\n",
      "Reset environment\n",
      "Episode reward: 1380.8632435798645\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.895993 11.886224 11.857291 10.925039 11.039279 11.892282]\n",
      "Reset environment\n",
      "Episode reward: 2373.1353021860123\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.897882 11.888108 11.859182 10.927083 11.040963 11.89417 ]\n",
      "Reset environment\n",
      "Episode reward: 3033.237324118614\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.899794 11.890008 11.861108 10.929164 11.042662 11.896083]\n",
      "Reset environment\n",
      "Episode reward: 3454.5998304486275\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.901965 11.892205 11.86325  10.931515 11.044633 11.898254]\n",
      "Reset environment\n",
      "Episode reward: 2831.7320421636105\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.903691  11.893901  11.864991  10.9333725 11.046172  11.899976 ]\n",
      "Reset environment\n",
      "Episode reward: 2383.537118911743\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.905101 11.895318 11.866392 10.934918 11.047437 11.901385]\n",
      "Reset environment\n",
      "Episode reward: 2924.39992249012\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.906844 11.897065 11.868136 10.936808 11.048963 11.90313 ]\n",
      "Reset environment\n",
      "Episode reward: 1308.5833566784859\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.907149 11.897439 11.868387 10.93706  11.049271 11.903445]\n",
      "Reset environment\n",
      "Episode reward: 3313.591933131218\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.909237  11.8995285 11.870475  10.939309  11.051156  11.905531 ]\n",
      "Reset environment\n",
      "Episode reward: 3389.174305677414\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.91116  11.901501 11.872361 10.941447 11.052856 11.90746 ]\n",
      "Reset environment\n",
      "Episode reward: 3192.234884083271\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.913148 11.903463 11.874363 10.943587 11.054642 11.909448]\n",
      "Reset environment\n",
      "Episode reward: 2663.542443871498\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.914707 11.905013 11.875945 10.945298 11.056053 11.91101 ]\n",
      "Reset environment\n",
      "Episode reward: 1371.2756760716438\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.915897 11.906201 11.877137 10.946613 11.057102 11.912197]\n",
      "Reset environment\n",
      "Episode reward: 312.7811242341995\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.915327 11.905653 11.876537 10.945836 11.05663  11.911623]\n",
      "Reset environment\n",
      "Episode reward: 2026.7189486026764\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.916461 11.906799 11.877657 10.947082 11.057628 11.912757]\n",
      "Reset environment\n",
      "Episode reward: 3897.373801112175\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.918954  11.909312  11.880118  10.9497795 11.059874  11.915254 ]\n",
      "Reset environment\n",
      "Episode reward: 2236.022950232029\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.920205 11.910567 11.88137  10.951169 11.060983 11.916506]\n",
      "Reset environment\n",
      "Episode reward: 2949.0601817443967\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.9219885 11.912389  11.883108  10.953123  11.062626  11.918292 ]\n",
      "Reset environment\n",
      "Episode reward: 4616.591063678265\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.9248295 11.915227  11.885947  10.9562    11.065133  11.921135 ]\n",
      "Reset environment\n",
      "Episode reward: 1719.6196096539497\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.926258 11.916652 11.887376 10.957774 11.066386 11.922563]\n",
      "Reset environment\n",
      "Episode reward: 1871.5495565533638\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.927301 11.917706 11.888412 10.958925 11.067312 11.923607]\n",
      "Reset environment\n",
      "Episode reward: 1802.2775375247002\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.928448 11.918861 11.889541 10.960179 11.068377 11.924753]\n",
      "Reset environment\n",
      "Episode reward: 2086.2829281687737\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.930145 11.920553 11.891235 10.962043 11.069884 11.92645 ]\n",
      "Reset environment\n",
      "Episode reward: 2600.7945907115936\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.931699 11.922109 11.892772 10.963733 11.071248 11.927999]\n",
      "Reset environment\n",
      "Episode reward: 4921.858966588974\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.934934 11.925326 11.896008 10.967192 11.074158 11.93123 ]\n",
      "Reset environment\n",
      "Episode reward: 4420.0562581419945\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.937821 11.928192 11.898909 10.970283 11.076752 11.934112]\n",
      "Reset environment\n",
      "Episode reward: 4086.9749265909195\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.940465 11.930846 11.901545 10.973112 11.079125 11.936753]\n",
      "Reset environment\n",
      "Episode reward: 5602.732254207134\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.944168 11.934563 11.905242 10.977069 11.082463 11.940462]\n",
      "Reset environment\n",
      "Episode reward: 2628.58435434103\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.945797 11.936187 11.90688  10.978839 11.083919 11.942092]\n",
      "Reset environment\n",
      "Episode reward: 2452.4561973810196\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.947239 11.937639 11.908311 10.980416 11.085208 11.943533]\n",
      "Reset environment\n",
      "Episode reward: 4417.480954110622\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.950137  11.9405155 11.911213  10.98351   11.087779  11.946421 ]\n",
      "Reset environment\n",
      "Episode reward: 2069.0007190704346\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.951327 11.94171  11.912402 10.984816 11.088814 11.947611]\n",
      "Reset environment\n",
      "Episode reward: 4747.589962661266\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.95444  11.944819 11.915519 10.988147 11.091597 11.95073 ]\n",
      "Reset environment\n",
      "Episode reward: 2312.366098323837\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.95571   11.946069  11.916814  10.989545  11.09272   11.9520035]\n",
      "Reset environment\n",
      "Episode reward: 3601.105743587017\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.957986 11.948345 11.91909  10.992002 11.094788 11.954279]\n",
      "Reset environment\n",
      "Episode reward: 1391.8974197506905\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.958653 11.948995 11.919773 10.992741 11.095445 11.954943]\n",
      "Reset environment\n",
      "Episode reward: 1021.1512078046799\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.95879  11.949186 11.919858 10.992747 11.095636 11.955077]\n",
      "Reset environment\n",
      "Episode reward: 2767.97080296278\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.960341 11.950783 11.921364 10.994454 11.097006 11.95663 ]\n",
      "Reset environment\n",
      "Episode reward: 2690.0747878327966\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.961756 11.952218 11.922734 10.99602  11.098254 11.958047]\n",
      "Reset environment\n",
      "Episode reward: 1978.0646770000458\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.962856 11.953357 11.923801 10.997239 11.099264 11.959151]\n",
      "Reset environment\n",
      "Episode reward: 2390.1915478631854\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.964083  11.954613  11.924988  10.998606  11.100341  11.9603815]\n",
      "Reset environment\n",
      "Episode reward: 3494.536789596081\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.966269  11.956802  11.92717   11.0009775 11.102293  11.9625635]\n",
      "Reset environment\n",
      "Episode reward: 4423.395628929138\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.969128 11.959655 11.930021 11.004049 11.104855 11.965425]\n",
      "Reset environment\n",
      "Episode reward: 1290.744121313095\n",
      "Total Steps: 41\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.96981  11.960333 11.93071  11.004809 11.105465 11.966106]\n",
      "Reset environment\n",
      "Episode reward: 2167.0081092715263\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.971543 11.962063 11.93245  11.006689 11.107003 11.967839]\n",
      "Reset environment\n",
      "Episode reward: 2152.1541341543198\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.9727745 11.963307  11.933673  11.008038  11.108094  11.969071 ]\n",
      "Reset environment\n",
      "Episode reward: 4702.7519963383675\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.975899 11.966433 11.936782 11.011375 11.110923 11.972194]\n",
      "Reset environment\n",
      "Episode reward: 4156.270124197006\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.978431  11.9689665 11.9393015 11.014112  11.113148  11.974724 ]\n",
      "Reset environment\n",
      "Episode reward: 4401.983857810497\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.981278 11.971812 11.942152 11.017167 11.115714 11.977574]\n",
      "Reset environment\n",
      "Episode reward: 1582.6646345555782\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.981929 11.972415 11.942847 11.017789 11.116326 11.978228]\n",
      "Reset environment\n",
      "Episode reward: 3422.748612344265\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.984052  11.974522  11.944978  11.0200815 11.11821   11.980349 ]\n",
      "Reset environment\n",
      "Episode reward: 2065.94041287899\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.985723 11.976194 11.946647 11.021908 11.119704 11.982021]\n",
      "Reset environment\n",
      "Episode reward: 2650.3000848293304\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.987221  11.977701  11.948146  11.023541  11.121054  11.9835205]\n",
      "Reset environment\n",
      "Episode reward: 5270.798456668854\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.990673 11.981151 11.95159  11.027228 11.124179 11.986972]\n",
      "Reset environment\n",
      "Episode reward: 5051.017429292202\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.993983 11.984448 11.9549   11.030777 11.127125 11.990283]\n",
      "Reset environment\n",
      "Episode reward: 2379.0145959854126\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.995221  11.985646  11.9561825 11.032154  11.128234  11.99152  ]\n",
      "Reset environment\n",
      "Episode reward: 2238.598174750805\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.996536 11.986968 11.957491 11.033596 11.129427 11.992837]\n",
      "Reset environment\n",
      "Episode reward: 1832.8994798511267\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.99749  11.98795  11.958418 11.034663 11.130309 11.993801]\n",
      "Reset environment\n",
      "Episode reward: 1912.6486753225327\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [11.998552  11.989005  11.959496  11.035845  11.1312475 11.9948635]\n",
      "Reset environment\n",
      "Episode reward: 3569.0947306752205\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.001001 11.991463 11.961932 11.038458 11.133507 11.997316]\n",
      "Reset environment\n",
      "Episode reward: 2592.600682556629\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.002582  11.993062  11.963486  11.0401745 11.134965  11.998904 ]\n",
      "Reset environment\n",
      "Episode reward: 2540.934020526707\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.003996 11.994442 11.964932 11.041721 11.136236 12.000317]\n",
      "Reset environment\n",
      "Episode reward: 3714.9674690663815\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.006328 11.996801 11.967234 11.044241 11.138347 12.002651]\n",
      "Reset environment\n",
      "Episode reward: 1056.671913743019\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.006575 11.997078 11.96743  11.044365 11.138613 12.002898]\n",
      "Reset environment\n",
      "Episode reward: 4357.983581483364\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.009423 11.999928 11.970274 11.047408 11.141181 12.005742]\n",
      "Reset environment\n",
      "Episode reward: 4904.932027339935\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.012593 12.003098 11.973421 11.050816 11.144034 12.008912]\n",
      "Reset environment\n",
      "Episode reward: 4756.453610479832\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.015664 12.006148 11.976499 11.054109 11.146777 12.011988]\n",
      "Reset environment\n",
      "Episode reward: 1824.8215714097023\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.016707 12.007197 11.977535 11.055252 11.147724 12.013031]\n",
      "Reset environment\n",
      "Episode reward: 1027.374820947647\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.016556 12.00712  11.977303 11.054952 11.147585 12.012881]\n",
      "Reset environment\n",
      "Episode reward: 799.8007154464722\n",
      "Total Steps: 26\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.016865 12.007417 11.97762  11.055334 11.147847 12.013189]\n",
      "Reset environment\n",
      "Episode reward: 2007.1355105638504\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.018023  12.008565  11.97879   11.0566025 11.148882  12.014347 ]\n",
      "Reset environment\n",
      "Episode reward: 2413.5486867427826\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.019306 12.00981  11.98012  11.058043 11.150051 12.01563 ]\n",
      "Reset environment\n",
      "Episode reward: 3714.6260363459587\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.021665 12.012181 11.982474 11.060571 11.152178 12.017997]\n",
      "Reset environment\n",
      "Episode reward: 2595.7138406336308\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.023184 12.013682 11.984003 11.062221 11.153545 12.019518]\n",
      "Reset environment\n",
      "Episode reward: 2796.467557132244\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.02487  12.015395 11.985669 11.064119 11.155165 12.021205]\n",
      "Reset environment\n",
      "Episode reward: 1921.5717424154282\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.026439 12.01696  11.987243 11.065825 11.156534 12.022775]\n",
      "Reset environment\n",
      "Episode reward: 3382.7962726950645\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.028512 12.019015 11.989331 11.068066 11.158396 12.024852]\n",
      "Reset environment\n",
      "Episode reward: 4879.8200571238995\n",
      "Total Steps: 209\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.031881 12.022386 11.992711 11.071666 11.161552 12.028229]\n",
      "Reset environment\n",
      "Episode reward: 4989.244354665279\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.0351305 12.025639  11.99595   11.075136  11.16448   12.031477 ]\n",
      "Reset environment\n",
      "Episode reward: 3531.3561794161797\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.037257 12.027761 11.998063 11.077462 11.166392 12.033605]\n",
      "Reset environment\n",
      "Episode reward: 4219.0681656599045\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.039939 12.030426 12.00076  11.08035  11.168789 12.036287]\n",
      "Reset environment\n",
      "Episode reward: 2137.9502233862877\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.041166 12.031658 12.001981 11.081692 11.1699   12.037511]\n",
      "Reset environment\n",
      "Episode reward: 1414.5966102480888\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.042383 12.032876 12.003197 11.083028 11.170976 12.038725]\n",
      "Reset environment\n",
      "Episode reward: 961.3064298629761\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.042823 12.033319 12.003635 11.083546 11.171365 12.039165]\n",
      "Reset environment\n",
      "Episode reward: 3291.8845080286264\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.044829 12.035357 12.005585 11.085719 11.173178 12.041175]\n",
      "Reset environment\n",
      "Episode reward: 2040.041140139103\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.045997  12.036527  12.006753  11.087018  11.1742115 12.042343 ]\n",
      "Reset environment\n",
      "Episode reward: 1565.8843141198158\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.046818 12.037339 12.007581 11.087938 11.174937 12.043165]\n",
      "Reset environment\n",
      "Episode reward: 4151.562047362328\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.049456 12.039973 12.010235 11.090773 11.177287 12.045806]\n",
      "Reset environment\n",
      "Episode reward: 2788.079157233238\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.051152  12.041658  12.011935  11.092613  11.178774  12.0475025]\n",
      "Reset environment\n",
      "Episode reward: 2807.071519076824\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.052806 12.043291 12.013603 11.094415 11.180269 12.04916 ]\n",
      "Reset environment\n",
      "Episode reward: 5539.922611176968\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.056456 12.046928 12.017267 11.098322 11.183545 12.052809]\n",
      "Reset environment\n",
      "Episode reward: 4171.327616453171\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.058979 12.04945  12.019795 11.101052 11.185773 12.055331]\n",
      "Reset environment\n",
      "Episode reward: 2478.7829306721687\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.060443  12.05092   12.0212555 11.102651  11.1870775 12.056792 ]\n",
      "Reset environment\n",
      "Episode reward: 2994.112584590912\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.062257 12.052734 12.023055 11.104616 11.188688 12.058601]\n",
      "Reset environment\n",
      "Episode reward: 4613.579912781715\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.065072 12.055536 12.025878 11.107666 11.19118  12.061418]\n",
      "Reset environment\n",
      "Episode reward: 123.23633256554604\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.063341 12.054004 12.023972 11.106047 11.189533 12.05971 ]\n",
      "Reset environment\n",
      "Episode reward: 1944.4534431695938\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.0648985 12.05556   12.025528  11.107756  11.190903  12.061266 ]\n",
      "Reset environment\n",
      "Episode reward: 1332.9762806892395\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.065578 12.056265 12.026182 11.108518 11.191518 12.061943]\n",
      "Reset environment\n",
      "Episode reward: 915.4753819704056\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.065178 12.055945 12.025697 11.107966 11.19113  12.061548]\n",
      "Reset environment\n",
      "Episode reward: 6042.5605635643005\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.069193 12.059956 12.029707 11.112212 11.194764 12.065562]\n",
      "Reset environment\n",
      "Episode reward: 3742.431390583515\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.0714855 12.062285  12.031973  11.114708  11.196842  12.067859 ]\n",
      "Reset environment\n",
      "Episode reward: 4278.408395469189\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.074199 12.064969 12.034689 11.117624 11.199257 12.070571]\n",
      "Reset environment\n",
      "Episode reward: 259.3631467819214\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.07336  12.064047 12.033968 11.116636 11.198569 12.069741]\n",
      "Reset environment\n",
      "Episode reward: 2770.327109262347\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.074937 12.065605 12.035572 11.118358 11.199977 12.071319]\n",
      "Reset environment\n",
      "Episode reward: 1920.3041532039642\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.076468 12.067125 12.037103 11.120038 11.201321 12.072848]\n",
      "Reset environment\n",
      "Episode reward: 1743.1104354858398\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.077405 12.068068 12.038036 11.121084 11.202161 12.073787]\n",
      "Reset environment\n",
      "Episode reward: 3511.6399211883545\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.079585 12.070222 12.040225 11.123419 11.204102 12.075962]\n",
      "Reset environment\n",
      "Episode reward: 1706.5002364516258\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.08097   12.071604  12.041608  11.124945  11.2052965 12.077347 ]\n",
      "Reset environment\n",
      "Episode reward: 2198.627737496048\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.082046 12.072633 12.042735 11.126149 11.206272 12.078422]\n",
      "Reset environment\n",
      "Episode reward: 1391.5305738449097\n",
      "Total Steps: 45\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.082779  12.073355  12.043479  11.126969  11.2069235 12.079156 ]\n",
      "Reset environment\n",
      "Episode reward: 1800.1357100605965\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.084229 12.074804 12.04492  11.128571 11.208187 12.080604]\n",
      "Reset environment\n",
      "Episode reward: 2176.897355750203\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.085391  12.075995  12.046032  11.1298685 11.209224  12.08177  ]\n",
      "Reset environment\n",
      "Episode reward: 2158.847023665905\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.086602 12.07719  12.047256 11.131196 11.210298 12.082979]\n",
      "Reset environment\n",
      "Episode reward: 3106.630695283413\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.088492 12.079086 12.049143 11.133258 11.212017 12.084876]\n",
      "Reset environment\n",
      "Episode reward: 330.63042628765106\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.087988 12.078614 12.048612 11.132529 11.211598 12.084377]\n",
      "Reset environment\n",
      "Episode reward: 2444.9772617816925\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.089326 12.079925 12.049977 11.133994 11.21282  12.085717]\n",
      "Reset environment\n",
      "Episode reward: 2424.352701306343\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.091226  12.0818205 12.051875  11.136044  11.21451   12.087619 ]\n",
      "Reset environment\n",
      "Episode reward: 5472.643298149109\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.094788 12.085372 12.055446 11.139847 11.217703 12.091187]\n",
      "Reset environment\n",
      "Episode reward: 1383.9888555407524\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.095957 12.086544 12.056616 11.141136 11.218739 12.092356]\n",
      "Reset environment\n",
      "Episode reward: 1413.5407312512398\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.097149 12.087737 12.057811 11.142445 11.219793 12.093549]\n",
      "Reset environment\n",
      "Episode reward: 2155.7319346368313\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.098183 12.088732 12.058889 11.143604 11.220711 12.094583]\n",
      "Reset environment\n",
      "Episode reward: 2035.1083141565323\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.099823 12.090363 12.060533 11.145392 11.222146 12.096221]\n",
      "Reset environment\n",
      "Episode reward: 2346.235285654664\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.101122  12.091625  12.061866  11.1468115 11.223311  12.09752  ]\n",
      "Reset environment\n",
      "Episode reward: 2196.82789760828\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.102354 12.092858 12.063094 11.148176 11.22438  12.098752]\n",
      "Reset environment\n",
      "Episode reward: 3935.969541311264\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.104826 12.095313 12.065589 11.150828 11.226608 12.101227]\n",
      "Reset environment\n",
      "Episode reward: 2119.964039862156\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.105807 12.096239 12.066622 11.151938 11.2275   12.102205]\n",
      "Reset environment\n",
      "Episode reward: 3146.9613962695003\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.107436  12.097888  12.0682535 11.153805  11.229056  12.103837 ]\n",
      "Reset environment\n",
      "Episode reward: 1012.1870629787445\n",
      "Total Steps: 32\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.107904 12.098343 12.068727 11.154339 11.22947  12.104303]\n",
      "Reset environment\n",
      "Episode reward: 4139.262626647949\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.110515  12.100957  12.071329  11.157167  11.231831  12.1069145]\n",
      "Reset environment\n",
      "Episode reward: 4758.772805273533\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.113544 12.103992 12.074349 11.160418 11.234553 12.109944]\n",
      "Reset environment\n",
      "Episode reward: 3153.701007962227\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.115467 12.105915 12.076264 11.162489 11.236282 12.111867]\n",
      "Reset environment\n",
      "Episode reward: 2298.727078527212\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.116616 12.107117 12.077345 11.163781 11.237301 12.113014]\n",
      "Reset environment\n",
      "Episode reward: 1708.661847680807\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.117491 12.10798  12.07824  11.164757 11.238064 12.113892]\n",
      "Reset environment\n",
      "Episode reward: 3896.1632565259933\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.119926  12.110421  12.080674  11.1673765 11.240265  12.11633  ]\n",
      "Reset environment\n",
      "Episode reward: 1743.891947582364\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.120646 12.111192 12.081341 11.168217 11.240894 12.117055]\n",
      "Reset environment\n",
      "Episode reward: 2378.3999307751656\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.122015  12.112566  12.082705  11.169713  11.242133  12.1184225]\n",
      "Reset environment\n",
      "Episode reward: 1358.7079366445541\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.12314  12.113689 12.083834 11.170957 11.243124 12.119546]\n",
      "Reset environment\n",
      "Episode reward: 3306.602823138237\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.125133 12.11569  12.085817 11.173115 11.244883 12.121542]\n",
      "Reset environment\n",
      "Episode reward: 5711.535601913929\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.128885  12.119425  12.089577  11.177113  11.248243  12.1252985]\n",
      "Reset environment\n",
      "Episode reward: 5723.49642688036\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.132457 12.123004 12.093149 11.180954 11.251423 12.128872]\n",
      "Reset environment\n",
      "Episode reward: 1400.5964446663857\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.133213 12.12376  12.093905 11.181792 11.252102 12.129627]\n",
      "Reset environment\n",
      "Episode reward: 1123.3325097560883\n",
      "Total Steps: 37\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.13374  12.124282 12.09444  11.182403 11.25256  12.130156]\n",
      "Reset environment\n",
      "Episode reward: 4703.8580649495125\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.136757 12.127284 12.097457 11.185623 11.255255 12.133175]\n",
      "Reset environment\n",
      "Episode reward: 2037.5323648899794\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.137835  12.128339  12.098558  11.1868105 11.256222  12.134254 ]\n",
      "Reset environment\n",
      "Episode reward: 1795.435599565506\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.138837  12.129335  12.0995655 11.187914  11.257129  12.135255 ]\n",
      "Reset environment\n",
      "Episode reward: 2487.917727291584\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.14031  12.130806 12.101033 11.189521 11.258463 12.136726]\n",
      "Reset environment\n",
      "Episode reward: 2902.076972603798\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.142058  12.132579  12.102747  11.191419  11.260048  12.1384735]\n",
      "Reset environment\n",
      "Episode reward: 4011.8586042523384\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.144574  12.1350765 12.105274  11.194106  11.262308  12.140988 ]\n",
      "Reset environment\n",
      "Episode reward: 2477.183231949806\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.146052 12.136539 12.106765 11.195715 11.263615 12.142465]\n",
      "Reset environment\n",
      "Episode reward: 4771.737119317055\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.149115 12.139596 12.10982  11.199    11.266369 12.14553 ]\n",
      "Reset environment\n",
      "Episode reward: 3433.9038612246513\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.151224 12.14171  12.111925 11.20127  11.268267 12.147637]\n",
      "Reset environment\n",
      "Episode reward: 2151.9334408044815\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.152918  12.1434    12.1136265 11.203112  11.26976   12.149335 ]\n",
      "Reset environment\n",
      "Episode reward: 3043.9814024567604\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.154662 12.145186 12.115311 11.205028 11.271326 12.151075]\n",
      "Reset environment\n",
      "Episode reward: 1367.4010422229767\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.15579   12.146316  12.116438  11.206283  11.272306  12.1522045]\n",
      "Reset environment\n",
      "Episode reward: -686.3083860874176\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.154046 12.144574 12.114681 11.204344 11.270711 12.150457]\n",
      "Reset environment\n",
      "Episode reward: 2291.216297149658\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.1552925 12.145863  12.115909  11.205743  11.271833  12.151705 ]\n",
      "Reset environment\n",
      "Episode reward: 909.656268119812\n",
      "Total Steps: 29\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.155679 12.146261 12.116282 11.206196 11.272179 12.152093]\n",
      "Reset environment\n",
      "Episode reward: 5041.69035422802\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.158909  12.1494875 12.119509  11.209656  11.275058  12.155321 ]\n",
      "Reset environment\n",
      "Episode reward: 3239.0996594429016\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.160881 12.151446 12.121483 11.211789 11.276798 12.157295]\n",
      "Reset environment\n",
      "Episode reward: 3265.207496792078\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.162887 12.153469 12.123462 11.213956 11.278628 12.159298]\n",
      "Reset environment\n",
      "Episode reward: 3735.10194003582\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.165177  12.155784  12.125732  11.216419  11.280691  12.1615925]\n",
      "Reset environment\n",
      "Episode reward: 2425.8878609985113\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.166544 12.157126 12.127121 11.217911 11.281916 12.162962]\n",
      "Reset environment\n",
      "Episode reward: 3139.851417183876\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.168411 12.159    12.128985 11.219955 11.283612 12.164834]\n",
      "Reset environment\n",
      "Episode reward: 2015.511646449566\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.1695595 12.160151  12.130135  11.221215  11.284627  12.165982 ]\n",
      "Reset environment\n",
      "Episode reward: 1436.8070118427277\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.1703005 12.160919  12.130848  11.222045  11.285296  12.166722 ]\n",
      "Reset environment\n",
      "Episode reward: 2528.258385416586\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.171699 12.162282 12.132274 11.223566 11.286546 12.168121]\n",
      "Reset environment\n",
      "Episode reward: 1524.579941034317\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.172477  12.163043  12.133079  11.224438  11.287245  12.1689005]\n",
      "Reset environment\n",
      "Episode reward: 2213.3371744155884\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.174206 12.164784 12.134802 11.226322 11.288822 12.170631]\n",
      "Reset environment\n",
      "Episode reward: -687.1291098594666\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.172437 12.163022 12.133024 11.224355 11.287224 12.16887 ]\n",
      "Reset environment\n",
      "Episode reward: 1976.3608687669039\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.173436 12.163989 12.134049 11.225456 11.288104 12.169869]\n",
      "Reset environment\n",
      "Episode reward: 2129.842359483242\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.1751   12.16566  12.135714 11.227271 11.289607 12.171534]\n",
      "Reset environment\n",
      "Episode reward: 4558.670711040497\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.178002 12.168542 12.138628 11.230384 11.292235 12.17444 ]\n",
      "Reset environment\n",
      "Episode reward: 1999.5260780453682\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.179093 12.169636 12.139709 11.231583 11.293199 12.175533]\n",
      "Reset environment\n",
      "Episode reward: 98.48064374923706\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.177959  12.168368  12.13872   11.2303505 11.292237  12.174408 ]\n",
      "Reset environment\n",
      "Episode reward: 1977.672925889492\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.179082  12.169476  12.139853  11.231579  11.293228  12.1755295]\n",
      "Reset environment\n",
      "Episode reward: 1127.3251297473907\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.179638  12.170031  12.140409  11.232209  11.29373   12.1760845]\n",
      "Reset environment\n",
      "Episode reward: -706.6573084592819\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.177899 12.168269 12.138699 11.230459 11.292143 12.174355]\n",
      "Reset environment\n",
      "Episode reward: 5008.99376155436\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.181076 12.171472 12.141823 11.233883 11.295037 12.177518]\n",
      "Reset environment\n",
      "Episode reward: 3723.819001197815\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.183366 12.173765 12.144102 11.236346 11.297074 12.179806]\n",
      "Reset environment\n",
      "Episode reward: 3032.470787703991\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.185172 12.17558  12.145909 11.238306 11.298704 12.181615]\n",
      "Reset environment\n",
      "Episode reward: 2688.938600823283\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.186637 12.177026 12.147405 11.239909 11.300003 12.183081]\n",
      "Reset environment\n",
      "Episode reward: 1380.490027666092\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.187135 12.177577 12.147857 11.240511 11.300442 12.183585]\n",
      "Reset environment\n",
      "Episode reward: 1664.9998281002045\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.187956  12.178443  12.148639  11.2414465 11.301193  12.184407 ]\n",
      "Reset environment\n",
      "Episode reward: 5609.891146600246\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.191574 12.182058 12.152242 11.245312 11.304458 12.188026]\n",
      "Reset environment\n",
      "Episode reward: 1532.0437725782394\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.192374 12.182866 12.153036 11.246201 11.30518  12.188828]\n",
      "Reset environment\n",
      "Episode reward: 3071.6284695267677\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.194215 12.184697 12.154884 11.24819  11.306824 12.190668]\n",
      "Reset environment\n",
      "Episode reward: 1991.120096564293\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.195289 12.185757 12.155968 11.249376 11.307759 12.191741]\n",
      "Reset environment\n",
      "Episode reward: 4443.112056076527\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.198069 12.188523 12.158757 11.252363 11.310239 12.19452 ]\n",
      "Reset environment\n",
      "Episode reward: 1386.6261237859726\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.199228 12.189684 12.159919 11.253635 11.311269 12.19568 ]\n",
      "Reset environment\n",
      "Episode reward: 857.0423986911774\n",
      "Total Steps: 30\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.199536 12.189966 12.160246 11.254007 11.311536 12.195989]\n",
      "Reset environment\n",
      "Episode reward: 1941.1246798671782\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.199682 12.19015  12.160361 11.254279 11.311591 12.196141]\n",
      "Reset environment\n",
      "Episode reward: 3213.551806986332\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.20159  12.192063 12.162256 11.256351 11.313273 12.198048]\n",
      "Reset environment\n",
      "Episode reward: 537.828607082367\n",
      "Total Steps: 17\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.201714 12.192188 12.162384 11.25653  11.31337  12.198172]\n",
      "Reset environment\n",
      "Episode reward: 3501.5956345275044\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.203688 12.194121 12.164391 11.258693 11.315125 12.200146]\n",
      "Reset environment\n",
      "Episode reward: 1972.6289194226265\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.20473  12.195166 12.165443 11.259851 11.316052 12.201189]\n",
      "Reset environment\n",
      "Episode reward: 4726.610565721989\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.20766   12.198073  12.168391  11.263001  11.3187065 12.20412  ]\n",
      "Reset environment\n",
      "Episode reward: 1386.7105815410614\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.2088175 12.199229  12.169548  11.264269  11.319724  12.205277 ]\n",
      "Reset environment\n",
      "Episode reward: 1314.7711752057076\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.209874 12.200289 12.170607 11.265452 11.320667 12.206335]\n",
      "Reset environment\n",
      "Episode reward: 4742.06731492281\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.212809 12.203201 12.173567 11.268635 11.323337 12.209274]\n",
      "Reset environment\n",
      "Episode reward: 2177.2830939888954\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.214494  12.204877  12.1752615 11.270471  11.32483   12.210961 ]\n",
      "Reset environment\n",
      "Episode reward: 2116.374473154545\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.215649 12.206024 12.176423 11.271742 11.325852 12.212116]\n",
      "Reset environment\n",
      "Episode reward: 2234.2885223925114\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.216819  12.2071705 12.17762   11.273033  11.3269    12.213288 ]\n",
      "Reset environment\n",
      "Episode reward: 326.65569722652435\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.216348 12.206685 12.177153 11.27246  11.326507 12.212818]\n",
      "Reset environment\n",
      "Episode reward: 3245.0357663184404\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.218267 12.208567 12.179098 11.274519 11.328246 12.214733]\n",
      "Reset environment\n",
      "Episode reward: 5543.913642168045\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.221811 12.21211  12.182638 11.278289 11.331435 12.218278]\n",
      "Reset environment\n",
      "Episode reward: 2307.19137686491\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.22363  12.213931 12.184462 11.280264 11.333068 12.2201  ]\n",
      "Reset environment\n",
      "Episode reward: 2726.8348639011383\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.225178 12.215466 12.18603  11.281953 11.334463 12.221649]\n",
      "Reset environment\n",
      "Episode reward: 1361.472208559513\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.225874  12.2161665 12.186721  11.28273   11.3351    12.222345 ]\n",
      "Reset environment\n",
      "Episode reward: 3832.7029851675034\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.2282295 12.218524  12.189071  11.285275  11.337206  12.224703 ]\n",
      "Reset environment\n",
      "Episode reward: 2341.8604194521904\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.229566 12.219856 12.190406 11.286739 11.338413 12.22604 ]\n",
      "Reset environment\n",
      "Episode reward: 6853.94261598587\n",
      "Total Steps: 234\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.233803 12.224088 12.194639 11.29131  11.342265 12.230274]\n",
      "Reset environment\n",
      "Episode reward: 2978.495410144329\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.235483 12.225793 12.196305 11.293168 11.343866 12.231958]\n",
      "Reset environment\n",
      "Episode reward: 1029.2161058187485\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.235543 12.225909 12.196307 11.293095 11.343965 12.232018]\n",
      "Reset environment\n",
      "Episode reward: 1794.7049174308777\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.236546  12.2269125 12.197312  11.294201  11.34488   12.233022 ]\n",
      "Reset environment\n",
      "Episode reward: 3139.6345854512183\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.238468 12.228883 12.199207 11.296404 11.346689 12.23495 ]\n",
      "Reset environment\n",
      "Episode reward: 1805.9409890174866\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.239307 12.229768 12.199999 11.297355 11.34743  12.235789]\n",
      "Reset environment\n",
      "Episode reward: 1833.037093937397\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.240768 12.231225 12.201464 11.298949 11.3487   12.237249]\n",
      "Reset environment\n",
      "Episode reward: 1718.681010901928\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.241698 12.232129 12.202406 11.29997  11.349525 12.238179]\n",
      "Reset environment\n",
      "Episode reward: 1837.3973834067583\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.242453 12.232832 12.20321  11.300842 11.35022  12.238937]\n",
      "Reset environment\n",
      "Episode reward: 5503.824090242386\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.245963 12.236325 12.206729 11.304592 11.35336  12.24245 ]\n",
      "Reset environment\n",
      "Episode reward: 5453.747578799725\n",
      "Total Steps: 187\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.249414 12.239753 12.21019  11.308299 11.356442 12.2459  ]\n",
      "Reset environment\n",
      "Episode reward: 3625.4528564214706\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.251621 12.241963 12.212392 11.310689 11.358434 12.248113]\n",
      "Reset environment\n",
      "Episode reward: 2495.363867044449\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.253059 12.243391 12.213842 11.312262 11.3597   12.249552]\n",
      "Reset environment\n",
      "Episode reward: 2604.223677814007\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.254553 12.244891 12.215329 11.313882 11.361048 12.251045]\n",
      "Reset environment\n",
      "Episode reward: 3019.1862030923367\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.256195 12.246574 12.216903 11.31569  11.362522 12.252692]\n",
      "Reset environment\n",
      "Episode reward: 2148.637869954109\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.257365  12.247743  12.2180805 11.316976  11.363566  12.253862 ]\n",
      "Reset environment\n",
      "Episode reward: 3250.774316072464\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.259202 12.249615 12.219898 11.319038 11.365298 12.255704]\n",
      "Reset environment\n",
      "Episode reward: -221.8857545852661\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.257605 12.248165 12.218181 11.31735  11.363865 12.254117]\n",
      "Reset environment\n",
      "Episode reward: 2882.660718679428\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.25925  12.249803 12.21984  11.319149 11.365345 12.255759]\n",
      "Reset environment\n",
      "Episode reward: 4069.7964556217194\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.261739 12.252285 12.222331 11.321826 11.367568 12.258248]\n",
      "Reset environment\n",
      "Episode reward: 3937.313036799431\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.264152 12.254693 12.22475  11.324417 11.369727 12.260664]\n",
      "Reset environment\n",
      "Episode reward: 2275.6471878290176\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.2658825 12.256428  12.2264805 11.326302  11.37129   12.262397 ]\n",
      "Reset environment\n",
      "Episode reward: 1395.5258281230927\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.266411 12.257012 12.226954 11.326932 11.371758 12.262929]\n",
      "Reset environment\n",
      "Episode reward: 3151.830311894417\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.268291 12.258887 12.228845 11.328953 11.373437 12.264809]\n",
      "Reset environment\n",
      "Episode reward: 1754.8533269762993\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.269045  12.259588  12.229649  11.329809  11.3741255 12.265562 ]\n",
      "Reset environment\n",
      "Episode reward: 2599.436900675297\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.27053   12.261065  12.231139  11.331431  11.3754425 12.267047 ]\n",
      "Reset environment\n",
      "Episode reward: 322.2724828720093\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.269911  12.260487  12.230471  11.3305645 11.3749275 12.266436 ]\n",
      "Reset environment\n",
      "Episode reward: 2505.6536229476333\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.271281 12.261835 12.231867 11.332059 11.37615  12.267807]\n",
      "Reset environment\n",
      "Episode reward: 3960.7495018541813\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.2736225 12.264157  12.23421   11.334615  11.378252  12.270149 ]\n",
      "Reset environment\n",
      "Episode reward: 2713.170966863632\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.275175 12.2657   12.235779 11.336303 11.379654 12.271703]\n",
      "Reset environment\n",
      "Episode reward: 3389.9634978175163\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.277211 12.267731 12.237822 11.338497 11.381478 12.273739]\n",
      "Reset environment\n",
      "Episode reward: 5912.570613324642\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.281015  12.271516  12.24162   11.3425665 11.384897  12.277543 ]\n",
      "Reset environment\n",
      "Episode reward: 1518.3404453992844\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.281393  12.27194   12.241961  11.342947  11.385258  12.2779255]\n",
      "Reset environment\n",
      "Episode reward: 1336.6499780416489\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.281741 12.272339 12.242255 11.343403 11.38552  12.278278]\n",
      "Reset environment\n",
      "Episode reward: 2154.990060508251\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.283413 12.274004 12.243932 11.345212 11.387006 12.279949]\n",
      "Reset environment\n",
      "Episode reward: 5349.8420723080635\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.286831  12.27741   12.247351  11.348857  11.3900795 12.283369 ]\n",
      "Reset environment\n",
      "Episode reward: 3818.5488807559013\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.28915   12.279734  12.249664  11.3513565 11.392167  12.285689 ]\n",
      "Reset environment\n",
      "Episode reward: 1960.381516456604\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.290022 12.280557 12.250581 11.352353 11.392943 12.286557]\n",
      "Reset environment\n",
      "Episode reward: 5181.060826659203\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.293239  12.283763  12.2538185 11.355834  11.395858  12.2897835]\n",
      "Reset environment\n",
      "Episode reward: 3559.6183858960867\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.295156 12.285625 12.255786 11.357933 11.397577 12.291699]\n",
      "Reset environment\n",
      "Episode reward: 2864.360160589218\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.296832 12.287284 12.257471 11.359744 11.399083 12.293376]\n",
      "Reset environment\n",
      "Episode reward: 1825.5365486741066\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.297817  12.288256  12.25847   11.3608265 11.399964  12.294358 ]\n",
      "Reset environment\n",
      "Episode reward: 2209.880661532283\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.298981 12.289394 12.259653 11.362104 11.400994 12.295525]\n",
      "Reset environment\n",
      "Episode reward: 2662.7992563638836\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.300481  12.290865  12.261175  11.3637295 11.402333  12.297025 ]\n",
      "Reset environment\n",
      "Episode reward: 2611.0805662572384\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.30181  12.292227 12.262466 11.365209 11.403513 12.298355]\n",
      "Reset environment\n",
      "Episode reward: 2820.6792175620794\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.303389 12.293787 12.264066 11.366927 11.404943 12.299935]\n",
      "Reset environment\n",
      "Episode reward: 4823.143272638321\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.306416  12.296796  12.267099  11.3701725 11.407612  12.302958 ]\n",
      "Reset environment\n",
      "Episode reward: 2409.5364702939987\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.307768 12.298139 12.268464 11.371651 11.408795 12.304313]\n",
      "Reset environment\n",
      "Episode reward: 2793.548372335732\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.309312 12.299719 12.269943 11.373349 11.410169 12.305857]\n",
      "Reset environment\n",
      "Episode reward: 1371.2575348615646\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.310431 12.300839 12.271063 11.374583 11.411163 12.306975]\n",
      "Reset environment\n",
      "Episode reward: 1807.5783420205116\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.311359 12.301771 12.271986 11.375619 11.411959 12.307904]\n",
      "Reset environment\n",
      "Episode reward: 1886.0660408735275\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.311906 12.30223  12.272616 11.376328 11.412442 12.30845 ]\n",
      "Reset environment\n",
      "Episode reward: 1419.0495599508286\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.3130665 12.303392  12.273777  11.377606  11.413451  12.30961  ]\n",
      "Reset environment\n",
      "Episode reward: 1456.5926522016525\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.31382  12.304147 12.274531 11.378443 11.414128 12.310367]\n",
      "Reset environment\n",
      "Episode reward: 1735.6933354139328\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.314731 12.305061 12.275438 11.379454 11.414961 12.311276]\n",
      "Reset environment\n",
      "Episode reward: 1204.211820140481\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.314855 12.305116 12.275638 11.379509 11.415141 12.311405]\n",
      "Reset environment\n",
      "Episode reward: 4800.525380551815\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.317864  12.308121  12.2786455 11.382722  11.417861  12.314415 ]\n",
      "Reset environment\n",
      "Episode reward: 2470.0564594864845\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.319169  12.309426  12.279951  11.384158  11.419011  12.3157215]\n",
      "Reset environment\n",
      "Episode reward: 3442.0789702534676\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.321136  12.311365  12.2819395 11.386287  11.420781  12.317683 ]\n",
      "Reset environment\n",
      "Episode reward: 4437.660374164581\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.323908 12.314142 12.284708 11.389252 11.423285 12.320457]\n",
      "Reset environment\n",
      "Episode reward: -76.5051796734333\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.322899 12.313068 12.283774 11.388177 11.422365 12.319453]\n",
      "Reset environment\n",
      "Episode reward: 4078.230678796768\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.325396 12.315563 12.286257 11.390856 11.424611 12.321947]\n",
      "Reset environment\n",
      "Episode reward: 2537.451069355011\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.32685  12.316994 12.28772  11.392432 11.425921 12.3234  ]\n",
      "Reset environment\n",
      "Episode reward: 1760.0393901765347\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.327765 12.31788  12.288659 11.39344  11.426741 12.324315]\n",
      "Reset environment\n",
      "Episode reward: 1540.7611846923828\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.328562 12.318674 12.289457 11.394324 11.427447 12.325111]\n",
      "Reset environment\n",
      "Episode reward: 1862.5205329209566\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.32952   12.319602  12.290441  11.395372  11.4283085 12.326068 ]\n",
      "Reset environment\n",
      "Episode reward: 1705.7817772328854\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.330246 12.320361 12.291126 11.396102 11.429042 12.326792]\n",
      "Reset environment\n",
      "Episode reward: 3314.243789434433\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.332212 12.322318 12.293092 11.398236 11.43081  12.328758]\n",
      "Reset environment\n",
      "Episode reward: 1589.8384526688606\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.332956  12.3230295 12.293866  11.399069  11.431481  12.329502 ]\n",
      "Reset environment\n",
      "Episode reward: 3475.577000245452\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.334999 12.325111 12.295872 11.401285 11.433324 12.331542]\n",
      "Reset environment\n",
      "Episode reward: 3802.0916095376015\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.337287 12.327416 12.298127 11.403728 11.435373 12.333829]\n",
      "Reset environment\n",
      "Episode reward: 2498.6767060756683\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.338689 12.328808 12.299536 11.405258 11.436616 12.33523 ]\n",
      "Reset environment\n",
      "Episode reward: 4400.580323040485\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.341399 12.33152  12.302225 11.408159 11.439031 12.337937]\n",
      "Reset environment\n",
      "Episode reward: 1130.711926460266\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.341943 12.332068 12.302769 11.408777 11.439526 12.338481]\n",
      "Reset environment\n",
      "Episode reward: 2034.5507768392563\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.342994  12.3331    12.3038435 11.409931  11.440459  12.33953  ]\n",
      "Reset environment\n",
      "Episode reward: 1175.0220379829407\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.343317  12.333448  12.304132  11.410181  11.4407835 12.339854 ]\n",
      "Reset environment\n",
      "Episode reward: 1486.0467417240143\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.344072 12.334192 12.304907 11.411024 11.441455 12.340612]\n",
      "Reset environment\n",
      "Episode reward: 2100.7425555586815\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.345691 12.335807 12.306525 11.412773 11.442888 12.342231]\n",
      "Reset environment\n",
      "Episode reward: 2392.7333332896233\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.346954  12.3370695 12.307796  11.414156  11.444004  12.343495 ]\n",
      "Reset environment\n",
      "Episode reward: 3155.207001313567\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.348609 12.338764 12.309452 11.416059 11.44558  12.345157]\n",
      "Reset environment\n",
      "Episode reward: 5761.730652928352\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.352278  12.342435  12.3131075 11.4199505 11.448902  12.348826 ]\n",
      "Reset environment\n",
      "Episode reward: 3546.4748284220695\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.354437  12.344603  12.31524   11.422279  11.450917  12.3509865]\n",
      "Reset environment\n",
      "Episode reward: 650.0351849794388\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.353984  12.344238  12.3147135 11.4216385 11.450525  12.350539 ]\n",
      "Reset environment\n",
      "Episode reward: 4307.72063857317\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.356627 12.346867 12.317357 11.424472 11.452872 12.353181]\n",
      "Reset environment\n",
      "Episode reward: 2558.2312174513936\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.358029 12.348301 12.318715 11.426003 11.454164 12.354584]\n",
      "Reset environment\n",
      "Episode reward: 1888.5456067919731\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.359038  12.34929   12.319735  11.4271145 11.455054  12.355591 ]\n",
      "Reset environment\n",
      "Episode reward: 2016.6664828658104\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.360097 12.35035  12.320792 11.428298 11.455984 12.356649]\n",
      "Reset environment\n",
      "Episode reward: 3888.8186418414116\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.362443 12.352691 12.323143 11.430818 11.458075 12.359001]\n",
      "Reset environment\n",
      "Episode reward: 5644.206618249416\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.366013 12.356259 12.3267   11.434623 11.461292 12.362574]\n",
      "Reset environment\n",
      "Episode reward: 1640.7870030403137\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.366836 12.357059 12.327546 11.43554  11.462035 12.363396]\n",
      "Reset environment\n",
      "Episode reward: 1681.0050690174103\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.367702 12.35792  12.328423 11.436507 11.462797 12.364263]\n",
      "Reset environment\n",
      "Episode reward: 3267.101264834404\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.369624 12.359852 12.330334 11.43858  11.464522 12.366187]\n",
      "Reset environment\n",
      "Episode reward: 4515.110100090504\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.372404 12.362627 12.333123 11.441556 11.467011 12.368966]\n",
      "Reset environment\n",
      "Episode reward: 2380.7331555485725\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.373715 12.363931 12.334447 11.442989 11.468166 12.370279]\n",
      "Reset environment\n",
      "Episode reward: 3069.2330905646086\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.37545  12.365641 12.336207 11.444866 11.469711 12.372014]\n",
      "Reset environment\n",
      "Episode reward: 3125.571331858635\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.377284 12.367484 12.338026 11.446848 11.47135  12.373845]\n",
      "Reset environment\n",
      "Episode reward: 2469.5994968414307\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.378664 12.368867 12.339408 11.448356 11.472595 12.375231]\n",
      "Reset environment\n",
      "Episode reward: 2975.0066661834717\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.380393 12.37059  12.341142 11.450218 11.474143 12.376959]\n",
      "Reset environment\n",
      "Episode reward: -519.1614220142365\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.378951  12.369167  12.339676  11.44845   11.472903  12.3755245]\n",
      "Reset environment\n",
      "Episode reward: 2184.6737181544304\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.380608 12.370826 12.341329 11.450254 11.47441  12.377186]\n",
      "Reset environment\n",
      "Episode reward: 227.87055230140686\n",
      "Total Steps: 4\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.380573  12.370793  12.341295  11.450236  11.474371  12.3771515]\n",
      "Reset environment\n",
      "Episode reward: 3174.1766653060913\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.382425 12.372652 12.343145 11.452252 11.476047 12.379009]\n",
      "Reset environment\n",
      "Episode reward: 2178.893097281456\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.384088 12.374319 12.344805 11.454066 11.477549 12.38067 ]\n",
      "Reset environment\n",
      "Episode reward: 4913.613139986992\n",
      "Total Steps: 169\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.387112 12.377342 12.347823 11.457297 11.480268 12.383696]\n",
      "Reset environment\n",
      "Episode reward: 1378.0103280842304\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.387221 12.377513 12.347883 11.457306 11.480351 12.383818]\n",
      "Reset environment\n",
      "Episode reward: 1412.8420104980469\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.388363 12.378655 12.349028 11.458558 11.481345 12.384958]\n",
      "Reset environment\n",
      "Episode reward: 2006.3844087589532\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.389062  12.379394  12.3496895 11.459294  11.481947  12.385658 ]\n",
      "Reset environment\n",
      "Episode reward: 2127.605594485998\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.390209  12.380528  12.350856  11.460548  11.482974  12.3868065]\n",
      "Reset environment\n",
      "Episode reward: 3056.860744982958\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.391994 12.382323 12.352622 11.462497 11.484594 12.388594]\n",
      "Reset environment\n",
      "Episode reward: 784.231004357338\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.391589 12.381834 12.352315 11.462004 11.484291 12.3882  ]\n",
      "Reset environment\n",
      "Episode reward: 2360.274717196822\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.392822 12.383045 12.353576 11.463354 11.485395 12.389434]\n",
      "Reset environment\n",
      "Episode reward: 5925.462306499481\n",
      "Total Steps: 200\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.396569 12.386774 12.357332 11.467359 11.488767 12.393182]\n",
      "Reset environment\n",
      "Episode reward: 5913.022476255894\n",
      "Total Steps: 200\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.400311 12.390479 12.361073 11.471358 11.492099 12.396918]\n",
      "Reset environment\n",
      "Episode reward: 6340.5789459347725\n",
      "Total Steps: 214\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.40433  12.394481 12.365093 11.475646 11.495736 12.40094 ]\n",
      "Reset environment\n",
      "Episode reward: 4556.289650022984\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.407112 12.397246 12.367886 11.478624 11.498226 12.403719]\n",
      "Reset environment\n",
      "Episode reward: 2132.6787945330143\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.408079 12.398166 12.368889 11.479715 11.499084 12.404682]\n",
      "Reset environment\n",
      "Episode reward: 2572.588042616844\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.4095335 12.399638  12.370325  11.481306  11.500415  12.406136 ]\n",
      "Reset environment\n",
      "Episode reward: 1778.7226573377848\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.410219 12.400365 12.370964 11.482094 11.50101  12.406824]\n",
      "Reset environment\n",
      "Episode reward: 175.79636216163635\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.409133  12.399149  12.370023  11.480954  11.500045  12.4057455]\n",
      "Reset environment\n",
      "Episode reward: 3205.830189138651\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.410967 12.401004 12.371833 11.482959 11.501677 12.407579]\n",
      "Reset environment\n",
      "Episode reward: 2557.3616625368595\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.412273 12.402277 12.37317  11.484393 11.502844 12.408887]\n",
      "Reset environment\n",
      "Episode reward: 1860.1755521893501\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.413692 12.403699 12.374596 11.485958 11.504089 12.410309]\n",
      "Reset environment\n",
      "Episode reward: 2158.1296870708466\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.414866 12.404865 12.375779 11.487252 11.505129 12.41148 ]\n",
      "Reset environment\n",
      "Episode reward: -542.9337936043739\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.412955 12.403117 12.373716 11.485233 11.50336  12.409585]\n",
      "Reset environment\n",
      "Episode reward: -688.3029419183731\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.411211 12.401382 12.371954 11.483303 11.501763 12.407836]\n",
      "Reset environment\n",
      "Episode reward: 2477.6868720911443\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.412319  12.402525  12.373003  11.4845295 11.502736  12.408948 ]\n",
      "Reset environment\n",
      "Episode reward: 2324.7316962257028\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.413471 12.403654 12.374191 11.485806 11.503765 12.410101]\n",
      "Reset environment\n",
      "Episode reward: 1487.22007265687\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.4142    12.404361  12.37494   11.486618  11.504409  12.4108305]\n",
      "Reset environment\n",
      "Episode reward: 2328.615785777569\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.415957 12.406113 12.3767   11.488515 11.505977 12.412588]\n",
      "Reset environment\n",
      "Episode reward: 719.3448766469955\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.415351 12.405415 12.376177 11.487832 11.505452 12.411988]\n",
      "Reset environment\n",
      "Episode reward: 1961.3302999138832\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.416829 12.406892 12.377657 11.489459 11.506774 12.413468]\n",
      "Reset environment\n",
      "Episode reward: 1875.0400641560555\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.417796 12.407868 12.378625 11.49053  11.507654 12.414435]\n",
      "Reset environment\n",
      "Episode reward: 3624.3452894687653\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.419944  12.410027  12.38076   11.492846  11.5095825 12.416583 ]\n",
      "Reset environment\n",
      "Episode reward: 2494.7178471833467\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.421078  12.411108  12.381943  11.494028  11.510648  12.4177265]\n",
      "Reset environment\n",
      "Episode reward: 2074.191134572029\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.422191 12.412216 12.38306  11.495264 11.511642 12.418839]\n",
      "Reset environment\n",
      "Episode reward: 2423.2238992750645\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.423504 12.413514 12.384391 11.496694 11.512826 12.420153]\n",
      "Reset environment\n",
      "Episode reward: 1767.1336516737938\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.424433  12.414433  12.385332  11.497714  11.5136595 12.421082 ]\n",
      "Reset environment\n",
      "Episode reward: 3108.569534122944\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.426187 12.416178 12.387103 11.499624 11.515212 12.422837]\n",
      "Reset environment\n",
      "Episode reward: 3181.036122560501\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.428018 12.417995 12.388951 11.501617 11.516855 12.424668]\n",
      "Reset environment\n",
      "Episode reward: 2177.995449900627\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.429654 12.419628 12.390594 11.503388 11.518305 12.426306]\n",
      "Reset environment\n",
      "Episode reward: 1338.382074356079\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.430305 12.420265 12.391252 11.504118 11.518874 12.426957]\n",
      "Reset environment\n",
      "Episode reward: 2010.536123752594\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.431832 12.421797 12.392778 11.505786 11.520258 12.428485]\n",
      "Reset environment\n",
      "Episode reward: 1598.57399263978\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.432322 12.422336 12.393223 11.506247 11.520712 12.428979]\n",
      "Reset environment\n",
      "Episode reward: 2097.487364411354\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.433449  12.4234705 12.394344  11.507481  11.521713  12.430108 ]\n",
      "Reset environment\n",
      "Episode reward: 327.105268239975\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.432533  12.4224205 12.393566  11.506519  11.520902  12.429197 ]\n",
      "Reset environment\n",
      "Episode reward: 1676.1595793366432\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.433417 12.423311 12.394444 11.507505 11.52171  12.430082]\n",
      "Reset environment\n",
      "Episode reward: 2136.270370721817\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.434545 12.424436 12.395576 11.50875  11.522687 12.43121 ]\n",
      "Reset environment\n",
      "Episode reward: 5403.560296356678\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.437919  12.427814  12.3989525 11.512347  11.52574   12.434586 ]\n",
      "Reset environment\n",
      "Episode reward: -125.54440116882324\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.436488 12.426503 12.397419 11.510849 11.524461 12.433165]\n",
      "Reset environment\n",
      "Episode reward: 1728.6480528712273\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.437357 12.427359 12.398303 11.511822 11.525233 12.434032]\n",
      "Reset environment\n",
      "Episode reward: 1360.349701166153\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.438443 12.428448 12.399391 11.513023 11.52618  12.435122]\n",
      "Reset environment\n",
      "Episode reward: 2424.4519339203835\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.440249  12.430258  12.401195  11.514996  11.5278015 12.436927 ]\n",
      "Reset environment\n",
      "Episode reward: 2481.133304834366\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.441623 12.431637 12.402566 11.516505 11.529032 12.438301]\n",
      "Reset environment\n",
      "Episode reward: 2284.9595192819834\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.442819  12.43287   12.403708  11.5178385 11.5301485 12.439492 ]\n",
      "Reset environment\n",
      "Episode reward: 1797.2612090967596\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.443701  12.4337225 12.404617  11.518808  11.530931  12.440373 ]\n",
      "Reset environment\n",
      "Episode reward: 4737.214888989925\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.446604  12.436631  12.407508  11.521914  11.533515  12.4432745]\n",
      "Reset environment\n",
      "Episode reward: 2389.005418777466\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.447944 12.437964 12.408858 11.523377 11.534706 12.444615]\n",
      "Reset environment\n",
      "Episode reward: 5343.803608357906\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.451595  12.441599  12.4124975 11.527273  11.53795   12.448272 ]\n",
      "Reset environment\n",
      "Episode reward: 5147.144865691662\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.455303 12.445287 12.416202 11.531226 11.541264 12.45198 ]\n",
      "Reset environment\n",
      "Episode reward: 5305.310806453228\n",
      "Total Steps: 181\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.458563 12.448533 12.419472 11.534721 11.544176 12.455247]\n",
      "Reset environment\n",
      "Episode reward: 2130.0948864221573\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.45972  12.449696 12.420624 11.535987 11.545228 12.456404]\n",
      "Reset environment\n",
      "Episode reward: 4109.250222027302\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.462065 12.452046 12.422969 11.538513 11.547302 12.458751]\n",
      "Reset environment\n",
      "Episode reward: 3938.8108569979668\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.464276 12.454263 12.425187 11.5409   11.549283 12.46097 ]\n",
      "Reset environment\n",
      "Episode reward: 5580.099592387676\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.467729  12.4576845 12.428654  11.544593  11.552374  12.464428 ]\n",
      "Reset environment\n",
      "Episode reward: 1795.5726398825645\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.468661 12.458612 12.429589 11.545623 11.553196 12.465358]\n",
      "Reset environment\n",
      "Episode reward: 3324.0134286880493\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.470598  12.460558  12.4315195 11.547717  11.554946  12.467299 ]\n",
      "Reset environment\n",
      "Episode reward: 2103.6741665005684\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.471707  12.461663  12.432632  11.548936  11.555934  12.4684105]\n",
      "Reset environment\n",
      "Episode reward: -340.90000879764557\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.470397 12.46035  12.431326 11.547667 11.554726 12.467104]\n",
      "Reset environment\n",
      "Episode reward: 2462.4856385588646\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.4717455 12.461703  12.432675  11.549147  11.555944  12.468452 ]\n",
      "Reset environment\n",
      "Episode reward: 2579.8606840372086\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.473157 12.463103 12.434102 11.550684 11.557183 12.469864]\n",
      "Reset environment\n",
      "Episode reward: 1649.6530622243881\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.474435 12.464383 12.435381 11.552087 11.558315 12.471142]\n",
      "Reset environment\n",
      "Episode reward: 2059.1771229058504\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.47552  12.465511 12.436393 11.553233 11.559346 12.472222]\n",
      "Reset environment\n",
      "Episode reward: 3690.685772329569\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.477647 12.467662 12.438513 11.555542 11.561274 12.474348]\n",
      "Reset environment\n",
      "Episode reward: 1652.0566362142563\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.4784775 12.468501  12.439339  11.556468  11.562032  12.47518  ]\n",
      "Reset environment\n",
      "Episode reward: 5667.900367796421\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.481941 12.471964 12.442792 11.560181 11.565149 12.478645]\n",
      "Reset environment\n",
      "Episode reward: 2711.4463632702827\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.483354 12.473338 12.444237 11.561723 11.5664   12.480059]\n",
      "Reset environment\n",
      "Episode reward: 2997.894989222288\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.484909 12.474914 12.445749 11.563438 11.567785 12.48161 ]\n",
      "Reset environment\n",
      "Episode reward: 2464.89455974102\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.486246 12.476263 12.447083 11.564897 11.568973 12.482953]\n",
      "Reset environment\n",
      "Episode reward: 1675.2054634094238\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.487123 12.477131 12.447962 11.565863 11.56975  12.483829]\n",
      "Reset environment\n",
      "Episode reward: 6102.58792245388\n",
      "Total Steps: 207\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.490946 12.480947 12.451773 11.569922 11.573213 12.487654]\n",
      "Reset environment\n",
      "Episode reward: 1917.5692479610443\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.492396 12.482395 12.453229 11.571508 11.574498 12.489103]\n",
      "Reset environment\n",
      "Episode reward: 3962.8460826277733\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.494697 12.484674 12.455546 11.573994 11.57656  12.491405]\n",
      "Reset environment\n",
      "Episode reward: 3209.391260355711\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.496597 12.486601 12.457407 11.57605  11.578331 12.493308]\n",
      "Reset environment\n",
      "Episode reward: 2249.782551318407\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.497779 12.487769 12.458614 11.577344 11.579387 12.494491]\n",
      "Reset environment\n",
      "Episode reward: 3404.341298684478\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.49971  12.48968  12.460555 11.579438 11.581107 12.496418]\n",
      "Reset environment\n",
      "Episode reward: 3348.172783046961\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.501642 12.491633 12.462465 11.581521 11.582855 12.498345]\n",
      "Reset environment\n",
      "Episode reward: 2334.658566042781\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.5027275 12.492669  12.463612  11.582734  11.583856  12.499431 ]\n",
      "Reset environment\n",
      "Episode reward: 2005.3140489459038\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.504235 12.49418  12.46512  11.584381 11.585191 12.500939]\n",
      "Reset environment\n",
      "Episode reward: 2072.7214843928814\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.50526  12.495182 12.466168 11.58552  11.586094 12.501963]\n",
      "Reset environment\n",
      "Episode reward: 3536.3628695607185\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.50733   12.497245  12.468241  11.5877495 11.587939  12.50403  ]\n",
      "Reset environment\n",
      "Episode reward: 3868.258941411972\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.509542 12.499432 12.470483 11.590142 11.589933 12.506243]\n",
      "Reset environment\n",
      "Episode reward: 1862.470367193222\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.51093  12.500811 12.471878 11.59167  11.591147 12.507632]\n",
      "Reset environment\n",
      "Episode reward: 142.64825201034546\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.5100155 12.4998    12.471072  11.590616  11.590326  12.506724 ]\n",
      "Reset environment\n",
      "Episode reward: 1726.8800545334816\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.511332 12.501115 12.472383 11.592066 11.59149  12.508039]\n",
      "Reset environment\n",
      "Episode reward: 1893.7447492480278\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.51229  12.502073 12.47335  11.593132 11.592336 12.508998]\n",
      "Reset environment\n",
      "Episode reward: 1921.6653838157654\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.513326 12.503097 12.474392 11.594265 11.593247 12.510027]\n",
      "Reset environment\n",
      "Episode reward: 4189.736798465252\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.51584  12.505599 12.476915 11.596959 11.595476 12.512541]\n",
      "Reset environment\n",
      "Episode reward: 1547.1217048168182\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.516645 12.506413 12.477715 11.597858 11.596216 12.513347]\n",
      "Reset environment\n",
      "Episode reward: 3005.128852069378\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.518318  12.508074  12.479398  11.59968   11.597712  12.5150175]\n",
      "Reset environment\n",
      "Episode reward: 2160.1788713037968\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.519411 12.509136 12.480517 11.600879 11.598696 12.516109]\n",
      "Reset environment\n",
      "Episode reward: 3318.0654534017667\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.521145 12.51092  12.482205 11.602791 11.600234 12.517848]\n",
      "Reset environment\n",
      "Episode reward: 1386.2767515182495\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.52223  12.512006 12.483292 11.603994 11.601191 12.518934]\n",
      "Reset environment\n",
      "Episode reward: 3739.007319509983\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.52447   12.5142565 12.485519  11.606388  11.603273  12.521181 ]\n",
      "Reset environment\n",
      "Episode reward: 2009.3410264253616\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.525563  12.515356  12.4866085 11.607584  11.604281  12.522275 ]\n",
      "Reset environment\n",
      "Episode reward: 2038.6850259304047\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.527077 12.516872 12.488117 11.609238 11.605645 12.523789]\n",
      "Reset environment\n",
      "Episode reward: 690.1312981843948\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.526587 12.516291 12.487711 11.608662 11.60525  12.523308]\n",
      "Reset environment\n",
      "Episode reward: 716.3441264629364\n",
      "Total Steps: 22\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.526832 12.516538 12.487955 11.608965 11.605462 12.523554]\n",
      "Reset environment\n",
      "Episode reward: 4208.265626728535\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.529369 12.519089 12.490474 11.611692 11.607749 12.526089]\n",
      "Reset environment\n",
      "Episode reward: 3626.818948686123\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.531457 12.521168 12.492567 11.613949 11.609606 12.528177]\n",
      "Reset environment\n",
      "Episode reward: 2768.7989916205406\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.532891 12.522613 12.493987 11.615517 11.61088  12.529612]\n",
      "Reset environment\n",
      "Episode reward: 2660.436621516943\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.534309 12.524013 12.495432 11.617055 11.612148 12.531029]\n",
      "Reset environment\n",
      "Episode reward: 1601.2768414020538\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.535109 12.524824 12.496219 11.617938 11.612874 12.531831]\n",
      "Reset environment\n",
      "Episode reward: 247.5636612176895\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.534279 12.523961 12.495427 11.616882 11.612243 12.531008]\n",
      "Reset environment\n",
      "Episode reward: 1378.3616552352905\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.534966 12.524654 12.496109 11.617656 11.612866 12.531695]\n",
      "Reset environment\n",
      "Episode reward: 3319.81481218338\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.536867 12.526534 12.498017 11.619709 11.614534 12.533595]\n",
      "Reset environment\n",
      "Episode reward: 3470.423119902611\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.538862 12.528533 12.500005 11.621855 11.616304 12.535588]\n",
      "Reset environment\n",
      "Episode reward: 4298.030382275581\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.54143  12.53109  12.502582 11.624601 11.6186   12.538158]\n",
      "Reset environment\n",
      "Episode reward: 3130.3883021473885\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.543207 12.532873 12.50435  11.626521 11.620196 12.539934]\n",
      "Reset environment\n",
      "Episode reward: 1505.5529562830925\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.543946 12.533605 12.505096 11.627346 11.620845 12.540673]\n",
      "Reset environment\n",
      "Episode reward: 1392.042408168316\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.545046  12.534703  12.506199  11.6285515 11.621812  12.541774 ]\n",
      "Reset environment\n",
      "Episode reward: 1947.5310260653496\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.546514  12.536165  12.507666  11.630149  11.623102  12.5432415]\n",
      "Reset environment\n",
      "Episode reward: 1413.263117492199\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.547628  12.537279  12.508782  11.6313715 11.624078  12.544355 ]\n",
      "Reset environment\n",
      "Episode reward: 2941.210892856121\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.549167 12.538825 12.51032  11.63306  11.625455 12.545896]\n",
      "Reset environment\n",
      "Episode reward: 2192.8814058825374\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.549826  12.539376  12.5110655 11.633759  11.626079  12.546556 ]\n",
      "Reset environment\n",
      "Episode reward: 1617.439616203308\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.55062  12.540201 12.511833 11.634655 11.626795 12.547353]\n",
      "Reset environment\n",
      "Episode reward: 2169.9909129738808\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.551694  12.54127   12.512912  11.635847  11.6277485 12.548428 ]\n",
      "Reset environment\n",
      "Episode reward: 4831.384660303593\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.554593 12.544179 12.515816 11.638979 11.630376 12.551337]\n",
      "Reset environment\n",
      "Episode reward: 2619.5186540186405\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.555924 12.545474 12.51718  11.640432 11.631551 12.552667]\n",
      "Reset environment\n",
      "Episode reward: 2692.0702418088913\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.557407  12.54696   12.51867   11.642052  11.6328535 12.554154 ]\n",
      "Reset environment\n",
      "Episode reward: 807.1127659082413\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.557304 12.546887 12.51852  11.641801 11.632801 12.554056]\n",
      "Reset environment\n",
      "Episode reward: 424.5297466516495\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.556472  12.545967  12.517798  11.640879  11.6320915 12.553238 ]\n",
      "Reset environment\n",
      "Episode reward: 1634.2478061914444\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.557715 12.54721  12.519036 11.642248 11.633173 12.554481]\n",
      "Reset environment\n",
      "Episode reward: 2737.956850707531\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.559173 12.54865  12.520526 11.643842 11.63447  12.555938]\n",
      "Reset environment\n",
      "Episode reward: 3508.6514649391174\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.561138 12.5506   12.5225   11.645972 11.636213 12.557902]\n",
      "Reset environment\n",
      "Episode reward: 3752.2997522205114\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.563246 12.552746 12.52459  11.648293 11.638139 12.560009]\n",
      "Reset environment\n",
      "Episode reward: 2010.090569600463\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.563777 12.553299 12.525081 11.648806 11.638654 12.560542]\n",
      "Reset environment\n",
      "Episode reward: 2948.4304215312004\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.565463 12.554974 12.526764 11.650631 11.640133 12.562226]\n",
      "Reset environment\n",
      "Episode reward: 5011.4247584939\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.568477 12.557989 12.529771 11.653849 11.642872 12.565237]\n",
      "Reset environment\n",
      "Episode reward: 1780.2986850738525\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.569409 12.558925 12.530694 11.654867 11.643713 12.566165]\n",
      "Reset environment\n",
      "Episode reward: 5847.184291064739\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.572982 12.562502 12.534258 11.658668 11.646948 12.569738]\n",
      "Reset environment\n",
      "Episode reward: 1611.9599513411522\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.573797 12.563317 12.535078 11.659567 11.647671 12.570556]\n",
      "Reset environment\n",
      "Episode reward: 2049.693167448044\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.574718 12.56426  12.535991 11.660514 11.648584 12.571486]\n",
      "Reset environment\n",
      "Episode reward: 1510.1275358200073\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.575487 12.565042 12.536756 11.661372 11.649291 12.572255]\n",
      "Reset environment\n",
      "Episode reward: 1957.6183149516582\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.576465 12.566036 12.537718 11.662436 11.650224 12.573235]\n",
      "Reset environment\n",
      "Episode reward: 4983.499852716923\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.579445 12.568996 12.540714 11.665635 11.652925 12.576214]\n",
      "Reset environment\n",
      "Episode reward: 1504.732299350202\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.579843 12.569445 12.54105  11.665964 11.653335 12.576611]\n",
      "Reset environment\n",
      "Episode reward: 5371.660243213177\n",
      "Total Steps: 203\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.583614 12.5732   12.544829 11.669983 11.656743 12.580386]\n",
      "Reset environment\n",
      "Episode reward: 1439.7670090198517\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.584149 12.573687 12.545417 11.670603 11.657251 12.580921]\n",
      "Reset environment\n",
      "Episode reward: -682.9217756986618\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.58238  12.571924 12.543645 11.668698 11.65564  12.579155]\n",
      "Reset environment\n",
      "Episode reward: 4041.8800409436226\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.584753 12.574299 12.546014 11.671259 11.65778  12.581529]\n",
      "Reset environment\n",
      "Episode reward: 3536.183197557926\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.586773 12.576348 12.548009 11.67345  11.659596 12.583551]\n",
      "Reset environment\n",
      "Episode reward: 3170.5825901702046\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.588221 12.577782 12.549464 11.675096 11.660961 12.584998]\n",
      "Reset environment\n",
      "Episode reward: 2697.0358236134052\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.589629 12.579228 12.550832 11.676643 11.662251 12.586406]\n",
      "Reset environment\n",
      "Episode reward: 1946.5096252560616\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.590655 12.580239 12.551867 11.677775 11.663158 12.587431]\n",
      "Reset environment\n",
      "Episode reward: 3692.74107016623\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.5927515 12.582312  12.553981  11.680036  11.665024  12.589533 ]\n",
      "Reset environment\n",
      "Episode reward: 5267.537417113781\n",
      "Total Steps: 182\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.595776 12.585336 12.557015 11.683273 11.667722 12.592556]\n",
      "Reset environment\n",
      "Episode reward: 1600.208808004856\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.5965805 12.586118  12.557831  11.684163  11.66844   12.593361 ]\n",
      "Reset environment\n",
      "Episode reward: 2161.5482302308083\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.598163 12.587694 12.559416 11.685883 11.669841 12.594943]\n",
      "Reset environment\n",
      "Episode reward: 2477.517620265484\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.599487  12.5890045 12.560744  11.68733   11.671008  12.596267 ]\n",
      "Reset environment\n",
      "Episode reward: 4291.70643055439\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.60201  12.591519 12.563276 11.690046 11.673243 12.598789]\n",
      "Reset environment\n",
      "Episode reward: 4276.498191416264\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.604575 12.594077 12.565844 11.692792 11.675538 12.601354]\n",
      "Reset environment\n",
      "Episode reward: 2212.8747431635857\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.606206 12.595706 12.567479 11.694548 11.676989 12.602986]\n",
      "Reset environment\n",
      "Episode reward: 2158.285575389862\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.607774 12.597279 12.569041 11.696261 11.678407 12.604553]\n",
      "Reset environment\n",
      "Episode reward: 1954.2849560827017\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.60847  12.598017 12.569694 11.69708  11.678986 12.605247]\n",
      "Reset environment\n",
      "Episode reward: 2438.1467847563326\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.609717 12.599236 12.570967 11.698445 11.680093 12.60649 ]\n",
      "Reset environment\n",
      "Episode reward: 783.0641901493073\n",
      "Total Steps: 24\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.610004 12.599523 12.571256 11.698789 11.68034  12.606778]\n",
      "Reset environment\n",
      "Episode reward: 5288.681094169617\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.613189 12.602686 12.574447 11.702194 11.683205 12.609966]\n",
      "Reset environment\n",
      "Episode reward: 2450.6992502212524\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.614324 12.603848 12.575534 11.703464 11.684206 12.611098]\n",
      "Reset environment\n",
      "Episode reward: 5792.202748775482\n",
      "Total Steps: 198\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.617854 12.607376 12.579059 11.707222 11.687371 12.61464 ]\n",
      "Reset environment\n",
      "Episode reward: 5217.793436944485\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.620986  12.6104765 12.582201  11.710586  11.6901865 12.617769 ]\n",
      "Reset environment\n",
      "Episode reward: 1444.4865508079529\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.622122 12.611615 12.583332 11.711837 11.691189 12.618904]\n",
      "Reset environment\n",
      "Episode reward: 1487.0795372128487\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.622849 12.612338 12.584067 11.712647 11.691826 12.619633]\n",
      "Reset environment\n",
      "Episode reward: 4587.131449043751\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.625574 12.615074 12.586797 11.71558  11.694289 12.622364]\n",
      "Reset environment\n",
      "Episode reward: 4620.902954041958\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.628318  12.617807  12.589547  11.718521  11.6967325 12.625106 ]\n",
      "Reset environment\n",
      "Episode reward: 3591.5295098870993\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.630367  12.619888  12.591542  11.720729  11.698574  12.6271515]\n",
      "Reset environment\n",
      "Episode reward: 2871.19777277112\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.631888  12.621428  12.5930195 11.72238   11.699933  12.628669 ]\n",
      "Reset environment\n",
      "Episode reward: 5754.495151221752\n",
      "Total Steps: 196\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.635365 12.62488  12.596485 11.726094 11.703053 12.632155]\n",
      "Reset environment\n",
      "Episode reward: 1489.7831308841705\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.635772 12.625232 12.596941 11.726434 11.703496 12.632571]\n",
      "Reset environment\n",
      "Episode reward: 2845.3149838149548\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.637312 12.626778 12.598471 11.728106 11.704854 12.63411 ]\n",
      "Reset environment\n",
      "Episode reward: 1699.0148267447948\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.638017 12.627439 12.599217 11.728895 11.705501 12.634817]\n",
      "Reset environment\n",
      "Episode reward: 1846.776087552309\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.638909 12.628313 12.600126 11.729884 11.706278 12.635708]\n",
      "Reset environment\n",
      "Episode reward: 1465.4186701476574\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.639609 12.629    12.600844 11.730661 11.706903 12.636408]\n",
      "Reset environment\n",
      "Episode reward: 4261.192349404097\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.642088 12.631453 12.603332 11.733334 11.709138 12.63889 ]\n",
      "Reset environment\n",
      "Episode reward: 2235.3417494893074\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.643234 12.632604 12.604475 11.7346   11.710175 12.640036]\n",
      "Reset environment\n",
      "Episode reward: 3286.954147517681\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.645128  12.634508  12.60636   11.7366295 11.711859  12.641932 ]\n",
      "Reset environment\n",
      "Episode reward: 4134.539553582668\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.64754  12.636922 12.608773 11.739228 11.714032 12.644343]\n",
      "Reset environment\n",
      "Episode reward: 3248.306740552187\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.649259 12.638616 12.610521 11.741096 11.715586 12.646055]\n",
      "Reset environment\n",
      "Episode reward: 4247.730583131313\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.651702 12.641064 12.612961 11.743752 11.717798 12.648497]\n",
      "Reset environment\n",
      "Episode reward: 1381.7763397097588\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.652775  12.6421385 12.614035  11.744937  11.718749  12.64957  ]\n",
      "Reset environment\n",
      "Episode reward: 2255.909686565399\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.653983 12.643364 12.615228 11.746265 11.719854 12.65078 ]\n",
      "Reset environment\n",
      "Episode reward: 1675.543658554554\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.654795  12.644177  12.6160345 11.747171  11.720576  12.65159  ]\n",
      "Reset environment\n",
      "Episode reward: 3929.0127611756325\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.657094  12.646471  12.61834   11.749627  11.7226305 12.653889 ]\n",
      "Reset environment\n",
      "Episode reward: 2276.721906721592\n",
      "Total Steps: 167\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.657905 12.647309 12.619134 11.750465 11.72339  12.654699]\n",
      "Reset environment\n",
      "Episode reward: 3317.188761651516\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.659845 12.649243 12.621076 11.752556 11.725133 12.656639]\n",
      "Reset environment\n",
      "Episode reward: 1712.314251422882\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.660628 12.649999 12.621888 11.753444 11.725848 12.657423]\n",
      "Reset environment\n",
      "Episode reward: 1399.6435590982437\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.661694 12.651063 12.622954 11.754629 11.726773 12.658489]\n",
      "Reset environment\n",
      "Episode reward: 3775.3315136432648\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.663774 12.653148 12.625032 11.756887 11.728621 12.660569]\n",
      "Reset environment\n",
      "Episode reward: 3607.849450170994\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.665865 12.655249 12.6271   11.759119 11.730501 12.662661]\n",
      "Reset environment\n",
      "Episode reward: 2899.3946222662926\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.667394  12.6567545 12.628658  11.760782  11.731878  12.664188 ]\n",
      "Reset environment\n",
      "Episode reward: 1367.097565293312\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.668451 12.65781  12.629715 11.761956 11.732806 12.665244]\n",
      "Reset environment\n",
      "Episode reward: 1107.1794880628586\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.668234 12.65766  12.629445 11.76162  11.732612 12.665038]\n",
      "Reset environment\n",
      "Episode reward: 6635.607930779457\n",
      "Total Steps: 223\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.672334 12.661724 12.633538 11.765972 11.73629  12.669141]\n",
      "Reset environment\n",
      "Episode reward: 2120.0049466490746\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.673455 12.662851 12.634657 11.7672   11.737313 12.670262]\n",
      "Reset environment\n",
      "Episode reward: 3766.377901047468\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.675557 12.664988 12.636725 11.769482 11.739219 12.672362]\n",
      "Reset environment\n",
      "Episode reward: 961.4110555648804\n",
      "Total Steps: 31\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.675948 12.665375 12.63712  11.76994  11.739561 12.672753]\n",
      "Reset environment\n",
      "Episode reward: 2660.424246132374\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.6774   12.666835 12.638566 11.771521 11.740864 12.674206]\n",
      "Reset environment\n",
      "Episode reward: 2475.279352426529\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.6787615 12.668193  12.639929  11.772999  11.742093  12.67557  ]\n",
      "Reset environment\n",
      "Episode reward: 1101.0243706703186\n",
      "Total Steps: 34\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.679258 12.668691 12.640426 11.773564 11.742542 12.676066]\n",
      "Reset environment\n",
      "Episode reward: 5188.187969624996\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.682339 12.671768 12.64349  11.776865 11.745321 12.679144]\n",
      "Reset environment\n",
      "Episode reward: 2433.4485780000687\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.68354  12.673006 12.644653 11.778194 11.746403 12.680345]\n",
      "Reset environment\n",
      "Episode reward: 1807.2295860052109\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.684467 12.673927 12.645582 11.779224 11.747229 12.681272]\n",
      "Reset environment\n",
      "Episode reward: 1941.2576799318194\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.685169  12.674673  12.646239  11.780052  11.7478285 12.681972 ]\n",
      "Reset environment\n",
      "Episode reward: 5167.785528063774\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.68826  12.677754 12.649319 11.783369 11.750592 12.685062]\n",
      "Reset environment\n",
      "Episode reward: 4219.065375983715\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.69068  12.680152 12.651763 11.785973 11.752763 12.687485]\n",
      "Reset environment\n",
      "Episode reward: 647.3876467943192\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.690689 12.680166 12.651765 11.78593  11.752813 12.687498]\n",
      "Reset environment\n",
      "Episode reward: 3142.1139895915985\n",
      "Total Steps: 108\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.692437 12.681897 12.653522 11.787829 11.754358 12.689246]\n",
      "Reset environment\n",
      "Episode reward: 3789.5368309020996\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.69455   12.684021  12.65562   11.7901325 11.756281  12.691358 ]\n",
      "Reset environment\n",
      "Episode reward: 2301.6543674003333\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.695368 12.684864 12.656399 11.791017 11.757024 12.692181]\n",
      "Reset environment\n",
      "Episode reward: 3001.9436477720737\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.696991 12.686469 12.65804  11.792788 11.758494 12.693807]\n",
      "Reset environment\n",
      "Episode reward: 3038.926360964775\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.698686  12.688176  12.659709  11.7946205 11.760035  12.6954975]\n",
      "Reset environment\n",
      "Episode reward: 1815.7019308805466\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.699616 12.689115 12.660638 11.795645 11.76089  12.696426]\n",
      "Reset environment\n",
      "Episode reward: 812.4031393527985\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.699089 12.688666 12.660032 11.794931 11.760409 12.695903]\n",
      "Reset environment\n",
      "Episode reward: 3184.0080279037356\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.700809 12.69043  12.661704 11.796813 11.761959 12.697627]\n",
      "Reset environment\n",
      "Episode reward: 4330.025224268436\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.703354 12.692966 12.664255 11.799539 11.764233 12.700173]\n",
      "Reset environment\n",
      "Episode reward: 3987.3730214238167\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.705674 12.695276 12.666579 11.802028 11.766284 12.702498]\n",
      "Reset environment\n",
      "Episode reward: 4159.034278690815\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.708107 12.697727 12.668979 11.804629 11.768477 12.704924]\n",
      "Reset environment\n",
      "Episode reward: 2180.4986069202423\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.709198  12.698817  12.670077  11.8058405 11.769445  12.706016 ]\n",
      "Reset environment\n",
      "Episode reward: 2952.221110612154\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.710712 12.700374 12.671549 11.807501 11.77081  12.707533]\n",
      "Reset environment\n",
      "Episode reward: 3841.4417857825756\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.712945 12.702621 12.673766 11.809911 11.772809 12.709761]\n",
      "Reset environment\n",
      "Episode reward: 5407.396918773651\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.716146  12.7058325 12.676964  11.813338  11.775716  12.712959 ]\n",
      "Reset environment\n",
      "Episode reward: 5067.183632075787\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.7191515 12.708815  12.679972  11.81655   11.778389  12.715963 ]\n",
      "Reset environment\n",
      "Episode reward: 2703.2362797260284\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.720614 12.710282 12.681433 11.818139 11.779707 12.717426]\n",
      "Reset environment\n",
      "Episode reward: 2691.810225188732\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.721884 12.711583 12.682653 11.819548 11.780844 12.718698]\n",
      "Reset environment\n",
      "Episode reward: 1822.4586118012667\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.722477 12.712228 12.683203 11.820253 11.781355 12.719293]\n",
      "Reset environment\n",
      "Episode reward: 6693.575298368931\n",
      "Total Steps: 225\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.726549 12.71628  12.687283 11.824597 11.78504  12.723371]\n",
      "Reset environment\n",
      "Episode reward: 3779.1647410988808\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.728672 12.718409 12.689403 11.826894 11.786933 12.725492]\n",
      "Reset environment\n",
      "Episode reward: 2969.9450047910213\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.730144 12.719834 12.690924 11.828516 11.788256 12.726966]\n",
      "Reset environment\n",
      "Episode reward: 1923.231908082962\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.731551  12.721245  12.69233   11.83005   11.789513  12.7283745]\n",
      "Reset environment\n",
      "Episode reward: 3746.9556851387024\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.733681 12.723373 12.694459 11.83236  11.791402 12.730506]\n",
      "Reset environment\n",
      "Episode reward: 4274.629672288895\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.736048 12.725739 12.696834 11.834909 11.793491 12.732876]\n",
      "Reset environment\n",
      "Episode reward: 2908.662149608135\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.737637  12.727323  12.698429  11.836632  11.794915  12.7344675]\n",
      "Reset environment\n",
      "Episode reward: 5394.560410737991\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.740859  12.7305355 12.70165   11.840079  11.797823  12.737692 ]\n",
      "Reset environment\n",
      "Episode reward: 3883.3841361403465\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.743079 12.732761 12.703868 11.842474 11.799823 12.739917]\n",
      "Reset environment\n",
      "Episode reward: 2533.406583148986\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.744395 12.734111 12.705151 11.843914 11.801039 12.741233]\n",
      "Reset environment\n",
      "Episode reward: 5554.819172680378\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.747572 12.737277 12.708318 11.847324 11.803863 12.7444  ]\n",
      "Reset environment\n",
      "Episode reward: 2030.5137484669685\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.748592 12.738301 12.709325 11.848443 11.804769 12.745415]\n",
      "Reset environment\n",
      "Episode reward: 4881.434632062912\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.751456  12.741174  12.712183  11.851502  11.8073635 12.748285 ]\n",
      "Reset environment\n",
      "Episode reward: -78.32216560840607\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.750217 12.739817 12.711068 11.850237 11.806204 12.747056]\n",
      "Reset environment\n",
      "Episode reward: -683.4224712848663\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.748461 12.738069 12.709303 11.848298 11.804616 12.745304]\n",
      "Reset environment\n",
      "Episode reward: 1432.791161119938\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.749141 12.738747 12.709988 11.849058 11.805231 12.745987]\n",
      "Reset environment\n",
      "Episode reward: 5616.609049260616\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.752524 12.74212  12.713362 11.852651 11.808273 12.749372]\n",
      "Reset environment\n",
      "Episode reward: 1582.8742769956589\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.753257  12.742834  12.714113  11.853468  11.8089285 12.750103 ]\n",
      "Reset environment\n",
      "Episode reward: 5719.430374145508\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.756515 12.746106 12.71737  11.856978 11.81185  12.753361]\n",
      "Reset environment\n",
      "Episode reward: 1274.6877686977386\n",
      "Total Steps: 40\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.757113 12.746715 12.717967 11.857655 11.8124   12.753961]\n",
      "Reset environment\n",
      "Episode reward: 4717.355461478233\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.759855 12.749443 12.720715 11.860597 11.814818 12.756703]\n",
      "Reset environment\n",
      "Episode reward: 2934.078951358795\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.761448 12.751045 12.722307 11.862332 11.816242 12.7583  ]\n",
      "Reset environment\n",
      "Episode reward: 2219.468472599983\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.763055 12.752653 12.723909 11.864065 11.817682 12.759908]\n",
      "Reset environment\n",
      "Episode reward: 1247.515748500824\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.763497 12.753099 12.724351 11.864508 11.818123 12.760355]\n",
      "Reset environment\n",
      "Episode reward: 3044.024949103594\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.765127 12.754751 12.725953 11.866302 11.8196   12.761979]\n",
      "Reset environment\n",
      "Episode reward: 5052.924869179726\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.768129  12.757741  12.72896   11.8694935 11.822288  12.764982 ]\n",
      "Reset environment\n",
      "Episode reward: 1918.3222474455833\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.769517  12.7591305 12.730351  11.871023  11.823534  12.766375 ]\n",
      "Reset environment\n",
      "Episode reward: 3062.6047249138355\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.771028 12.760594 12.73191  11.87269  11.824896 12.767883]\n",
      "Reset environment\n",
      "Episode reward: 1837.7685810178518\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.771626 12.76124  12.732468 11.873407 11.82541  12.768486]\n",
      "Reset environment\n",
      "Episode reward: 3443.5389391183853\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.773442 12.763055 12.734286 11.875385 11.827012 12.770304]\n",
      "Reset environment\n",
      "Episode reward: 3823.6895649433136\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.775616 12.765233 12.736456 11.877714 11.828956 12.77248 ]\n",
      "Reset environment\n",
      "Episode reward: 2745.025455147028\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.776988  12.766634  12.7377825 11.879219  11.830178  12.773853 ]\n",
      "Reset environment\n",
      "Episode reward: 4396.540405571461\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.779529 12.769175 12.740315 11.881941 11.832481 12.776394]\n",
      "Reset environment\n",
      "Episode reward: 2673.580491423607\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.780779 12.770461 12.741523 11.88332  11.833589 12.777645]\n",
      "Reset environment\n",
      "Episode reward: 2884.6558217406273\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.782351 12.77203  12.743099 11.885025 11.835004 12.779217]\n",
      "Reset environment\n",
      "Episode reward: 4681.250862956047\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.785085  12.774764  12.7458315 11.887953  11.8374605 12.781956 ]\n",
      "Reset environment\n",
      "Episode reward: 1569.8327829241753\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.785818 12.775496 12.74657  11.888776 11.838106 12.782692]\n",
      "Reset environment\n",
      "Episode reward: 2334.4891767166555\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.786867 12.776513 12.747652 11.889954 11.839024 12.783739]\n",
      "Reset environment\n",
      "Episode reward: 1236.3455667495728\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.786354 12.775939 12.747213 11.889502 11.838525 12.783229]\n",
      "Reset environment\n",
      "Episode reward: 2031.005960226059\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.787802  12.777394  12.748661  11.891083  11.8398285 12.784679 ]\n",
      "Reset environment\n",
      "Episode reward: 2509.9866222813725\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.789059  12.7787    12.749886  11.892487  11.8409815 12.785934 ]\n",
      "Reset environment\n",
      "Episode reward: 3264.83994063735\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.790751 12.780431 12.751505 11.894336 11.842488 12.787628]\n",
      "Reset environment\n",
      "Episode reward: 2600.7569242417812\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.792135 12.781843 12.752858 11.895838 11.8438   12.789015]\n",
      "Reset environment\n",
      "Episode reward: 113.5885237455368\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.791406 12.781114 12.75213  11.894929 11.843144 12.788297]\n",
      "Reset environment\n",
      "Episode reward: 1690.6055154204369\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.792245 12.781967 12.752959 11.895861 11.843897 12.789135]\n",
      "Reset environment\n",
      "Episode reward: 1916.0338485836983\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.793612 12.783334 12.754326 11.897372 11.845092 12.790502]\n",
      "Reset environment\n",
      "Episode reward: 1383.7235420942307\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.794651  12.784374  12.755366  11.8985195 11.846011  12.791542 ]\n",
      "Reset environment\n",
      "Episode reward: 2205.034925699234\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.796219 12.785937 12.756943 11.900219 11.847408 12.793111]\n",
      "Reset environment\n",
      "Episode reward: 1805.983360350132\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.797141 12.786866 12.757859 11.901233 11.848254 12.794032]\n",
      "Reset environment\n",
      "Episode reward: 1575.7978748083115\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.797467 12.787216 12.758161 11.901504 11.848551 12.79436 ]\n",
      "Reset environment\n",
      "Episode reward: 2757.390506386757\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.798954 12.788678 12.759657 11.903116 11.849865 12.795843]\n",
      "Reset environment\n",
      "Episode reward: -682.6882885694504\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.797184 12.786884 12.757934 11.901315 11.848283 12.794071]\n",
      "Reset environment\n",
      "Episode reward: 1617.9694178700447\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.798365 12.788066 12.759115 11.902621 11.849328 12.795252]\n",
      "Reset environment\n",
      "Episode reward: 1650.6859022974968\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.799153 12.78884  12.759922 11.903488 11.850039 12.79604 ]\n",
      "Reset environment\n",
      "Episode reward: 2559.5121911019087\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.800089 12.789807 12.760817 11.904421 11.850887 12.79698 ]\n",
      "Reset environment\n",
      "Episode reward: 2228.0381515026093\n",
      "Total Steps: 2001\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.776977 12.76667  12.737726 11.870697 11.828952 12.773851]\n",
      "Reset environment\n",
      "Episode reward: 929.374237537384\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.776412 12.766023 12.737245 11.870157 11.828453 12.773298]\n",
      "Reset environment\n",
      "Episode reward: 3298.134459141642\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.778186  12.767841  12.738945  11.872091  11.8300495 12.775075 ]\n",
      "Reset environment\n",
      "Episode reward: 2585.3469479829073\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.779494 12.769123 12.740274 11.873507 11.831217 12.776379]\n",
      "Reset environment\n",
      "Episode reward: 1330.2753862142563\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.780475 12.770102 12.741253 11.874605 11.832077 12.777358]\n",
      "Reset environment\n",
      "Episode reward: 2097.786404520273\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.781532 12.771151 12.74233  11.875769 11.833032 12.778416]\n",
      "Reset environment\n",
      "Episode reward: 4118.619849860668\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.78387   12.7734785 12.74468   11.878292  11.835135  12.780755 ]\n",
      "Reset environment\n",
      "Episode reward: 3094.0080974660814\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.785465 12.775062 12.746265 11.880016 11.836612 12.782351]\n",
      "Reset environment\n",
      "Episode reward: 1884.9761103391647\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.786361  12.775936  12.747182  11.8810005 11.837397  12.783245 ]\n",
      "Reset environment\n",
      "Episode reward: 2120.737634062767\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.78742  12.776978 12.748262 11.882165 11.838333 12.784305]\n",
      "Reset environment\n",
      "Episode reward: 533.0548771619797\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.786798 12.776257 12.747749 11.881426 11.837826 12.783685]\n",
      "Reset environment\n",
      "Episode reward: 4969.508958697319\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.789698 12.779163 12.750642 11.884532 11.840412 12.786587]\n",
      "Reset environment\n",
      "Episode reward: 5130.379646420479\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.79267  12.782109 12.753633 11.887727 11.843074 12.789563]\n",
      "Reset environment\n",
      "Episode reward: 2175.8554104864597\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.793719 12.783139 12.754697 11.88889  11.844015 12.790612]\n",
      "Reset environment\n",
      "Episode reward: 5641.115067660809\n",
      "Total Steps: 189\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.7970915 12.7865095 12.75806   11.892473  11.847074  12.793988 ]\n",
      "Reset environment\n",
      "Episode reward: 2072.4310113191605\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.798552  12.787979  12.759511  11.894074  11.848373  12.7954445]\n",
      "Reset environment\n",
      "Episode reward: 3620.669850051403\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.800581 12.790013 12.761535 11.896255 11.850159 12.797475]\n",
      "Reset environment\n",
      "Episode reward: 3691.559334695339\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.8026705 12.792095  12.76363   11.898503  11.852029  12.799565 ]\n",
      "Reset environment\n",
      "Episode reward: 2315.7689713835716\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.804315 12.793742 12.765266 11.900277 11.853494 12.801209]\n",
      "Reset environment\n",
      "Episode reward: 1445.7909340560436\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.804538  12.7940235 12.76543   11.900425  11.853718  12.8014345]\n",
      "Reset environment\n",
      "Episode reward: 4464.767765939236\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.807089  12.796576  12.767974  11.9031725 11.856003  12.803986 ]\n",
      "Reset environment\n",
      "Episode reward: 4856.398417294025\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.809893 12.79936  12.770788 11.906184 11.858513 12.806792]\n",
      "Reset environment\n",
      "Episode reward: 1481.7867279052734\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.810602 12.800064 12.771504 11.906973 11.859149 12.807502]\n",
      "Reset environment\n",
      "Episode reward: 2410.118809223175\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.811861 12.801328 12.772754 11.908345 11.860278 12.80876 ]\n",
      "Reset environment\n",
      "Episode reward: 1443.7035457491875\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.812931 12.802399 12.773827 11.90953  11.86123  12.80983 ]\n",
      "Reset environment\n",
      "Episode reward: 1493.4848492145538\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.813627 12.803078 12.774542 11.910307 11.861851 12.810529]\n",
      "Reset environment\n",
      "Episode reward: 4116.895733952522\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.815949 12.805392 12.776855 11.912817 11.863948 12.812845]\n",
      "Reset environment\n",
      "Episode reward: 1626.8279754519463\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.816721 12.806156 12.777634 11.913674 11.864631 12.813617]\n",
      "Reset environment\n",
      "Episode reward: 3507.16941344738\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.8186655 12.808096  12.779594  11.915768  11.866368  12.815563 ]\n",
      "Reset environment\n",
      "Episode reward: 1848.1768120527267\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.819561  12.808997  12.78048   11.9167595 11.867182  12.81646  ]\n",
      "Reset environment\n",
      "Episode reward: 2277.9001754522324\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.821152 12.81059  12.782068 11.918497 11.8686   12.818053]\n",
      "Reset environment\n",
      "Episode reward: 2417.755522310734\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.822388 12.81183  12.7833   11.919864 11.869709 12.819291]\n",
      "Reset environment\n",
      "Episode reward: 286.26688408851624\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.821577 12.810943 12.782565 11.918836 11.869042 12.818483]\n",
      "Reset environment\n",
      "Episode reward: 4946.680786252022\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.824468  12.813827  12.785465  11.9219265 11.871617  12.821377 ]\n",
      "Reset environment\n",
      "Episode reward: 1643.8069095015526\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.825235  12.814602  12.786225  11.922777  11.872295  12.8221445]\n",
      "Reset environment\n",
      "Episode reward: 4422.753551721573\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.827631 12.816985 12.788623 11.92537  11.874422 12.824542]\n",
      "Reset environment\n",
      "Episode reward: 5356.558679223061\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.830727  12.820059  12.791737  11.928686  11.877231  12.8276415]\n",
      "Reset environment\n",
      "Episode reward: 3836.240523815155\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.832757  12.822089  12.793769  11.930887  11.8790245 12.82967  ]\n",
      "Reset environment\n",
      "Episode reward: 1950.0965970158577\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.834143  12.82348   12.7951565 11.932411  11.880241  12.831059 ]\n",
      "Reset environment\n",
      "Episode reward: 1628.9369577094913\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.834874 12.824188 12.795916 11.933224 11.880889 12.831791]\n",
      "Reset environment\n",
      "Episode reward: 2937.407552868128\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.836327 12.825691 12.797339 11.934833 11.882221 12.833241]\n",
      "Reset environment\n",
      "Episode reward: 2527.2106163799763\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.837619 12.827015 12.798598 11.936258 11.883405 12.834529]\n",
      "Reset environment\n",
      "Episode reward: 3682.529715657234\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.839562  12.8289585 12.800545  11.938367  11.885112  12.836472 ]\n",
      "Reset environment\n",
      "Episode reward: 5822.1870257258415\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.843017 12.832388 12.803995 11.942049 11.888195 12.83993 ]\n",
      "Reset environment\n",
      "Episode reward: 2293.3381428718567\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.844029 12.83337  12.805037 11.943178 11.889091 12.840944]\n",
      "Reset environment\n",
      "Episode reward: 3899.8156201113015\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.846193  12.8355665 12.807141  11.945523  11.891044  12.843106 ]\n",
      "Reset environment\n",
      "Episode reward: 4629.4906841516495\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.84883  12.838186 12.809784 11.94835  11.893412 12.845737]\n",
      "Reset environment\n",
      "Episode reward: 2450.7842748761177\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.850069 12.839431 12.811013 11.949701 11.894506 12.846975]\n",
      "Reset environment\n",
      "Episode reward: 4909.083491265774\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.852912 12.842258 12.813867 11.952741 11.897042 12.84982 ]\n",
      "Reset environment\n",
      "Episode reward: 5506.7461487054825\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.856169  12.845507  12.817114  11.956226  11.8999815 12.853067 ]\n",
      "Reset environment\n",
      "Episode reward: 1739.5564484596252\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.85743   12.8467655 12.818372  11.957608  11.901099  12.854327 ]\n",
      "Reset environment\n",
      "Episode reward: 1860.1212745308876\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.858216 12.847511 12.819193 11.958485 11.901814 12.855115]\n",
      "Reset environment\n",
      "Episode reward: 1413.627147078514\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.859275 12.848569 12.820245 11.959643 11.902747 12.856173]\n",
      "Reset environment\n",
      "Episode reward: 2139.904987413436\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.860263  12.849527  12.8212595 11.960725  11.90362   12.85716  ]\n",
      "Reset environment\n",
      "Episode reward: 5436.379116714001\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.863393 12.852636 12.824403 11.964094 11.906447 12.860291]\n",
      "Reset environment\n",
      "Episode reward: 3541.334510371089\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.865085  12.854361  12.826062  11.965968  11.9079685 12.861988 ]\n",
      "Reset environment\n",
      "Episode reward: 4533.543232798576\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.867664 12.856932 12.828641 11.968748 11.910297 12.864565]\n",
      "Reset environment\n",
      "Episode reward: 2216.7071782946587\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.869234  12.858499  12.8302145 11.970439  11.911689  12.866132 ]\n",
      "Reset environment\n",
      "Episode reward: 5030.8871658444405\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.872124  12.861366  12.833126  11.9735365 11.914321  12.869021 ]\n",
      "Reset environment\n",
      "Episode reward: 2588.014820575714\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.873425 12.862652 12.834436 11.974965 11.915459 12.870321]\n",
      "Reset environment\n",
      "Episode reward: 2907.5135213136673\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.874939 12.864149 12.835971 11.97662  11.916821 12.871832]\n",
      "Reset environment\n",
      "Episode reward: 3952.632700741291\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.8771515 12.866364  12.838183  11.979006  11.91881   12.874047 ]\n",
      "Reset environment\n",
      "Episode reward: 2595.7880559563637\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.878487 12.867675 12.839531 11.980456 11.920003 12.875378]\n",
      "Reset environment\n",
      "Episode reward: -1068.059083558619\n",
      "Total Steps: 125\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.876082 12.865109 12.837296 11.977989 11.91776  12.872989]\n",
      "Reset environment\n",
      "Episode reward: 4789.217525839806\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.878824  12.867838  12.84005   11.98093   11.920196  12.8757305]\n",
      "Reset environment\n",
      "Episode reward: 210.91604948043823\n",
      "Total Steps: 8\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.878714 12.867726 12.839942 11.980823 11.920105 12.875621]\n",
      "Reset environment\n",
      "Episode reward: 4892.496004462242\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.881521  12.870536  12.842739  11.983833  11.922626  12.8784275]\n",
      "Reset environment\n",
      "Episode reward: 864.1333622932434\n",
      "Total Steps: 27\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.881842 12.870868 12.843053 11.984217 11.92291  12.878749]\n",
      "Reset environment\n",
      "Episode reward: 2146.1468775868416\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.883351 12.872377 12.844564 11.985867 11.924239 12.880261]\n",
      "Reset environment\n",
      "Episode reward: 6545.435275793076\n",
      "Total Steps: 228\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.886946 12.875997 12.848149 11.989763 11.927481 12.883866]\n",
      "Reset environment\n",
      "Episode reward: 3461.9340978860855\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.888724 12.877774 12.849932 11.991693 11.929055 12.885645]\n",
      "Reset environment\n",
      "Episode reward: 4950.625436961651\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.89155  12.880607 12.852742 11.994725 11.931595 12.88848 ]\n",
      "Reset environment\n",
      "Episode reward: 2190.7498500347137\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.893079  12.882134  12.8542795 11.996392  11.932955  12.890014 ]\n",
      "Reset environment\n",
      "Episode reward: 1380.9357033967972\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.893499 12.882568 12.854677 11.996805 11.933375 12.890439]\n",
      "Reset environment\n",
      "Episode reward: 1323.4865200519562\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.894105 12.883188 12.855257 11.997481 11.93392  12.891043]\n",
      "Reset environment\n",
      "Episode reward: 1574.462622873485\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.894787  12.883843  12.855964  11.9982395 11.934532  12.891726 ]\n",
      "Reset environment\n",
      "Episode reward: -346.1585259437561\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.89341  12.882431 12.85463  11.99674  11.933342 12.890342]\n",
      "Reset environment\n",
      "Episode reward: 4571.622529864311\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.896019 12.885048 12.857225 11.999537 11.93565  12.892953]\n",
      "Reset environment\n",
      "Episode reward: 1516.3177406191826\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.896745 12.885777 12.857947 12.000343 11.936316 12.89368 ]\n",
      "Reset environment\n",
      "Episode reward: 2582.228077352047\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.898084  12.88711   12.859292  12.0018015 11.937519  12.895018 ]\n",
      "Reset environment\n",
      "Episode reward: 5267.185648322105\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.901117  12.890135  12.862315  12.0050535 11.940233  12.8980465]\n",
      "Reset environment\n",
      "Episode reward: -502.2234293818474\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.899162 12.888084 12.860465 12.002935 11.938504 12.8961  ]\n",
      "Reset environment\n",
      "Episode reward: 1004.7239322662354\n",
      "Total Steps: 35\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.899528 12.888469 12.860809 12.003363 11.938826 12.896468]\n",
      "Reset environment\n",
      "Episode reward: 2363.9182741343975\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.900327  12.8893    12.861567  12.004143  11.9395485 12.897273 ]\n",
      "Reset environment\n",
      "Episode reward: 2189.668067216873\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.901406 12.890387 12.862637 12.005337 11.940514 12.898355]\n",
      "Reset environment\n",
      "Episode reward: 1126.5560607910156\n",
      "Total Steps: 38\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.901863  12.89084   12.863093  12.005877  11.9409075 12.898808 ]\n",
      "Reset environment\n",
      "Episode reward: 2673.729077875614\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.903258  12.892234  12.864499  12.0074005 11.942158  12.900204 ]\n",
      "Reset environment\n",
      "Episode reward: 4632.946346223354\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.905895 12.894875 12.867135 12.01024  11.944539 12.902845]\n",
      "Reset environment\n",
      "Episode reward: 2679.2852152585983\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.907301 12.896286 12.868537 12.011777 11.945799 12.90425 ]\n",
      "Reset environment\n",
      "Episode reward: 541.9124736785889\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.906729  12.895775  12.8679085 12.010976  11.945322  12.903681 ]\n",
      "Reset environment\n",
      "Episode reward: 382.0237714648247\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.905872 12.894835 12.86715  12.010019 11.944597 12.902827]\n",
      "Reset environment\n",
      "Episode reward: 2555.909336162731\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.90713  12.896128 12.868374 12.011418 11.945745 12.904084]\n",
      "Reset environment\n",
      "Episode reward: 4986.101142287254\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.909823  12.898825  12.8710575 12.014317  11.948125  12.906777 ]\n",
      "Reset environment\n",
      "Episode reward: 1922.2869880795479\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.910726 12.899704 12.871984 12.015306 11.948933 12.90768 ]\n",
      "Reset environment\n",
      "Episode reward: 3531.7005420029163\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.912651 12.901637 12.8739   12.017396 11.950654 12.909602]\n",
      "Reset environment\n",
      "Episode reward: 2799.728705227375\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.914141 12.903107 12.875398 12.019009 11.951983 12.911095]\n",
      "Reset environment\n",
      "Episode reward: 3541.2675506174564\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.916072 12.905064 12.877289 12.021102 11.953729 12.913028]\n",
      "Reset environment\n",
      "Episode reward: 2601.2566848397255\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.917441 12.906445 12.878641 12.0226   11.954972 12.914395]\n",
      "Reset environment\n",
      "Episode reward: 3249.7285735309124\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.919148 12.908139 12.880368 12.024446 11.956503 12.916104]\n",
      "Reset environment\n",
      "Episode reward: 1708.314493983984\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.919944 12.908913 12.881185 12.025322 11.957211 12.916898]\n",
      "Reset environment\n",
      "Episode reward: 1259.5585986375809\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.920069 12.908997 12.881369 12.025368 11.957417 12.917031]\n",
      "Reset environment\n",
      "Episode reward: 2499.76858741045\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.921788  12.910719  12.8830805 12.0272455 11.958943  12.91875  ]\n",
      "Reset environment\n",
      "Episode reward: 2019.277205169201\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.922773 12.911713 12.884059 12.028326 11.959838 12.919735]\n",
      "Reset environment\n",
      "Episode reward: 1319.9787746816874\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.922653 12.911515 12.884018 12.028182 11.959758 12.919618]\n",
      "Reset environment\n",
      "Episode reward: 4169.08835542202\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.925005 12.913859 12.886361 12.030715 11.961854 12.921969]\n",
      "Reset environment\n",
      "Episode reward: -284.24112233519554\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.923356 12.912072 12.884852 12.028993 11.960351 12.92033 ]\n",
      "Reset environment\n",
      "Episode reward: 2842.8418402671814\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.924853 12.913555 12.88636  12.030618 11.961675 12.921828]\n",
      "Reset environment\n",
      "Episode reward: 5430.615326106548\n",
      "Total Steps: 185\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.927995 12.916685 12.889485 12.033973 11.964501 12.924965]\n",
      "Reset environment\n",
      "Episode reward: 1860.1649010181427\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.928886 12.917573 12.890384 12.03497  11.965301 12.925857]\n",
      "Reset environment\n",
      "Episode reward: 4392.678085327148\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.931323  12.919992  12.892834  12.037591  11.9675045 12.928298 ]\n",
      "Reset environment\n",
      "Episode reward: 3188.840612322092\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.932869 12.921589 12.894326 12.039293 11.968888 12.929846]\n",
      "Reset environment\n",
      "Episode reward: 3533.708690881729\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.934806 12.923521 12.896264 12.041384 11.97062  12.931781]\n",
      "Reset environment\n",
      "Episode reward: 3221.1483621001244\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.936484 12.925183 12.897958 12.043214 11.972117 12.93346 ]\n",
      "Reset environment\n",
      "Episode reward: 2636.8485071361065\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.937809 12.926488 12.899302 12.044653 11.973307 12.934783]\n",
      "Reset environment\n",
      "Episode reward: -326.958700299263\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.936499  12.925166  12.897989  12.043392  11.9720545 12.933473 ]\n",
      "Reset environment\n",
      "Episode reward: 1489.6243604421616\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.93762  12.926287 12.899113 12.044622 11.973051 12.934593]\n",
      "Reset environment\n",
      "Episode reward: 4049.250942468643\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.939852 12.92853  12.90134  12.047021 11.975046 12.936826]\n",
      "Reset environment\n",
      "Episode reward: 4817.116780936718\n",
      "Total Steps: 166\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.942548 12.931208 12.904047 12.04992  11.977472 12.939522]\n",
      "Reset environment\n",
      "Episode reward: 4397.926167070866\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.94505  12.933701 12.906551 12.052593 11.979711 12.942024]\n",
      "Reset environment\n",
      "Episode reward: 4249.2922593951225\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.947308 12.935957 12.90881  12.055025 11.981709 12.944283]\n",
      "Reset environment\n",
      "Episode reward: 1414.0020334720612\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.948347 12.936998 12.909851 12.056168 11.982623 12.945322]\n",
      "Reset environment\n",
      "Episode reward: 3695.4062715172768\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.950354 12.939008 12.911846 12.058324 11.984428 12.947325]\n",
      "Reset environment\n",
      "Episode reward: 2234.9647097587585\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.951525  12.9401865 12.913008  12.059601  11.985505  12.948495 ]\n",
      "Reset environment\n",
      "Episode reward: 5663.928436994553\n",
      "Total Steps: 191\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.954804 12.943463 12.916275 12.063101 11.98848  12.951772]\n",
      "Reset environment\n",
      "Episode reward: 4016.4679690375924\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.956984  12.9456835 12.918403  12.065463  11.990462  12.953952 ]\n",
      "Reset environment\n",
      "Episode reward: 5051.593027949333\n",
      "Total Steps: 172\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.959875 12.948567 12.921286 12.068549 11.993037 12.956844]\n",
      "Reset environment\n",
      "Episode reward: 1476.401922404766\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.960981 12.949675 12.92239  12.06977  11.994029 12.957951]\n",
      "Reset environment\n",
      "Episode reward: 2621.30028373003\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.962328 12.951013 12.923742 12.071238 11.995227 12.959297]\n",
      "Reset environment\n",
      "Episode reward: 2001.6740425229073\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.963726 12.952424 12.925134 12.072774 11.996485 12.960699]\n",
      "Reset environment\n",
      "Episode reward: 5697.668689608574\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.967015 12.955714 12.928411 12.07627  11.999451 12.963992]\n",
      "Reset environment\n",
      "Episode reward: 3705.4100307822227\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.969029 12.95771  12.930434 12.078427 12.001259 12.966004]\n",
      "Reset environment\n",
      "Episode reward: 1726.3054267168045\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.970248 12.958929 12.931656 12.079767 12.002343 12.967223]\n",
      "Reset environment\n",
      "Episode reward: 2344.8356543779373\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.971432 12.960119 12.932832 12.081076 12.003397 12.968405]\n",
      "Reset environment\n",
      "Episode reward: 1551.9171482920647\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.972151 12.960843 12.933548 12.081874 12.00405  12.969124]\n",
      "Reset environment\n",
      "Episode reward: 2662.1497014164925\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.973533 12.96221  12.934948 12.083369 12.005296 12.970508]\n",
      "Reset environment\n",
      "Episode reward: 1368.0991567373276\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.974511 12.963186 12.935923 12.084456 12.006151 12.971486]\n",
      "Reset environment\n",
      "Episode reward: 4277.745463728905\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.976891 12.965571 12.938295 12.087012 12.008281 12.973867]\n",
      "Reset environment\n",
      "Episode reward: 3434.7949455976486\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.978655 12.967335 12.94006  12.08892  12.009849 12.975633]\n",
      "Reset environment\n",
      "Episode reward: 1783.4522881507874\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.979495 12.968187 12.940894 12.08986  12.010614 12.976472]\n",
      "Reset environment\n",
      "Episode reward: 1449.6050493121147\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.980169 12.968863 12.941562 12.090608 12.011229 12.977144]\n",
      "Reset environment\n",
      "Episode reward: 4686.058544516563\n",
      "Total Steps: 159\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.982818 12.971517 12.944212 12.093453 12.013622 12.979797]\n",
      "Reset environment\n",
      "Episode reward: 3475.897893309593\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.984606  12.973307  12.946003  12.0953865 12.015223  12.981586 ]\n",
      "Reset environment\n",
      "Episode reward: 2159.2591204047203\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.986102 12.974796 12.947508 12.097007 12.016557 12.983084]\n",
      "Reset environment\n",
      "Episode reward: 4859.41661965847\n",
      "Total Steps: 165\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.988857  12.977556  12.9502735 12.0999565 12.01903   12.985847 ]\n",
      "Reset environment\n",
      "Episode reward: 1408.7112669944763\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.989886  12.978585  12.9513035 12.10109   12.019948  12.986877 ]\n",
      "Reset environment\n",
      "Episode reward: 2002.165995389223\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.99036   12.9790945 12.951747  12.101549  12.020392  12.987357 ]\n",
      "Reset environment\n",
      "Episode reward: 2565.2585338056087\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.991583 12.980357 12.952926 12.102909 12.021495 12.98858 ]\n",
      "Reset environment\n",
      "Episode reward: 1331.3299841880798\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.991859 12.980659 12.953169 12.103121 12.021821 12.988859]\n",
      "Reset environment\n",
      "Episode reward: 4943.458724856377\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.994669 12.983463 12.955978 12.106129 12.024347 12.991662]\n",
      "Reset environment\n",
      "Episode reward: 3619.445825576782\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.99663  12.985428 12.957917 12.108254 12.026128 12.99362 ]\n",
      "Reset environment\n",
      "Episode reward: 1356.2760461568832\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.997615 12.986415 12.958902 12.109349 12.027002 12.994604]\n",
      "Reset environment\n",
      "Episode reward: 1909.61225181818\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.99855  12.987344 12.959841 12.110375 12.027841 12.995542]\n",
      "Reset environment\n",
      "Episode reward: 3016.60452093184\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.999875 12.988617 12.961222 12.111856 12.029078 12.996867]\n",
      "Reset environment\n",
      "Episode reward: 1152.4501734972\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.999739 12.988552 12.96101  12.111577 12.028966 12.996735]\n",
      "Reset environment\n",
      "Episode reward: 2003.5994857549667\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.001132 12.989953 12.962395 12.1131   12.030217 12.998132]\n",
      "Reset environment\n",
      "Episode reward: -681.982324719429\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [12.999409  12.9882345 12.960675  12.111282  12.028633  12.996416 ]\n",
      "Reset environment\n",
      "Episode reward: 3581.554398596287\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.001293  12.990099  12.96258   12.113326  12.030333  12.9983015]\n",
      "Reset environment\n",
      "Episode reward: 1840.5340297222137\n",
      "Total Steps: 64\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.00215   12.990939  12.963448  12.114266  12.0310955 12.99916  ]\n",
      "Reset environment\n",
      "Episode reward: 2748.859925508499\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.003413 12.992175 12.964739 12.11566  12.032236 13.000423]\n",
      "Reset environment\n",
      "Episode reward: 1420.8394294977188\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.004455 12.993217 12.965778 12.116815 12.033158 13.001464]\n",
      "Reset environment\n",
      "Episode reward: 2554.162260055542\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.005776 12.994549 12.967095 12.118254 12.034348 13.002788]\n",
      "Reset environment\n",
      "Episode reward: 2661.492537677288\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.007163 12.995917 12.968495 12.119764 12.035587 13.004172]\n",
      "Reset environment\n",
      "Episode reward: 6136.022929430008\n",
      "Total Steps: 208\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.010546  12.9992895 12.971895  12.123411  12.038606  13.00756  ]\n",
      "Reset environment\n",
      "Episode reward: -412.2535526752472\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.009167  12.9979725 12.970443  12.121838  12.037349  13.0061865]\n",
      "Reset environment\n",
      "Episode reward: 1922.7957753539085\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.010087 12.998896 12.971367 12.122861 12.038168 13.007108]\n",
      "Reset environment\n",
      "Episode reward: 2122.993553251028\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.011068 12.999851 12.972364 12.123951 12.03904  13.008088]\n",
      "Reset environment\n",
      "Episode reward: 5468.324224293232\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.01419   13.0029745 12.97548   12.127287  12.04186   13.011216 ]\n",
      "Reset environment\n",
      "Episode reward: 2493.0173405865207\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.015361 13.004127 12.976676 12.128567 12.042907 13.01239 ]\n",
      "Reset environment\n",
      "Episode reward: 2613.9612073004246\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.016277 13.004955 12.977685 12.129562 12.04377  13.013315]\n",
      "Reset environment\n",
      "Episode reward: 1889.8315110206604\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.017583  13.0062685 12.978985  12.130994  12.044923  13.014623 ]\n",
      "Reset environment\n",
      "Episode reward: 2321.314931809902\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.018714  13.007406  12.980107  12.1322365 12.04592   13.015753 ]\n",
      "Reset environment\n",
      "Episode reward: 1828.2901217341423\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0199995 13.008689  12.981395  12.133637  12.047057  13.017038 ]\n",
      "Reset environment\n",
      "Episode reward: 1600.037493109703\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.020749 13.009426 12.982161 12.134469 12.047729 13.017789]\n",
      "Reset environment\n",
      "Episode reward: 5441.065935492516\n",
      "Total Steps: 183\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.02387  13.012542 12.985274 12.137806 12.05055  13.020909]\n",
      "Reset environment\n",
      "Episode reward: 3942.7554609775543\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.026028 13.014698 12.987438 12.140137 12.052482 13.023069]\n",
      "Reset environment\n",
      "Episode reward: 2591.9259709715843\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.02734  13.01602  12.988739 12.141589 12.05365  13.024383]\n",
      "Reset environment\n",
      "Episode reward: 1739.8732137680054\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.028571 13.017252 12.989968 12.142936 12.054727 13.025612]\n",
      "Reset environment\n",
      "Episode reward: 2062.041966021061\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.029981 13.018663 12.991376 12.14448  12.055995 13.027021]\n",
      "Reset environment\n",
      "Episode reward: 2047.4898251891136\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.031389 13.020075 12.992781 12.14602  12.057252 13.02843 ]\n",
      "Reset environment\n",
      "Episode reward: 2767.005844324827\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.032809 13.021479 12.994218 12.147557 12.058531 13.02985 ]\n",
      "Reset environment\n",
      "Episode reward: 5064.341860592365\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.035689  13.024344  12.997094  12.150625  12.0611105 13.032735 ]\n",
      "Reset environment\n",
      "Episode reward: 5858.601848036051\n",
      "Total Steps: 199\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.039027  13.0276575 13.000438  12.154214  12.064152  13.036071 ]\n",
      "Reset environment\n",
      "Episode reward: 4423.103797376156\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.041561 13.0302   13.002965 12.156921 12.066429 13.038605]\n",
      "Reset environment\n",
      "Episode reward: 3861.4045798182487\n",
      "Total Steps: 133\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.043641  13.032269  13.005054  12.159169  12.068287  13.0406885]\n",
      "Reset environment\n",
      "Episode reward: 3465.0794745385647\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.045509 13.034151 13.006913 12.161182 12.069965 13.042557]\n",
      "Reset environment\n",
      "Episode reward: 2458.6489574611187\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.046707 13.035321 13.008135 12.16249  12.071039 13.043757]\n",
      "Reset environment\n",
      "Episode reward: 2972.148794710636\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.04825   13.036859  13.009683  12.16417   12.0724125 13.045298 ]\n",
      "Reset environment\n",
      "Episode reward: 1571.4384816884995\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.049345  13.037956  13.0107765 12.165384  12.073385  13.046393 ]\n",
      "Reset environment\n",
      "Episode reward: 3804.6753707528114\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.051415  13.039997  13.012861  12.167612  12.0752325 13.048466 ]\n",
      "Reset environment\n",
      "Episode reward: 2153.8209159318358\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.05224  13.040783 13.013728 12.168564 12.075965 13.049293]\n",
      "Reset environment\n",
      "Episode reward: 2092.4739756584167\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.053684 13.042234 13.015167 12.170139 12.077252 13.050737]\n",
      "Reset environment\n",
      "Episode reward: 1932.4018990397453\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.055006 13.043556 13.016487 12.171595 12.078414 13.052062]\n",
      "Reset environment\n",
      "Episode reward: 3129.6634026169777\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0566635 13.045226  13.018123  12.173399  12.079906  13.053716 ]\n",
      "Reset environment\n",
      "Episode reward: 2737.211917579174\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.058376 13.046935 13.01984  12.175251 12.081442 13.055426]\n",
      "Reset environment\n",
      "Episode reward: 3292.656047716737\n",
      "Total Steps: 174\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0599575 13.048511  13.021421  12.176994  12.082937  13.057012 ]\n",
      "Reset environment\n",
      "Episode reward: 1708.1215254962444\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.060645 13.04922  13.022067 12.177682 12.083624 13.057704]\n",
      "Reset environment\n",
      "Episode reward: 1448.4334166049957\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.061297 13.04986  13.022734 12.178413 12.084209 13.058356]\n",
      "Reset environment\n",
      "Episode reward: 2213.9920539855957\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.062822 13.051388 13.024257 12.180063 12.085588 13.059879]\n",
      "Reset environment\n",
      "Episode reward: 4786.464671432972\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.065498 13.054064 13.026922 12.182935 12.087991 13.062556]\n",
      "Reset environment\n",
      "Episode reward: 2171.5921525359154\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.066542 13.055114 13.027965 12.18409  12.088929 13.063602]\n",
      "Reset environment\n",
      "Episode reward: 1902.3056611418724\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0674715 13.0560465 13.028894  12.185119  12.089763  13.064534 ]\n",
      "Reset environment\n",
      "Episode reward: 2127.0118241906166\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.068508 13.057098 13.029902 12.186253 12.090707 13.065573]\n",
      "Reset environment\n",
      "Episode reward: 3069.7054964900017\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.070115 13.058711 13.031501 12.187984 12.092153 13.067182]\n",
      "Reset environment\n",
      "Episode reward: 1413.5522584319115\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0711355 13.059734  13.032522  12.189106  12.093054  13.0682   ]\n",
      "Reset environment\n",
      "Episode reward: 2217.728140592575\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.072655 13.061253 13.034043 12.190745 12.094403 13.069722]\n",
      "Reset environment\n",
      "Episode reward: 4137.549867659807\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.07491  13.063545 13.036265 12.193199 12.096439 13.071976]\n",
      "Reset environment\n",
      "Episode reward: 2216.099350452423\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.076433  13.0650625 13.037792  12.194837  12.097801  13.073499 ]\n",
      "Reset environment\n",
      "Episode reward: 2149.786182284355\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.077511 13.066154 13.038864 12.196022 12.098786 13.074577]\n",
      "Reset environment\n",
      "Episode reward: 1908.3771181702614\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.078822 13.067467 13.040177 12.19747  12.099962 13.075891]\n",
      "Reset environment\n",
      "Episode reward: 2129.4378203749657\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.080276 13.068916 13.041635 12.19904  12.101261 13.077345]\n",
      "Reset environment\n",
      "Episode reward: 1911.7992420345545\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.081094  13.069703  13.04249   12.1999445 12.101991  13.078166 ]\n",
      "Reset environment\n",
      "Episode reward: 2087.153182208538\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.082087  13.070708  13.043475  12.2010355 12.102867  13.079159 ]\n",
      "Reset environment\n",
      "Episode reward: 4442.114629983902\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.084533 13.073146 13.045933 12.203658 12.105075 13.081603]\n",
      "Reset environment\n",
      "Episode reward: 4711.15978205204\n",
      "Total Steps: 161\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.087008  13.075625  13.048412  12.20632   12.107286  13.0840845]\n",
      "Reset environment\n",
      "Episode reward: 814.6853754520416\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.086824 13.075386 13.048299 12.20601  12.107224 13.083908]\n",
      "Reset environment\n",
      "Episode reward: 1335.0305885076523\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.087762 13.076327 13.049235 12.20707  12.108052 13.084846]\n",
      "Reset environment\n",
      "Episode reward: 1161.004802942276\n",
      "Total Steps: 36\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.088266 13.076834 13.049739 12.207642 12.108508 13.085352]\n",
      "Reset environment\n",
      "Episode reward: 5294.8086431622505\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.091243 13.079786 13.052721 12.210825 12.111167 13.088328]\n",
      "Reset environment\n",
      "Episode reward: 673.0885344743729\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.0909605 13.079545  13.052398  12.210403  12.110945  13.088052 ]\n",
      "Reset environment\n",
      "Episode reward: 2217.1067415475845\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.092013  13.080587  13.053472  12.211561  12.111882  13.0891075]\n",
      "Reset environment\n",
      "Episode reward: 185.8620992898941\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.090899 13.079356 13.052488 12.210319 12.110905 13.088011]\n",
      "Reset environment\n",
      "Episode reward: 2764.9602676033974\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.092261 13.080694 13.053869 12.211795 12.112118 13.089371]\n",
      "Reset environment\n",
      "Episode reward: 2370.0942809581757\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.093463  13.0819025 13.05507   12.213102  12.113208  13.090573 ]\n",
      "Reset environment\n",
      "Episode reward: 1336.9848980903625\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.093849  13.082336  13.05541   12.213568  12.113541  13.0909605]\n",
      "Reset environment\n",
      "Episode reward: -318.86596035957336\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.092577  13.0810995 13.054078  12.212087  12.112405  13.089696 ]\n",
      "Reset environment\n",
      "Episode reward: 5781.073324024677\n",
      "Total Steps: 197\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.09581  13.084316 13.057325 12.215551 12.115332 13.092932]\n",
      "Reset environment\n",
      "Episode reward: 1722.4623846411705\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.09664   13.085134  13.0581665 12.216465  12.116075  13.093761 ]\n",
      "Reset environment\n",
      "Episode reward: 3642.854851961136\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.098551 13.087072 13.060051 12.218539 12.117811 13.095673]\n",
      "Reset environment\n",
      "Episode reward: 1503.176639199257\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.098958 13.087535 13.060405 12.219057 12.11814  13.096084]\n",
      "Reset environment\n",
      "Episode reward: 1429.1158697903156\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.099315 13.087847 13.060804 12.219363 12.118504 13.096448]\n",
      "Reset environment\n",
      "Episode reward: 2253.7428171038628\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.100819 13.089358 13.062307 12.22101  12.119853 13.097954]\n",
      "Reset environment\n",
      "Episode reward: 2194.0351010262966\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.101846 13.090367 13.063353 12.222136 12.120773 13.098977]\n",
      "Reset environment\n",
      "Episode reward: 2224.335688456893\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.102855  13.091347  13.0643835 12.223252  12.121659  13.099985 ]\n",
      "Reset environment\n",
      "Episode reward: 1665.4191833734512\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.103998 13.092496 13.065529 12.224515 12.122674 13.101131]\n",
      "Reset environment\n",
      "Episode reward: 1639.307609796524\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.104781 13.093293 13.066297 12.225373 12.123398 13.101912]\n",
      "Reset environment\n",
      "Episode reward: 3165.7069897055626\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.106346 13.094825 13.067897 12.227065 12.124794 13.103475]\n",
      "Reset environment\n",
      "Episode reward: 1840.794106900692\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.106682 13.095192 13.068208 12.227339 12.125131 13.103822]\n",
      "Reset environment\n",
      "Episode reward: 4811.679510593414\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.109382 13.097882 13.070903 12.230222 12.127549 13.10652 ]\n",
      "Reset environment\n",
      "Episode reward: 1950.4040959179401\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.109974 13.098497 13.071464 12.230821 12.128109 13.107112]\n",
      "Reset environment\n",
      "Episode reward: 1378.5553014874458\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.11095   13.099473  13.072441  12.231901  12.1289835 13.108088 ]\n",
      "Reset environment\n",
      "Episode reward: 1919.3962591290474\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.11225  13.100771 13.073744 12.23333  12.130127 13.109389]\n",
      "Reset environment\n",
      "Episode reward: 1867.9460349082947\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.11317  13.101699 13.074652 12.234352 12.130967 13.110309]\n",
      "Reset environment\n",
      "Episode reward: 6125.122979104519\n",
      "Total Steps: 206\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.116685 13.105203 13.078157 12.2381   12.13412  13.113823]\n",
      "Reset environment\n",
      "Episode reward: 3089.001589477062\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.118329 13.106856 13.079793 12.239873 12.13559  13.115465]\n",
      "Reset environment\n",
      "Episode reward: 3615.1445821523666\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1201515 13.108679  13.081612  12.241842  12.137216  13.117289 ]\n",
      "Reset environment\n",
      "Episode reward: 1482.8209347128868\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.120815 13.109328 13.082292 12.242583 12.137822 13.117953]\n",
      "Reset environment\n",
      "Episode reward: 2373.593730792403\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.121916  13.110445  13.0833645 12.2438    12.138802  13.119053 ]\n",
      "Reset environment\n",
      "Episode reward: 3145.4746682029217\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.123501  13.112058  13.0849085 12.245528  12.140214  13.120641 ]\n",
      "Reset environment\n",
      "Episode reward: 5035.637992203236\n",
      "Total Steps: 170\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1263275 13.114873  13.087738  12.248552  12.142756  13.12347  ]\n",
      "Reset environment\n",
      "Episode reward: 2300.620852291584\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.127877 13.11642  13.089288 12.250228 12.14413  13.125018]\n",
      "Reset environment\n",
      "Episode reward: 2967.257584619336\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.129245  13.117814  13.0906105 12.251726  12.14534   13.126385 ]\n",
      "Reset environment\n",
      "Episode reward: 4037.4257802069187\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.131356 13.119904 13.092754 12.254019 12.147251 13.1285  ]\n",
      "Reset environment\n",
      "Episode reward: 1966.869897633791\n",
      "Total Steps: 68\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.132273 13.120803 13.093692 12.255023 12.148074 13.129416]\n",
      "Reset environment\n",
      "Episode reward: 4105.350909471512\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.134572 13.12311  13.095965 12.257509 12.150178 13.131716]\n",
      "Reset environment\n",
      "Episode reward: 4632.899735927582\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.137134 13.125663 13.098533 12.260259 12.152458 13.134275]\n",
      "Reset environment\n",
      "Episode reward: 2153.6992796063423\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.138597 13.127125 13.100004 12.261845 12.153762 13.135738]\n",
      "Reset environment\n",
      "Episode reward: 329.19298577308655\n",
      "Total Steps: 11\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.138564 13.127095 13.099967 12.26185  12.153718 13.135706]\n",
      "Reset environment\n",
      "Episode reward: 3927.997295796871\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.140659 13.129188 13.102065 12.264113 12.155589 13.137805]\n",
      "Reset environment\n",
      "Episode reward: 1864.6881922483444\n",
      "Total Steps: 65\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.141513 13.130047 13.102915 12.265063 12.156367 13.138661]\n",
      "Reset environment\n",
      "Episode reward: 3.127415180206299\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.140323  13.128726  13.101866  12.263778  12.155294  13.1374855]\n",
      "Reset environment\n",
      "Episode reward: 2215.7031912207603\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.141831  13.13024   13.1033745 12.265414  12.156656  13.138998 ]\n",
      "Reset environment\n",
      "Episode reward: -179.57261714339256\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.14053  13.128853 13.10218  12.264064 12.155495 13.137705]\n",
      "Reset environment\n",
      "Episode reward: 4033.6731758117676\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1426935 13.131003  13.104352  12.266391  12.157428  13.139872 ]\n",
      "Reset environment\n",
      "Episode reward: 1189.1930866837502\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.14261  13.130861 13.104324 12.26621  12.157408 13.139799]\n",
      "Reset environment\n",
      "Episode reward: 5613.210985779762\n",
      "Total Steps: 192\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.145767  13.13402   13.107464  12.2695675 12.160231  13.142957 ]\n",
      "Reset environment\n",
      "Episode reward: 2686.698129981756\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.147084 13.135315 13.108804 12.270991 12.161407 13.144276]\n",
      "Reset environment\n",
      "Episode reward: 3751.014012992382\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1490555 13.137275  13.110792  12.273115  12.163183  13.146249 ]\n",
      "Reset environment\n",
      "Episode reward: 4461.348758369684\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.151489 13.139707 13.113206 12.275724 12.165354 13.148674]\n",
      "Reset environment\n",
      "Episode reward: 1989.386331582442\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.152363 13.140552 13.114107 12.276692 12.166135 13.149548]\n",
      "Reset environment\n",
      "Episode reward: 2100.454330097884\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.153285  13.141448  13.115053  12.277705  12.1669445 13.150471 ]\n",
      "Reset environment\n",
      "Episode reward: 3283.3907256424427\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.154976 13.143135 13.116759 12.27955  12.168463 13.152172]\n",
      "Reset environment\n",
      "Episode reward: 5735.760938167572\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.158225 13.146375 13.119993 12.283001 12.171376 13.155418]\n",
      "Reset environment\n",
      "Episode reward: 4183.914618909359\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.160494  13.148644  13.122246  12.285439  12.173408  13.1576805]\n",
      "Reset environment\n",
      "Episode reward: -339.7215172275901\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.158518 13.146548 13.120396 12.283531 12.171603 13.15571 ]\n",
      "Reset environment\n",
      "Episode reward: 4214.246180593967\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1608   13.14883  13.122667 12.285974 12.173624 13.157988]\n",
      "Reset environment\n",
      "Episode reward: 5868.1014534235\n",
      "Total Steps: 202\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.163892 13.151927 13.125768 12.289333 12.176394 13.16108 ]\n",
      "Reset environment\n",
      "Episode reward: 2562.675576746464\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.16513   13.153152  13.12702   12.290689  12.1774845 13.162317 ]\n",
      "Reset environment\n",
      "Episode reward: -338.90004086494446\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.163803 13.151824 13.125696 12.289456 12.176257 13.160991]\n",
      "Reset environment\n",
      "Episode reward: 4983.019283354282\n",
      "Total Steps: 171\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1665535 13.154588  13.128444  12.2924    12.178725  13.163743 ]\n",
      "Reset environment\n",
      "Episode reward: 2362.9875578284264\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.167713 13.155753 13.129595 12.293663 12.179745 13.164901]\n",
      "Reset environment\n",
      "Episode reward: 3852.0488743185997\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.169754 13.157795 13.131624 12.295864 12.18158  13.166943]\n",
      "Reset environment\n",
      "Episode reward: 4603.973279953003\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.17224  13.160271 13.134114 12.298535 12.183843 13.169424]\n",
      "Reset environment\n",
      "Episode reward: 1682.5282185673714\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.173408 13.161439 13.135282 12.299814 12.184877 13.170591]\n",
      "Reset environment\n",
      "Episode reward: 1314.5658093690872\n",
      "Total Steps: 42\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.173989 13.162019 13.135865 12.300475 12.185399 13.171172]\n",
      "Reset environment\n",
      "Episode reward: 1679.3593771904707\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.174561 13.162547 13.136487 12.30114  12.185933 13.171743]\n",
      "Reset environment\n",
      "Episode reward: 2312.3847372233868\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.175626  13.16364   13.1375265 12.30232   12.186906  13.172806 ]\n",
      "Reset environment\n",
      "Episode reward: 3202.9987363815308\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.17731  13.165328 13.139201 12.30413  12.1884   13.174492]\n",
      "Reset environment\n",
      "Episode reward: 5289.932603478432\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.180249 13.16825  13.142136 12.307277 12.191029 13.177433]\n",
      "Reset environment\n",
      "Episode reward: 1722.8936748355627\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.180777 13.168825 13.142627 12.307915 12.191481 13.177963]\n",
      "Reset environment\n",
      "Episode reward: 1710.3307890892029\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.181513 13.169557 13.143367 12.308748 12.192136 13.1787  ]\n",
      "Reset environment\n",
      "Episode reward: 3000.0283948481083\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.183042 13.171066 13.144914 12.310405 12.193507 13.180229]\n",
      "Reset environment\n",
      "Episode reward: 4301.478505551815\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.18537  13.173392 13.147239 12.312919 12.195589 13.182556]\n",
      "Reset environment\n",
      "Episode reward: 2306.4157798290253\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.186904 13.174918 13.148777 12.314577 12.196955 13.184089]\n",
      "Reset environment\n",
      "Episode reward: 5171.537851035595\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.189773 13.177789 13.151648 12.317652 12.199542 13.186959]\n",
      "Reset environment\n",
      "Episode reward: 2938.1657208800316\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.191252  13.179275  13.15312   12.319255  12.200841  13.1884365]\n",
      "Reset environment\n",
      "Episode reward: 1685.1897690743208\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.191743  13.179808  13.153571  12.319854  12.2012615 13.18893  ]\n",
      "Reset environment\n",
      "Episode reward: 2098.093518257141\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.193163 13.181229 13.154996 12.321394 12.202511 13.190351]\n",
      "Reset environment\n",
      "Episode reward: 2801.301498681307\n",
      "Total Steps: 110\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.194395 13.182424 13.156274 12.322759 12.20367  13.191582]\n",
      "Reset environment\n",
      "Episode reward: 1730.0976250171661\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.1955805 13.183608  13.15746   12.3240595 12.204707  13.192768 ]\n",
      "Reset environment\n",
      "Episode reward: 2647.68756210804\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.19693  13.184958 13.158809 12.325523 12.205892 13.19412 ]\n",
      "Reset environment\n",
      "Episode reward: 2483.2104722857475\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.198159 13.18618  13.160051 12.326864 12.206983 13.19535 ]\n",
      "Reset environment\n",
      "Episode reward: 406.05703604221344\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.197297 13.185423 13.159093 12.325834 12.206207 13.194495]\n",
      "Reset environment\n",
      "Episode reward: 1534.0232439041138\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.197985 13.186104 13.159785 12.326605 12.206829 13.195183]\n",
      "Reset environment\n",
      "Episode reward: 4154.1705013513565\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.200249  13.1883745 13.162037  12.329023  12.2088375 13.197446 ]\n",
      "Reset environment\n",
      "Episode reward: 1416.2263797521591\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.201253 13.189383 13.16304  12.330126 12.209716 13.198453]\n",
      "Reset environment\n",
      "Episode reward: 1664.01235049963\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.201991 13.190107 13.163791 12.330947 12.210366 13.199191]\n",
      "Reset environment\n",
      "Episode reward: 2739.912735670805\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.203347 13.191446 13.165159 12.332423 12.211581 13.200547]\n",
      "Reset environment\n",
      "Episode reward: 2172.1501388549805\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.204292  13.192365  13.166131  12.3334675 12.21242   13.201492 ]\n",
      "Reset environment\n",
      "Episode reward: 1698.3821090459824\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.205079 13.193135 13.166936 12.334337 12.213139 13.202282]\n",
      "Reset environment\n",
      "Episode reward: 3308.7829774171114\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.206655 13.194687 13.168551 12.336061 12.214571 13.203861]\n",
      "Reset environment\n",
      "Episode reward: 2580.90990014188\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.207829 13.195894 13.169693 12.337366 12.215622 13.205035]\n",
      "Reset environment\n",
      "Episode reward: 2447.4278687238693\n",
      "Total Steps: 155\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.208769 13.196874 13.170589 12.338365 12.216481 13.205987]\n",
      "Reset environment\n",
      "Episode reward: 2029.7169253230095\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.209716 13.197819 13.171539 12.339418 12.217308 13.206934]\n",
      "Reset environment\n",
      "Episode reward: 1728.8369427919388\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.210526  13.198631  13.172344  12.340308  12.2180395 13.207746 ]\n",
      "Reset environment\n",
      "Episode reward: 2123.4220103025436\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.211947  13.2000475 13.173767  12.341854  12.219295  13.2091675]\n",
      "Reset environment\n",
      "Episode reward: 1777.1833702027798\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.212721 13.200798 13.17456  12.342709 12.219976 13.209941]\n",
      "Reset environment\n",
      "Episode reward: 3265.9141387045383\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.214338 13.202456 13.176117 12.344477 12.221423 13.211559]\n",
      "Reset environment\n",
      "Episode reward: 5263.702591717243\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.217235 13.205329 13.17902  12.347583 12.224028 13.214454]\n",
      "Reset environment\n",
      "Episode reward: 3674.504513949156\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.2191725 13.207289  13.180944  12.349671  12.225774  13.2163925]\n",
      "Reset environment\n",
      "Episode reward: -500.9645835161209\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.217415 13.2054   13.179329 12.347882 12.224162 13.214645]\n",
      "Reset environment\n",
      "Episode reward: 3342.5636469125748\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.219114 13.207083 13.181044 12.349719 12.225688 13.216342]\n",
      "Reset environment\n",
      "Episode reward: 3924.3643761873245\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.221189 13.209139 13.183129 12.351947 12.227535 13.218419]\n",
      "Reset environment\n",
      "Episode reward: 3009.1754912137985\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.222694  13.210637  13.1846485 12.353579  12.22889   13.219928 ]\n",
      "Reset environment\n",
      "Episode reward: 2601.426042638719\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.223735 13.211715 13.185654 12.354755 12.229823 13.220973]\n",
      "Reset environment\n",
      "Episode reward: 2060.7069495916367\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.225133 13.213116 13.18705  12.35629  12.231061 13.222373]\n",
      "Reset environment\n",
      "Episode reward: 2503.9633169472218\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.226304 13.214263 13.188243 12.357572 12.232103 13.223541]\n",
      "Reset environment\n",
      "Episode reward: 2474.0040444731712\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.227546  13.215498  13.189491  12.3589325 12.233215  13.224783 ]\n",
      "Reset environment\n",
      "Episode reward: 4177.765281617641\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.229832  13.2177925 13.191769  12.361393  12.235281  13.227071 ]\n",
      "Reset environment\n",
      "Episode reward: 1912.7468804121017\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.231102 13.21906  13.193044 12.362788 12.236388 13.228342]\n",
      "Reset environment\n",
      "Episode reward: 3155.220250993967\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.232523  13.220443  13.1945095 12.364369  12.237695  13.229767 ]\n",
      "Reset environment\n",
      "Episode reward: 1152.4581283330917\n",
      "Total Steps: 124\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.232504 13.220473 13.194443 12.364205 12.237727 13.229752]\n",
      "Reset environment\n",
      "Episode reward: 4790.688268661499\n",
      "Total Steps: 164\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.235116 13.223085 13.197055 12.367007 12.240072 13.232365]\n",
      "Reset environment\n",
      "Episode reward: 2941.2499490603805\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.236447 13.22438  13.198418 12.368471 12.241264 13.233699]\n",
      "Reset environment\n",
      "Episode reward: 3195.9332253932953\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.238061 13.226008 13.200013 12.37022  12.242722 13.235312]\n",
      "Reset environment\n",
      "Episode reward: 2402.4431216716766\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.239214  13.227145  13.201179  12.371471  12.243753  13.2364645]\n",
      "Reset environment\n",
      "Episode reward: 2764.5423577725887\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.240558 13.228477 13.20254  12.37293  12.244929 13.237809]\n",
      "Reset environment\n",
      "Episode reward: 2066.1042115688324\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.241947 13.229861 13.203934 12.374437 12.246153 13.239197]\n",
      "Reset environment\n",
      "Episode reward: 2335.736962854862\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.2435   13.23141  13.205491 12.376109 12.247544 13.240746]\n",
      "Reset environment\n",
      "Episode reward: 2082.1392017900944\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.244434  13.2323265 13.206452  12.377135  12.248384  13.24168  ]\n",
      "Reset environment\n",
      "Episode reward: 1977.2604905366898\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.245641 13.233534 13.20766  12.378459 12.249452 13.242887]\n",
      "Reset environment\n",
      "Episode reward: 1554.041553914547\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.246329 13.234209 13.208355 12.379221 12.250054 13.243573]\n",
      "Reset environment\n",
      "Episode reward: 2373.155646145344\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.247461 13.23533  13.209496 12.380463 12.251042 13.244706]\n",
      "Reset environment\n",
      "Episode reward: 3391.0173178315163\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.24921  13.237082 13.211235 12.38236  12.252585 13.246453]\n",
      "Reset environment\n",
      "Episode reward: 2084.0462854504585\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.250186 13.238057 13.212209 12.383442 12.25345  13.247431]\n",
      "Reset environment\n",
      "Episode reward: 4313.1442091465\n",
      "Total Steps: 148\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.252488  13.240366  13.2144985 12.385906  12.255501  13.249728 ]\n",
      "Reset environment\n",
      "Episode reward: 3011.9983573183417\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.25366  13.241483 13.215737 12.387258 12.256539 13.2509  ]\n",
      "Reset environment\n",
      "Episode reward: 2915.0745517611504\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.255044  13.242904  13.217077  12.388766  12.2577715 13.252282 ]\n",
      "Reset environment\n",
      "Episode reward: 1631.1085066199303\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.255772 13.243627 13.217813 12.389583 12.258428 13.253008]\n",
      "Reset environment\n",
      "Episode reward: 995.8415336608887\n",
      "Total Steps: 32\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.256141 13.243985 13.218193 12.390009 12.258754 13.25338 ]\n",
      "Reset environment\n",
      "Episode reward: 4085.8023478388786\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.258343 13.246174 13.220397 12.392371 12.260721 13.255584]\n",
      "Reset environment\n",
      "Episode reward: 2267.820861876011\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.259322 13.247115 13.221417 12.393452 12.261608 13.256565]\n",
      "Reset environment\n",
      "Episode reward: 1774.3739340305328\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.260173 13.247975 13.22226  12.39439  12.262384 13.257414]\n",
      "Reset environment\n",
      "Episode reward: 4605.669033765793\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.262661  13.250471  13.22473   12.397047  12.2645855 13.259898 ]\n",
      "Reset environment\n",
      "Episode reward: 3228.113285124302\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.26431  13.252125 13.226377 12.398833 12.266076 13.261549]\n",
      "Reset environment\n",
      "Episode reward: 2553.5939380526543\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.265528 13.253329 13.227616 12.400165 12.267172 13.26277 ]\n",
      "Reset environment\n",
      "Episode reward: 2096.3554636240005\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.266932 13.254738 13.229017 12.401688 12.268431 13.264174]\n",
      "Reset environment\n",
      "Episode reward: -886.3986073471606\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.264671 13.252637 13.226609 12.399342 12.266359 13.261929]\n",
      "Reset environment\n",
      "Episode reward: 2597.1410541534424\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.265923 13.253898 13.227857 12.400719 12.267482 13.263181]\n",
      "Reset environment\n",
      "Episode reward: 2861.127672493458\n",
      "Total Steps: 101\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.267309 13.255266 13.229259 12.402235 12.268722 13.264567]\n",
      "Reset environment\n",
      "Episode reward: 4013.65721565485\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.269444 13.257409 13.231367 12.404532 12.270616 13.266697]\n",
      "Reset environment\n",
      "Episode reward: 3525.564291000366\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.271272 13.259234 13.233194 12.406516 12.27225  13.268524]\n",
      "Reset environment\n",
      "Episode reward: 1944.003119468689\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.272564 13.260525 13.234493 12.407941 12.27338  13.269816]\n",
      "Reset environment\n",
      "Episode reward: 2250.274942457676\n",
      "Total Steps: 104\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.274028 13.261993 13.235951 12.409537 12.274674 13.27128 ]\n",
      "Reset environment\n",
      "Episode reward: 5378.16178381443\n",
      "Total Steps: 186\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.276813 13.264772 13.238741 12.412515 12.277176 13.274062]\n",
      "Reset environment\n",
      "Episode reward: 5490.076040625572\n",
      "Total Steps: 188\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.279808 13.26775  13.241738 12.41573  12.279839 13.277063]\n",
      "Reset environment\n",
      "Episode reward: 2675.3027040958405\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.280972 13.268957 13.24285  12.417027 12.28087  13.278224]\n",
      "Reset environment\n",
      "Episode reward: 3009.559188425541\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.282314 13.270267 13.244233 12.418504 12.282101 13.279572]\n",
      "Reset environment\n",
      "Episode reward: 2208.458715260029\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.283287  13.2712145 13.245227  12.419573  12.2829685 13.280545 ]\n",
      "Reset environment\n",
      "Episode reward: 5186.236979901791\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.286114 13.274045 13.248044 12.422602 12.285513 13.283377]\n",
      "Reset environment\n",
      "Episode reward: 4473.762721180916\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.2885   13.276423 13.250435 12.42516  12.287646 13.285766]\n",
      "Reset environment\n",
      "Episode reward: 2171.6943869292736\n",
      "Total Steps: 160\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.289036 13.276914 13.251032 12.425683 12.288194 13.286312]\n",
      "Reset environment\n",
      "Episode reward: 3301.2004796266556\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.290727 13.278595 13.252725 12.427511 12.289708 13.288004]\n",
      "Reset environment\n",
      "Episode reward: 1646.8566286563873\n",
      "Total Steps: 59\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.29141  13.279249 13.253439 12.428274 12.29034  13.288691]\n",
      "Reset environment\n",
      "Episode reward: 5564.582901299\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.294298 13.28213  13.256339 12.431394 12.292912 13.291578]\n",
      "Reset environment\n",
      "Episode reward: 1908.277992784977\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.295577 13.283409 13.25762  12.432797 12.294035 13.29286 ]\n",
      "Reset environment\n",
      "Episode reward: -81.70440715551376\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.29417  13.282113 13.256116 12.431276 12.292774 13.291464]\n",
      "Reset environment\n",
      "Episode reward: 2912.146966665983\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.295584  13.283511  13.25755   12.432809  12.2940445 13.292876 ]\n",
      "Reset environment\n",
      "Episode reward: 2777.119418606162\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.29687  13.284847 13.258805 12.434221 12.295208 13.294164]\n",
      "Reset environment\n",
      "Episode reward: 1932.2296983003616\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.297759  13.285737  13.259694  12.435203  12.295994  13.2950535]\n",
      "Reset environment\n",
      "Episode reward: 3713.1379051804543\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.299664 13.287646 13.261589 12.43727  12.297701 13.296956]\n",
      "Reset environment\n",
      "Episode reward: 2008.9106641411781\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.301004  13.288985  13.262933  12.43873   12.2988825 13.298298 ]\n",
      "Reset environment\n",
      "Episode reward: 4353.336578309536\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.303194  13.29118   13.265128  12.441096  12.3008375 13.300488 ]\n",
      "Reset environment\n",
      "Episode reward: 1751.8130301237106\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.303973 13.291946 13.265924 12.441964 12.301534 13.30127 ]\n",
      "Reset environment\n",
      "Episode reward: 2068.688215136528\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.3053255 13.293299  13.2672825 12.443443  12.302726  13.302621 ]\n",
      "Reset environment\n",
      "Episode reward: 3891.1040790081024\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.307365  13.2953415 13.269319  12.4456415 12.304562  13.304659 ]\n",
      "Reset environment\n",
      "Episode reward: 3302.246656268835\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.308871 13.296895 13.27078  12.447317 12.305906 13.306167]\n",
      "Reset environment\n",
      "Episode reward: 1491.9746597707272\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.309109 13.297089 13.271066 12.447508 12.306172 13.306405]\n",
      "Reset environment\n",
      "Episode reward: 1354.099331855774\n",
      "Total Steps: 43\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.309712 13.29769  13.271671 12.448182 12.306704 13.307009]\n",
      "Reset environment\n",
      "Episode reward: -158.24138963222504\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.308573 13.296609 13.270473 12.446878 12.305685 13.305873]\n",
      "Reset environment\n",
      "Episode reward: 3693.159793794155\n",
      "Total Steps: 130\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.310438 13.298451 13.272344 12.448893 12.307372 13.307734]\n",
      "Reset environment\n",
      "Episode reward: 2464.0277396440506\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.311556 13.299572 13.273462 12.450127 12.308368 13.308852]\n",
      "Reset environment\n",
      "Episode reward: 1415.6722392439842\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.312537  13.300553  13.2744465 12.451207  12.309234  13.309835 ]\n",
      "Reset environment\n",
      "Episode reward: 2146.11547267437\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.313522 13.301549 13.275428 12.452292 12.310133 13.310823]\n",
      "Reset environment\n",
      "Episode reward: 6408.529985964298\n",
      "Total Steps: 216\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.317093 13.305102 13.278993 12.456106 12.31335  13.314393]\n",
      "Reset environment\n",
      "Episode reward: 2186.16103720665\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.318145 13.306164 13.28003  12.45726  12.314315 13.315446]\n",
      "Reset environment\n",
      "Episode reward: 3037.02569013834\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.319657 13.307663 13.281552 12.458891 12.315657 13.316958]\n",
      "Reset environment\n",
      "Episode reward: 3878.428209900856\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.3216715 13.309658  13.283578  12.461052  12.31745   13.318974 ]\n",
      "Reset environment\n",
      "Episode reward: 1822.420530974865\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.32289   13.310883  13.284794  12.462394  12.3185215 13.320191 ]\n",
      "Reset environment\n",
      "Episode reward: 1542.0813208818436\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.324005 13.312    13.28591  12.463614 12.319536 13.321306]\n",
      "Reset environment\n",
      "Episode reward: 2121.619883775711\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.325394 13.313385 13.287299 12.465125 12.320764 13.322697]\n",
      "Reset environment\n",
      "Episode reward: 1805.7480444908142\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.326196  13.31419   13.2881    12.4660225 12.321471  13.323499 ]\n",
      "Reset environment\n",
      "Episode reward: 2677.5886991024017\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.327472 13.315446 13.289394 12.467406 12.322615 13.324774]\n",
      "Reset environment\n",
      "Episode reward: 2547.43169683218\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.32861  13.316588 13.290534 12.468662 12.323628 13.325912]\n",
      "Reset environment\n",
      "Episode reward: 5073.166905105114\n",
      "Total Steps: 175\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.331186 13.319163 13.293115 12.471448 12.32589  13.328487]\n",
      "Reset environment\n",
      "Episode reward: 1799.9011130332947\n",
      "Total Steps: 61\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.332     13.319991  13.293919  12.472348  12.3266115 13.3293   ]\n",
      "Reset environment\n",
      "Episode reward: 4050.981349527836\n",
      "Total Steps: 140\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.333991  13.3219795 13.29591   12.474501  12.328366  13.331292 ]\n",
      "Reset environment\n",
      "Episode reward: 2230.405669569969\n",
      "Total Steps: 75\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.335069 13.323065 13.29698  12.475673 12.329337 13.332373]\n",
      "Reset environment\n",
      "Episode reward: 1730.7588414549828\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.33544  13.323465 13.297326 12.475993 12.329696 13.332751]\n",
      "Reset environment\n",
      "Episode reward: 5176.455089390278\n",
      "Total Steps: 179\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.338211 13.326194 13.300097 12.478973 12.332146 13.335519]\n",
      "Reset environment\n",
      "Episode reward: 4742.014314889908\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.340728 13.328712 13.302607 12.481667 12.334442 13.338035]\n",
      "Reset environment\n",
      "Episode reward: 4937.38912409544\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.343388 13.331365 13.305274 12.484513 12.336838 13.340699]\n",
      "Reset environment\n",
      "Episode reward: 177.4284737110138\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.342625 13.330636 13.304464 12.483495 12.336169 13.339941]\n",
      "Reset environment\n",
      "Episode reward: 2004.0328536629677\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.34395  13.331962 13.305791 12.484945 12.337345 13.341268]\n",
      "Reset environment\n",
      "Episode reward: 1436.6690364480019\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.34459   13.33261   13.306425  12.485656  12.3379345 13.341908 ]\n",
      "Reset environment\n",
      "Episode reward: 1398.3254108428955\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.3455515 13.333575  13.307386  12.486727  12.338777  13.342872 ]\n",
      "Reset environment\n",
      "Episode reward: 601.4045480489731\n",
      "Total Steps: 118\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.345018 13.333104 13.306796 12.485977 12.338344 13.342339]\n",
      "Reset environment\n",
      "Episode reward: 1917.0470660924911\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.346288  13.33438   13.308059  12.4873705 12.339482  13.343608 ]\n",
      "Reset environment\n",
      "Episode reward: 1490.3222000598907\n",
      "Total Steps: 51\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.346917 13.335027 13.308661 12.488077 12.340061 13.344236]\n",
      "Reset environment\n",
      "Episode reward: 3020.8849523961544\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.348261 13.336337 13.310058 12.489559 12.341277 13.345582]\n",
      "Reset environment\n",
      "Episode reward: 1575.1460826396942\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.34893  13.337011 13.31072  12.49031  12.341883 13.34625 ]\n",
      "Reset environment\n",
      "Episode reward: 4054.8576688468456\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.351091 13.339181 13.31287  12.492623 12.343808 13.348411]\n",
      "Reset environment\n",
      "Episode reward: 2030.012350142002\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.352066 13.340168 13.31383  12.493696 12.344671 13.349386]\n",
      "Reset environment\n",
      "Episode reward: 2193.060660660267\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.353498  13.341606  13.315254  12.495255  12.3459635 13.350819 ]\n",
      "Reset environment\n",
      "Episode reward: 3285.1520964354277\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.355068 13.343145 13.316855 12.496952 12.347364 13.35239 ]\n",
      "Reset environment\n",
      "Episode reward: 1340.2952244281769\n",
      "Total Steps: 49\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.355561 13.343615 13.317376 12.497527 12.347808 13.352885]\n",
      "Reset environment\n",
      "Episode reward: 3501.4396671056747\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.357253 13.345308 13.319069 12.499363 12.349299 13.354573]\n",
      "Reset environment\n",
      "Episode reward: 1722.2582658529282\n",
      "Total Steps: 84\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.358399 13.346458 13.320211 12.500627 12.3503   13.355718]\n",
      "Reset environment\n",
      "Episode reward: 2663.2391496896744\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.359676 13.34772  13.3215   12.502021 12.351441 13.356995]\n",
      "Reset environment\n",
      "Episode reward: -682.3623996973038\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.357924 13.34595  13.319791 12.500237 12.349881 13.355246]\n",
      "Reset environment\n",
      "Episode reward: 5705.230961024761\n",
      "Total Steps: 194\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.361027 13.349034 13.322894 12.50356  12.35267  13.358354]\n",
      "Reset environment\n",
      "Episode reward: 1857.8540366590023\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.3614855 13.349458  13.3234005 12.504007  12.353145  13.358822 ]\n",
      "Reset environment\n",
      "Episode reward: 1392.7731531262398\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.362437 13.35041  13.324355 12.50506  12.353988 13.359774]\n",
      "Reset environment\n",
      "Episode reward: 2207.0683901309967\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.363455 13.351423 13.32538  12.50618  12.354889 13.360793]\n",
      "Reset environment\n",
      "Episode reward: 1352.118676006794\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.364352 13.352321 13.326281 12.507183 12.355689 13.361692]\n",
      "Reset environment\n",
      "Episode reward: 4242.101768434048\n",
      "Total Steps: 145\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.366601  13.3545685 13.3285055 12.509593  12.35771   13.363939 ]\n",
      "Reset environment\n",
      "Episode reward: 2952.7535366266966\n",
      "Total Steps: 120\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.367776 13.355689 13.329735 12.51093  12.358773 13.36511 ]\n",
      "Reset environment\n",
      "Episode reward: 2170.9484035372734\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.368806 13.356726 13.330761 12.512057 12.359713 13.366141]\n",
      "Reset environment\n",
      "Episode reward: 1911.4593167304993\n",
      "Total Steps: 67\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.369643 13.357545 13.331618 12.512983 12.360461 13.366979]\n",
      "Reset environment\n",
      "Episode reward: 4602.79382455349\n",
      "Total Steps: 157\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.372101 13.36001  13.334071 12.515609 12.36267  13.369439]\n",
      "Reset environment\n",
      "Episode reward: 2857.856856331229\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.373256 13.361115 13.335274 12.516911 12.363713 13.37059 ]\n",
      "Reset environment\n",
      "Episode reward: 1620.8091767430305\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.373971 13.36182  13.336005 12.517704 12.364357 13.371308]\n",
      "Reset environment\n",
      "Episode reward: 1730.6602501571178\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.374744 13.362577 13.336795 12.518559 12.365057 13.372082]\n",
      "Reset environment\n",
      "Episode reward: 3562.150257706642\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.376465 13.364305 13.338519 12.520428 12.366591 13.373803]\n",
      "Reset environment\n",
      "Episode reward: 5214.403908073902\n",
      "Total Steps: 177\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.379288  13.3671255 13.341336  12.523447  12.369098  13.376626 ]\n",
      "Reset environment\n",
      "Episode reward: 1846.089165687561\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.380119 13.367942 13.342177 12.524357 12.36984  13.377457]\n",
      "Reset environment\n",
      "Episode reward: 2920.854057153687\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.381339  13.369123  13.3434305 12.525721  12.370936  13.378669 ]\n",
      "Reset environment\n",
      "Episode reward: 1806.3130438923836\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.382532 13.370315 13.344624 12.527032 12.371982 13.379861]\n",
      "Reset environment\n",
      "Episode reward: 2268.9318112134933\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.383314  13.371135  13.345371  12.5278635 12.3727045 13.380651 ]\n",
      "Reset environment\n",
      "Episode reward: 4037.8694044947624\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.385371 13.373181 13.347433 12.530088 12.374556 13.382706]\n",
      "Reset environment\n",
      "Episode reward: 2805.052368029952\n",
      "Total Steps: 153\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.386646 13.374443 13.34869  12.531449 12.375756 13.383986]\n",
      "Reset environment\n",
      "Episode reward: 2016.8555079698563\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.387501 13.375306 13.34954  12.532415 12.376525 13.384841]\n",
      "Reset environment\n",
      "Episode reward: 3979.0146443247795\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.389497 13.377315 13.351519 12.534587 12.378282 13.386837]\n",
      "Reset environment\n",
      "Episode reward: 2705.6845919415355\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.390758  13.378603  13.3527355 12.535972  12.379409  13.388098 ]\n",
      "Reset environment\n",
      "Episode reward: 2337.9964095652103\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.391582 13.379458 13.353534 12.536825 12.380155 13.388926]\n",
      "Reset environment\n",
      "Episode reward: 1782.3147027492523\n",
      "Total Steps: 62\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.392353 13.38022  13.354314 12.537689 12.380839 13.389698]\n",
      "Reset environment\n",
      "Episode reward: 2009.9423834085464\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.393672  13.381539  13.3556385 12.539126  12.382015  13.391019 ]\n",
      "Reset environment\n",
      "Episode reward: 1876.6310834884644\n",
      "Total Steps: 63\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.39453   13.382407  13.356491  12.5400715 12.382801  13.391877 ]\n",
      "Reset environment\n",
      "Episode reward: 2075.7597540020943\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.3958845 13.383761  13.357839  12.541552  12.384011  13.3932295]\n",
      "Reset environment\n",
      "Episode reward: 2088.940644264221\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.396843 13.384722 13.358793 12.542607 12.384867 13.394189]\n",
      "Reset environment\n",
      "Episode reward: 1384.3619219064713\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.397792  13.385671  13.359744  12.543654  12.3857155 13.395139 ]\n",
      "Reset environment\n",
      "Episode reward: 2714.4920956492424\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.399018 13.3869   13.360973 12.545002 12.386809 13.396365]\n",
      "Reset environment\n",
      "Episode reward: 4080.88230240345\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.401082 13.388946 13.363048 12.547229 12.388676 13.39843 ]\n",
      "Reset environment\n",
      "Episode reward: 2380.907436609268\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.40199  13.389853 13.363951 12.548204 12.389559 13.399343]\n",
      "Reset environment\n",
      "Episode reward: 3332.339238911867\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.403566 13.391408 13.365558 12.549918 12.390995 13.400917]\n",
      "Reset environment\n",
      "Episode reward: -680.5013502836227\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.401991  13.389838  13.363954  12.548068  12.389605  13.3993435]\n",
      "Reset environment\n",
      "Episode reward: 1448.676548242569\n",
      "Total Steps: 47\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.402627 13.390483 13.364579 12.548778 12.390181 13.399979]\n",
      "Reset environment\n",
      "Episode reward: -683.5401190519333\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.400942 13.388779 13.362923 12.547075 12.388633 13.398301]\n",
      "Reset environment\n",
      "Episode reward: 1653.447470486164\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.401705 13.389537 13.363685 12.547912 12.389321 13.399064]\n",
      "Reset environment\n",
      "Episode reward: 5224.035810291767\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.404519 13.392338 13.366503 12.55093  12.391828 13.401876]\n",
      "Reset environment\n",
      "Episode reward: 2555.0599517822266\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.405723  13.393519  13.367723  12.5522375 12.392923  13.403082 ]\n",
      "Reset environment\n",
      "Episode reward: 4275.1063131690025\n",
      "Total Steps: 146\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.407969 13.395756 13.369981 12.554651 12.394927 13.405332]\n",
      "Reset environment\n",
      "Episode reward: 4100.678696513176\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.410089 13.397855 13.3721   12.556933 12.396803 13.407445]\n",
      "Reset environment\n",
      "Episode reward: 1309.5260719656944\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.410951 13.398718 13.372958 12.557901 12.397557 13.408307]\n",
      "Reset environment\n",
      "Episode reward: 4118.19385188818\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.41307  13.400848 13.375056 12.560177 12.399423 13.410425]\n",
      "Reset environment\n",
      "Episode reward: 1509.6544392108917\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.41372   13.401502  13.375709  12.560904  12.400009  13.4110775]\n",
      "Reset environment\n",
      "Episode reward: 625.3403060436249\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.41309  13.400948 13.375    12.56013  12.399419 13.410449]\n",
      "Reset environment\n",
      "Episode reward: 2099.6242107748985\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.414461 13.402327 13.376374 12.561621 12.400657 13.411823]\n",
      "Reset environment\n",
      "Episode reward: 314.082924246788\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.413589 13.401538 13.375413 12.560532 12.399879 13.410964]\n",
      "Reset environment\n",
      "Episode reward: 2114.6603627204895\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.414541  13.4024725 13.376385  12.561585  12.400729  13.411917 ]\n",
      "Reset environment\n",
      "Episode reward: 2201.034350633621\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.415529  13.403448  13.377381  12.5626745 12.401598  13.412906 ]\n",
      "Reset environment\n",
      "Episode reward: 2587.5743396878242\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.416776 13.404688 13.378631 12.564026 12.402705 13.414152]\n",
      "Reset environment\n",
      "Episode reward: 2488.3885937929153\n",
      "Total Steps: 86\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.417961 13.405876 13.379808 12.565318 12.403773 13.415334]\n",
      "Reset environment\n",
      "Episode reward: 3961.8890500068665\n",
      "Total Steps: 135\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.420027  13.407945  13.3818655 12.567537  12.405618  13.417399 ]\n",
      "Reset environment\n",
      "Episode reward: -683.8370313644409\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.418316 13.406235 13.380141 12.565583 12.404055 13.415686]\n",
      "Reset environment\n",
      "Episode reward: 4171.24722379446\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.420483  13.408395  13.382313  12.567916  12.405995  13.4178505]\n",
      "Reset environment\n",
      "Episode reward: 4517.690982401371\n",
      "Total Steps: 154\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.422858 13.410773 13.38468  12.570473 12.408115 13.420227]\n",
      "Reset environment\n",
      "Episode reward: 1684.4544105529785\n",
      "Total Steps: 82\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.423983 13.411898 13.385808 12.571704 12.409092 13.42135 ]\n",
      "Reset environment\n",
      "Episode reward: 3655.174937322736\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.425785 13.413733 13.387582 12.57368  12.410736 13.423157]\n",
      "Reset environment\n",
      "Episode reward: 3069.868457019329\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.427226  13.4151745 13.389027  12.57525   12.4120245 13.424599 ]\n",
      "Reset environment\n",
      "Episode reward: 2894.6815185546875\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.428604  13.4165535 13.390409  12.576746  12.413258  13.425978 ]\n",
      "Reset environment\n",
      "Episode reward: 632.7677669525146\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.42816  13.416147 13.389918 12.576118 12.412901 13.425539]\n",
      "Reset environment\n",
      "Episode reward: 2005.0334038212895\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.428991 13.416955 13.390775 12.577036 12.413636 13.426372]\n",
      "Reset environment\n",
      "Episode reward: 1567.0260865688324\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.429216 13.417217 13.390969 12.577263 12.413847 13.426601]\n",
      "Reset environment\n",
      "Episode reward: 1963.7271687984467\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.430487 13.418485 13.392237 12.578651 12.41496  13.427872]\n",
      "Reset environment\n",
      "Episode reward: 2695.18779951334\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.431767 13.419757 13.393531 12.580043 12.416099 13.429155]\n",
      "Reset environment\n",
      "Episode reward: 1373.6106644272804\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.432691 13.420678 13.394453 12.581072 12.416905 13.430079]\n",
      "Reset environment\n",
      "Episode reward: 1654.3114619255066\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.433418  13.421389  13.395201  12.581875  12.4175625 13.430807 ]\n",
      "Reset environment\n",
      "Episode reward: 1654.242228090763\n",
      "Total Steps: 55\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.434145 13.422105 13.395938 12.582683 12.418208 13.431533]\n",
      "Reset environment\n",
      "Episode reward: 2858.6554990410805\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.435473 13.423435 13.397267 12.584123 12.419402 13.432863]\n",
      "Reset environment\n",
      "Episode reward: 3752.0166197419167\n",
      "Total Steps: 128\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.437399 13.425365 13.399195 12.586203 12.421144 13.434791]\n",
      "Reset environment\n",
      "Episode reward: 1804.8457378149033\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.438217 13.426186 13.400004 12.58711  12.421886 13.435609]\n",
      "Reset environment\n",
      "Episode reward: 1609.9980986118317\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.438823 13.426758 13.400648 12.587781 12.422455 13.436214]\n",
      "Reset environment\n",
      "Episode reward: 1659.0594886541367\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.439477  13.4274235 13.401285  12.588539  12.423045  13.436869 ]\n",
      "Reset environment\n",
      "Episode reward: 2345.349577128887\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.440573 13.428515 13.402388 12.589732 12.424019 13.437964]\n",
      "Reset environment\n",
      "Episode reward: 1752.4439507722855\n",
      "Total Steps: 58\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.441364 13.429291 13.403187 12.590599 12.424724 13.438754]\n",
      "Reset environment\n",
      "Episode reward: 1906.9288343191147\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.442607 13.430531 13.404433 12.591964 12.425827 13.44    ]\n",
      "Reset environment\n",
      "Episode reward: 1773.4499179720879\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.44339  13.431302 13.405224 12.592825 12.426518 13.440783]\n",
      "Reset environment\n",
      "Episode reward: 4274.859599769115\n",
      "Total Steps: 147\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.445616 13.433535 13.407431 12.595205 12.428501 13.44301 ]\n",
      "Reset environment\n",
      "Episode reward: 1830.9792640805244\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.446818  13.434735  13.408632  12.5965185 12.429557  13.444213 ]\n",
      "Reset environment\n",
      "Episode reward: 2073.325492322445\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.44816  13.436074 13.409977 12.59798  12.430736 13.445554]\n",
      "Reset environment\n",
      "Episode reward: 1857.9731006026268\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.449383  13.437292  13.411203  12.599314  12.431814  13.4467745]\n",
      "Reset environment\n",
      "Episode reward: 3383.495706498623\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.451077 13.438991 13.412892 12.601151 12.433353 13.448472]\n",
      "Reset environment\n",
      "Episode reward: 3893.7776779532433\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.453065 13.440983 13.414868 12.603276 12.435131 13.450456]\n",
      "Reset environment\n",
      "Episode reward: 2379.2315858006477\n",
      "Total Steps: 94\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.454003 13.441891 13.415841 12.604324 12.435966 13.451391]\n",
      "Reset environment\n",
      "Episode reward: 2095.4446345567703\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.455363  13.443253  13.417194  12.6058035 12.437181  13.452751 ]\n",
      "Reset environment\n",
      "Episode reward: 1828.0018144249916\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.456557 13.44445  13.418383 12.607109 12.438243 13.453941]\n",
      "Reset environment\n",
      "Episode reward: 3048.016168117523\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.458061 13.445959 13.419876 12.608737 12.439601 13.455448]\n",
      "Reset environment\n",
      "Episode reward: 4766.977006971836\n",
      "Total Steps: 163\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.460577  13.448459  13.4223995 12.611431  12.441839  13.457962 ]\n",
      "Reset environment\n",
      "Episode reward: 2218.112573862076\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.461607 13.449498 13.423427 12.612565 12.442772 13.458991]\n",
      "Reset environment\n",
      "Episode reward: 3342.404031276703\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.463294 13.451196 13.425112 12.614394 12.444295 13.460679]\n",
      "Reset environment\n",
      "Episode reward: 2216.4491823017597\n",
      "Total Steps: 77\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.464302 13.452187 13.426137 12.615499 12.445205 13.461688]\n",
      "Reset environment\n",
      "Episode reward: 2034.3091227412224\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.46561  13.453501 13.427446 12.616928 12.446369 13.462998]\n",
      "Reset environment\n",
      "Episode reward: 2764.8333598971367\n",
      "Total Steps: 96\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.466937 13.454822 13.428778 12.618371 12.447547 13.464326]\n",
      "Reset environment\n",
      "Episode reward: 3932.5627931952477\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.468789  13.456673  13.430621  12.6203985 12.449169  13.466177 ]\n",
      "Reset environment\n",
      "Episode reward: 1961.548244357109\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.469629  13.45749   13.431479  12.6213255 12.449916  13.467016 ]\n",
      "Reset environment\n",
      "Episode reward: 2122.6083232164383\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.47099   13.458853  13.432837  12.622814  12.4511385 13.468377 ]\n",
      "Reset environment\n",
      "Episode reward: 1390.6094768047333\n",
      "Total Steps: 44\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.471593 13.459447 13.433447 12.623482 12.451675 13.46898 ]\n",
      "Reset environment\n",
      "Episode reward: 3187.9131385982037\n",
      "Total Steps: 112\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.4731045 13.460981  13.434942  12.625152  12.453071  13.470491 ]\n",
      "Reset environment\n",
      "Episode reward: 3355.4287874102592\n",
      "Total Steps: 116\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.474775 13.462642 13.436618 12.626961 12.454561 13.472164]\n",
      "Reset environment\n",
      "Episode reward: 4154.186514735222\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.476922  13.464768  13.4387665 12.629266  12.456463  13.47431  ]\n",
      "Reset environment\n",
      "Episode reward: 5328.753183722496\n",
      "Total Steps: 178\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.479792 13.467649 13.441613 12.632329 12.459054 13.477174]\n",
      "Reset environment\n",
      "Episode reward: 1859.0101321935654\n",
      "Total Steps: 66\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.480577 13.468435 13.442403 12.633215 12.459753 13.47796 ]\n",
      "Reset environment\n",
      "Episode reward: 3246.3775486946106\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.482198 13.470049 13.444025 12.634957 12.461193 13.479581]\n",
      "Reset environment\n",
      "Episode reward: 4373.24263638258\n",
      "Total Steps: 150\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.484454 13.472284 13.446289 12.637382 12.463218 13.481841]\n",
      "Reset environment\n",
      "Episode reward: 2292.182487666607\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.485546  13.473378  13.447374  12.6385765 12.464191  13.482932 ]\n",
      "Reset environment\n",
      "Episode reward: 1575.6655638217926\n",
      "Total Steps: 53\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.4862175 13.474075  13.448015  12.639318  12.464812  13.4836   ]\n",
      "Reset environment\n",
      "Episode reward: 4069.292090535164\n",
      "Total Steps: 139\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.488311 13.476158 13.450119 12.64157  12.466678 13.485692]\n",
      "Reset environment\n",
      "Episode reward: 3090.409562692046\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.489592  13.4774685 13.4513645 12.642976  12.467833  13.486975 ]\n",
      "Reset environment\n",
      "Episode reward: 3988.120343565941\n",
      "Total Steps: 136\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.4916525 13.479521  13.453427  12.645181  12.469666  13.489035 ]\n",
      "Reset environment\n",
      "Episode reward: 3688.5128540992737\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.493407 13.481276 13.455177 12.647085 12.471222 13.490788]\n",
      "Reset environment\n",
      "Episode reward: 3331.7127348184586\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.495066 13.482927 13.456839 12.648874 12.472693 13.492447]\n",
      "Reset environment\n",
      "Episode reward: 1368.9741065502167\n",
      "Total Steps: 73\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.49597  13.483835 13.457743 12.649886 12.473508 13.493354]\n",
      "Reset environment\n",
      "Episode reward: 1327.7498007416725\n",
      "Total Steps: 138\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.495905  13.483825  13.457638  12.649737  12.4735155 13.493297 ]\n",
      "Reset environment\n",
      "Episode reward: 2671.435436666012\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.496909 13.484869 13.4586   12.650746 12.474435 13.494307]\n",
      "Reset environment\n",
      "Episode reward: 399.553618311882\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.495983 13.483839 13.457792 12.649747 12.473634 13.493389]\n",
      "Reset environment\n",
      "Episode reward: -338.7644522190094\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.494599 13.482431 13.456448 12.648168 12.472425 13.492007]\n",
      "Reset environment\n",
      "Episode reward: 1323.6297304332256\n",
      "Total Steps: 132\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.49463   13.482543  13.4564085 12.648098  12.472467  13.492047 ]\n",
      "Reset environment\n",
      "Episode reward: 1863.3414132595062\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.495847 13.483759 13.457628 12.649424 12.473548 13.493266]\n",
      "Reset environment\n",
      "Episode reward: 983.909182548523\n",
      "Total Steps: 31\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.496208 13.48412  13.457991 12.649846 12.473866 13.493628]\n",
      "Reset environment\n",
      "Episode reward: 503.9469927549362\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.495523  13.483372  13.45737   12.649045  12.473293  13.4929495]\n",
      "Reset environment\n",
      "Episode reward: 2059.5673087239265\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.496822 13.484667 13.458671 12.65047  12.474432 13.494247]\n",
      "Reset environment\n",
      "Episode reward: 1592.8833552598953\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.49787  13.485706 13.45973  12.651623 12.475389 13.495298]\n",
      "Reset environment\n",
      "Episode reward: 2045.5373858213425\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.498785 13.486622 13.460637 12.652632 12.476212 13.49621 ]\n",
      "Reset environment\n",
      "Episode reward: 2294.978496491909\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.499857 13.48768  13.461718 12.653798 12.477171 13.497282]\n",
      "Reset environment\n",
      "Episode reward: 3301.5915457606316\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.501491 13.489309 13.46336  12.655562 12.478626 13.498917]\n",
      "Reset environment\n",
      "Episode reward: 2180.1457504034042\n",
      "Total Steps: 99\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.502887 13.490712 13.464755 12.657075 12.479886 13.500314]\n",
      "Reset environment\n",
      "Episode reward: 2069.7729110717773\n",
      "Total Steps: 95\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.504212 13.492048 13.46608  12.658523 12.48108  13.501643]\n",
      "Reset environment\n",
      "Episode reward: 5287.852538943291\n",
      "Total Steps: 180\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.507026 13.494854 13.46889  12.661525 12.483576 13.504456]\n",
      "Reset environment\n",
      "Episode reward: 2575.143274731934\n",
      "Total Steps: 144\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.508183  13.496029  13.470017  12.662742  12.4846735 13.505613 ]\n",
      "Reset environment\n",
      "Episode reward: 2464.226668894291\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.509314 13.497167 13.471139 12.663983 12.485688 13.506744]\n",
      "Reset environment\n",
      "Episode reward: 2197.9029166698456\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.510253 13.498082 13.472105 12.665016 12.486534 13.507684]\n",
      "Reset environment\n",
      "Episode reward: 2001.8084005713463\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.511145 13.498974 13.472992 12.665997 12.487334 13.508575]\n",
      "Reset environment\n",
      "Episode reward: 2156.8940358161926\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.512534 13.500369 13.47438  12.667502 12.488575 13.509967]\n",
      "Reset environment\n",
      "Episode reward: 3368.943770825863\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.514172 13.502017 13.476008 12.669271 12.490023 13.511607]\n",
      "Reset environment\n",
      "Episode reward: 5438.136290580034\n",
      "Total Steps: 190\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.517022  13.5048895 13.4788265 12.672313  12.492581  13.514461 ]\n",
      "Reset environment\n",
      "Episode reward: 1785.1843611896038\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.517506 13.505423 13.479274 12.67289  12.493002 13.514951]\n",
      "Reset environment\n",
      "Episode reward: 3045.2696729004383\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.518864 13.506815 13.480588 12.674375 12.4942   13.51631 ]\n",
      "Reset environment\n",
      "Episode reward: 2919.5194148421288\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5202875 13.508245  13.482006  12.675912  12.495469  13.517733 ]\n",
      "Reset environment\n",
      "Episode reward: -369.36750993318856\n",
      "Total Steps: 106\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.518961 13.50695  13.480655 12.674543 12.494256 13.51641 ]\n",
      "Reset environment\n",
      "Episode reward: 4572.372436702251\n",
      "Total Steps: 158\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.521305 13.509273 13.483015 12.677062 12.496347 13.518756]\n",
      "Reset environment\n",
      "Episode reward: 5115.217414677143\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.523977  13.511938  13.4856825 12.679918  12.498731  13.521431 ]\n",
      "Reset environment\n",
      "Episode reward: 1499.3794059753418\n",
      "Total Steps: 52\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.524579 13.512516 13.486306 12.680599 12.499269 13.522033]\n",
      "Reset environment\n",
      "Episode reward: 2133.0730022192\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.525939 13.513873 13.487666 12.682075 12.500471 13.523395]\n",
      "Reset environment\n",
      "Episode reward: 100.07083714008331\n",
      "Total Steps: 126\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.524755 13.512797 13.486381 12.680771 12.499388 13.522212]\n",
      "Reset environment\n",
      "Episode reward: 4382.236279278994\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.526989  13.515031  13.48861   12.683167  12.501375  13.5244465]\n",
      "Reset environment\n",
      "Episode reward: 1400.7052357196808\n",
      "Total Steps: 79\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.527107 13.515082 13.488786 12.683397 12.501479 13.524565]\n",
      "Reset environment\n",
      "Episode reward: 1800.5968919992447\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5282955 13.516273  13.489972  12.684698  12.50253   13.525753 ]\n",
      "Reset environment\n",
      "Episode reward: 1598.3493585586548\n",
      "Total Steps: 50\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.529027 13.517002 13.490704 12.685503 12.5032   13.526485]\n",
      "Reset environment\n",
      "Episode reward: 1489.0366015434265\n",
      "Total Steps: 48\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.529671 13.51766  13.491341 12.686232 12.503796 13.527129]\n",
      "Reset environment\n",
      "Episode reward: 3652.2474873661995\n",
      "Total Steps: 127\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.531469 13.519441 13.493158 12.688176 12.505422 13.528926]\n",
      "Reset environment\n",
      "Episode reward: 6232.268242239952\n",
      "Total Steps: 214\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.534616 13.522586 13.496315 12.691558 12.508236 13.532073]\n",
      "Reset environment\n",
      "Episode reward: 3615.8132387697697\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.536454 13.524441 13.498144 12.693545 12.509902 13.53391 ]\n",
      "Reset environment\n",
      "Episode reward: 1960.1034969091415\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.537301  13.525295  13.4989805 12.694478  12.510656  13.534757 ]\n",
      "Reset environment\n",
      "Episode reward: 1407.97476375103\n",
      "Total Steps: 46\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.537887 13.525887 13.499563 12.695131 12.511189 13.535342]\n",
      "Reset environment\n",
      "Episode reward: 3006.2845872044563\n",
      "Total Steps: 111\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.539235 13.527283 13.500867 12.696617 12.512393 13.536698]\n",
      "Reset environment\n",
      "Episode reward: 4132.843393146992\n",
      "Total Steps: 141\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.541355 13.529398 13.502988 12.698893 12.514291 13.538817]\n",
      "Reset environment\n",
      "Episode reward: 2199.104127705097\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.542322 13.530376 13.503941 12.699955 12.515135 13.539784]\n",
      "Reset environment\n",
      "Episode reward: 2572.5819278359413\n",
      "Total Steps: 89\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.543524  13.531581  13.5051365 12.701275  12.516213  13.540984 ]\n",
      "Reset environment\n",
      "Episode reward: 2457.738528419286\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.544585 13.532622 13.506224 12.702433 12.517152 13.54205 ]\n",
      "Reset environment\n",
      "Episode reward: 2524.274479702115\n",
      "Total Steps: 93\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.545672  13.533685  13.507339  12.70363   12.518116  13.5431385]\n",
      "Reset environment\n",
      "Episode reward: 2214.0209287405014\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.547085 13.535095 13.508753 12.705156 12.519357 13.544552]\n",
      "Reset environment\n",
      "Episode reward: 3359.25230678916\n",
      "Total Steps: 115\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.548756  13.536775  13.510412  12.706961  12.5208435 13.546221 ]\n",
      "Reset environment\n",
      "Episode reward: 2463.5277611613274\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.549902 13.537926 13.51156  12.708213 12.521871 13.547369]\n",
      "Reset environment\n",
      "Episode reward: -104.34775066375732\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.548896  13.5369425 13.510518  12.706922  12.520976  13.546367 ]\n",
      "Reset environment\n",
      "Episode reward: 3189.390215396881\n",
      "Total Steps: 119\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.550309 13.538313 13.511973 12.708457 12.522241 13.547781]\n",
      "Reset environment\n",
      "Episode reward: 1733.3097219467163\n",
      "Total Steps: 134\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.550706 13.538747 13.512335 12.7088   12.522625 13.548186]\n",
      "Reset environment\n",
      "Episode reward: 5145.06101924181\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.55325  13.541283 13.514889 12.711539 12.524876 13.550732]\n",
      "Reset environment\n",
      "Episode reward: 1644.9574199318886\n",
      "Total Steps: 81\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.554327 13.542361 13.515964 12.712725 12.525816 13.551807]\n",
      "Reset environment\n",
      "Episode reward: 2018.8991433382034\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.555189 13.54323  13.516816 12.713677 12.526575 13.55267 ]\n",
      "Reset environment\n",
      "Episode reward: 2171.8510594964027\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.556549 13.544587 13.518186 12.715163 12.527777 13.554029]\n",
      "Reset environment\n",
      "Episode reward: 2316.7482840567827\n",
      "Total Steps: 88\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.557495  13.5455065 13.519159  12.7162075 12.528606  13.554973 ]\n",
      "Reset environment\n",
      "Episode reward: 3539.3746650218964\n",
      "Total Steps: 122\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5592575 13.54728   13.520917  12.718103  12.530184  13.556736 ]\n",
      "Reset environment\n",
      "Episode reward: 2518.2489100694656\n",
      "Total Steps: 87\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5604315 13.54846   13.522083  12.719376  12.531247  13.557911 ]\n",
      "Reset environment\n",
      "Episode reward: 2194.2819296717644\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5613985 13.549437  13.523041  12.720435  12.532113  13.558875 ]\n",
      "Reset environment\n",
      "Episode reward: 4271.463831961155\n",
      "Total Steps: 149\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.563539 13.551557 13.52518  12.722736 12.534039 13.561015]\n",
      "Reset environment\n",
      "Episode reward: 2528.9422305822372\n",
      "Total Steps: 143\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.564659 13.552697 13.526279 12.723935 12.535089 13.56214 ]\n",
      "Reset environment\n",
      "Episode reward: 1542.9200514554977\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.565252  13.553314  13.526846  12.7246065 12.53563   13.562735 ]\n",
      "Reset environment\n",
      "Episode reward: 1393.0360831022263\n",
      "Total Steps: 72\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.566185 13.554248 13.527779 12.725633 12.536459 13.563666]\n",
      "Reset environment\n",
      "Episode reward: 2995.0849719643593\n",
      "Total Steps: 102\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.567643  13.5556965 13.529248  12.727207  12.53774   13.5651245]\n",
      "Reset environment\n",
      "Episode reward: 3654.6536072660238\n",
      "Total Steps: 131\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.56938  13.557457 13.530958 12.729104 12.539307 13.566859]\n",
      "Reset environment\n",
      "Episode reward: 2029.7771389484406\n",
      "Total Steps: 69\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.570288 13.558358 13.531883 12.730104 12.540126 13.567769]\n",
      "Reset environment\n",
      "Episode reward: -685.5838162899017\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.568588 13.556665 13.530164 12.728207 12.538569 13.56607 ]\n",
      "Reset environment\n",
      "Episode reward: 3125.39712870121\n",
      "Total Steps: 107\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.570111  13.558185  13.5316925 12.729851  12.539932  13.567594 ]\n",
      "Reset environment\n",
      "Episode reward: 1722.284049987793\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.570867 13.558941 13.532447 12.730691 12.540618 13.568348]\n",
      "Reset environment\n",
      "Episode reward: 2961.7783640027046\n",
      "Total Steps: 114\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.572116 13.56023  13.533648 12.732074 12.541721 13.5696  ]\n",
      "Reset environment\n",
      "Episode reward: 1606.386974453926\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.572799 13.560918 13.534319 12.732831 12.542338 13.570281]\n",
      "Reset environment\n",
      "Episode reward: 1647.4941691756248\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.573492 13.56162  13.535009 12.733601 12.542967 13.570976]\n",
      "Reset environment\n",
      "Episode reward: 4487.306722998619\n",
      "Total Steps: 152\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.575805 13.563924 13.537325 12.736066 12.545034 13.573286]\n",
      "Reset environment\n",
      "Episode reward: 1829.2520514130592\n",
      "Total Steps: 60\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.576626 13.564748 13.538147 12.736972 12.545788 13.574107]\n",
      "Reset environment\n",
      "Episode reward: 4750.28683757782\n",
      "Total Steps: 162\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.579072  13.567199  13.540593  12.7396    12.5480175 13.576557 ]\n",
      "Reset environment\n",
      "Episode reward: 2733.1657323241234\n",
      "Total Steps: 103\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.580222  13.568314  13.5417795 12.74086   12.549061  13.577705 ]\n",
      "Reset environment\n",
      "Episode reward: 2694.0226396918297\n",
      "Total Steps: 92\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.581504 13.569599 13.543059 12.742255 12.550213 13.578985]\n",
      "Reset environment\n",
      "Episode reward: 1524.5310311615467\n",
      "Total Steps: 142\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.581566 13.569601 13.543187 12.742251 12.550356 13.579056]\n",
      "Reset environment\n",
      "Episode reward: 3754.6942591667175\n",
      "Total Steps: 129\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.583449 13.571488 13.545064 12.744279 12.55205  13.580939]\n",
      "Reset environment\n",
      "Episode reward: 1689.311140537262\n",
      "Total Steps: 56\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.584187 13.57223  13.545795 12.74509  12.552716 13.581678]\n",
      "Reset environment\n",
      "Episode reward: 4523.546206533909\n",
      "Total Steps: 156\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.586354 13.574401 13.547971 12.747433 12.554633 13.58385 ]\n",
      "Reset environment\n",
      "Episode reward: 1685.3928619623184\n",
      "Total Steps: 57\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.58707  13.575098 13.548706 12.74823  12.555281 13.584566]\n",
      "Reset environment\n",
      "Episode reward: 5115.17847186327\n",
      "Total Steps: 173\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.589738  13.577766  13.55137   12.751084  12.557716  13.5872345]\n",
      "Reset environment\n",
      "Episode reward: 2485.7695170640945\n",
      "Total Steps: 83\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.590922 13.578944 13.552561 12.752367 12.55878  13.588421]\n",
      "Reset environment\n",
      "Episode reward: 2194.8519361019135\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.592311 13.580329 13.553953 12.753874 12.560005 13.589809]\n",
      "Reset environment\n",
      "Episode reward: 1989.0957765579224\n",
      "Total Steps: 70\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.5931635 13.581192  13.5548    12.754815  12.5607815 13.590663 ]\n",
      "Reset environment\n",
      "Episode reward: 2990.853189215064\n",
      "Total Steps: 117\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.594406 13.58247  13.556005 12.756197 12.561885 13.591908]\n",
      "Reset environment\n",
      "Episode reward: 2762.269833445549\n",
      "Total Steps: 90\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.595778  13.5838375 13.55737   12.757691  12.563139  13.59328  ]\n",
      "Reset environment\n",
      "Episode reward: 2883.9277225732803\n",
      "Total Steps: 100\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.59715   13.5852165 13.55873   12.759171  12.564353  13.59465  ]\n",
      "Reset environment\n",
      "Episode reward: 3065.199652850628\n",
      "Total Steps: 105\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.59864   13.586714  13.560219  12.7607765 12.565682  13.596142 ]\n",
      "Reset environment\n",
      "Episode reward: 2416.850419282913\n",
      "Total Steps: 80\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.599686 13.58776  13.561265 12.761926 12.566604 13.597186]\n",
      "Reset environment\n",
      "Episode reward: 3493.016298731789\n",
      "Total Steps: 123\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.601336  13.5894375 13.562885  12.763734  12.568105  13.598839 ]\n",
      "Reset environment\n",
      "Episode reward: 2840.934035420418\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.602683 13.590793 13.56422  12.765188 12.569309 13.600187]\n",
      "Reset environment\n",
      "Episode reward: 2206.340083062649\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.603673 13.591786 13.565205 12.766271 12.570193 13.601178]\n",
      "Reset environment\n",
      "Episode reward: 2006.98042178154\n",
      "Total Steps: 71\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.604525 13.592623 13.566072 12.767221 12.570952 13.60203 ]\n",
      "Reset environment\n",
      "Episode reward: 4921.054565727711\n",
      "Total Steps: 168\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.607065  13.595147  13.568615  12.7699375 12.573222  13.6045685]\n",
      "Reset environment\n",
      "Episode reward: 2154.0813665390015\n",
      "Total Steps: 98\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.608426 13.596504 13.56998  12.771407 12.574423 13.605928]\n",
      "Reset environment\n",
      "Episode reward: 2031.4312732368708\n",
      "Total Steps: 76\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.609229 13.597286 13.570813 12.772298 12.575134 13.606734]\n",
      "Reset environment\n",
      "Episode reward: 382.5825958251953\n",
      "Total Steps: 109\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.608589 13.596626 13.570192 12.771489 12.574642 13.606094]\n",
      "Reset environment\n",
      "Episode reward: 3346.7324411273003\n",
      "Total Steps: 113\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.610134 13.598173 13.571741 12.773164 12.576016 13.60764 ]\n",
      "Reset environment\n",
      "Episode reward: 2600.027115881443\n",
      "Total Steps: 91\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.61133  13.599367 13.572936 12.774469 12.577075 13.608837]\n",
      "Reset environment\n",
      "Episode reward: 2178.5905143022537\n",
      "Total Steps: 74\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.612313 13.600344 13.573929 12.775552 12.577946 13.60982 ]\n",
      "Reset environment\n",
      "Episode reward: 5170.891188442707\n",
      "Total Steps: 176\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.615014 13.603048 13.576624 12.778433 12.580376 13.612523]\n",
      "Reset environment\n",
      "Episode reward: 1577.0454202890396\n",
      "Total Steps: 54\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.615653 13.603669 13.577279 12.77915  12.580955 13.613163]\n",
      "Reset environment\n",
      "Episode reward: 3469.5358507931232\n",
      "Total Steps: 121\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.617318 13.605318 13.578964 12.780951 12.582465 13.614832]\n",
      "Reset environment\n",
      "Episode reward: 1771.4894006848335\n",
      "Total Steps: 85\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.618447 13.606448 13.580095 12.782197 12.583482 13.615963]\n",
      "Reset environment\n",
      "Episode reward: 3950.128874897957\n",
      "Total Steps: 137\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.620402 13.608388 13.582055 12.7843   12.585216 13.617913]\n",
      "Reset environment\n",
      "Episode reward: 2088.8708590343595\n",
      "Total Steps: 78\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.62124   13.6092    13.582925  12.785231  12.585969  13.6187525]\n",
      "Reset environment\n",
      "Episode reward: 2759.9647538661957\n",
      "Total Steps: 97\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.62251  13.610455 13.584207 12.786624 12.587091 13.620024]\n",
      "Reset environment\n",
      "Episode reward: 4407.476291835308\n",
      "Total Steps: 151\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.624752 13.612686 13.586451 12.789026 12.58908  13.622262]\n",
      "Reset environment\n",
      "Episode reward: 5807.717503786087\n",
      "Total Steps: 195\n",
      "Agent status: Game started\n",
      "Mean Q-values: [13.627846 13.615775 13.58954  12.79232  12.591829 13.625358]\n",
      "Training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = FighterJetEnv()\n",
    "env = Monitor(env)  \n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Set the model path\n",
    "model_path = \"fighter_jet_dqn_exp_extend_copy_v14.zip\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "reload = False\n",
    "# Check if a saved model exists\n",
    "if os.path.exists(model_path) and reload:\n",
    "    print(f\"Loading existing model from {model_path}\")\n",
    "    model = DQN.load(model_path, env=env, tensorboard_log=\"./fighter_jet_dqn_exp_extend_copy_v14/\",  device=device)\n",
    "    print(\"Model loaded successfully. Continuing training...\")\n",
    "else:\n",
    "    print(\"No existing model found. Creating a new DQN agent.\")\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        verbose=0,\n",
    "        tensorboard_log=\"./fighter_jet_dqn_exp_extend_copy_v14/\",\n",
    "        learning_rate=5e-5,\n",
    "        buffer_size=500000,\n",
    "        learning_starts=50000,\n",
    "        batch_size=256,\n",
    "        tau=0.001,\n",
    "        gamma=0.99,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=5000,\n",
    "        exploration_fraction=0.7,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.1,\n",
    "        max_grad_norm=10,\n",
    "        policy_kwargs=dict(net_arch=[256, 256,256]),\n",
    "        device=device\n",
    "        # double_q=True\n",
    "    )   \n",
    "\n",
    "print_hyperparameters(model)\n",
    "# Create the callback\n",
    "callback = TensorboardCallback()\n",
    "\n",
    "# Train the agent\n",
    "total_timesteps = int(1e6)  # Convert to integer\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback, reset_num_timesteps=False)\n",
    "\n",
    "# model_path = \"fighter_jet_dqn_v12.zip\"\n",
    "# Save the trained model\n",
    "model.save(model_path)\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "print(stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset environment\n",
      "Initial observation shape: (13,)\n",
      "Reset environment\n"
     ]
    }
   ],
   "source": [
    "env = FighterJetEnv()\n",
    "# env = Monitor(env)  # Wrap with Monitor to get episode stats\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "jetArrray = []\n",
    "targetArray = []\n",
    "# Load the saved model\n",
    "modelname = \"fighter_jet_dqn_exp_extend_v14.zip\"\n",
    "model = DQN.load(modelname)\n",
    "\n",
    "# Test the loaded model\n",
    "obs, _ = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    jetArrray.append(env.jet_pos)\n",
    "    targetArray.append(env.target_pos)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetFight_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
